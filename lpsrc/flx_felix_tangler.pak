@execfile("config"+os.sep+'flx_data.py')
@head(1,"Interscript syntax colouring for Felix")
@head(1,"Tokeniser")
Start with the Python one.
@h = tangler("interscript/tokenisers/felix_keyword.py")
@select(h)
#! /usr/bin/python2.4

__all__ = ["iskeyword", "kwlist"]

kwlist = [
#--start keywords--
@for kw,name in flx_keywords:
  tangle(repr(kw)+',')
@#
#--end keywords--
        ]

iskeyword = frozenset(kwlist).__contains__

@h = tangler("interscript/tokenisers/felix_token.py")
@select(h)
#! /usr/bin/python2.4

#--start constants--
ENDMARKER = 0
NAME = 1
NUMBER = 2
STRING = 3
NEWLINE = 4
INDENT = 5
DEDENT = 6
LPAR = 7
RPAR = 8
LSQB = 9
RSQB = 10
COLON = 11
COMMA = 12
SEMI = 13
PLUS = 14
MINUS = 15
STAR = 16
SLASH = 17
VBAR = 18
AMPER = 19
LESS = 20
GREATER = 21
EQUAL = 22
DOT = 23
PERCENT = 24
BACKQUOTE = 25
LBRACE = 26
RBRACE = 27
EQEQUAL = 28
NOTEQUAL = 29
LESSEQUAL = 30
GREATEREQUAL = 31
TILDE = 32
CIRCUMFLEX = 33
LEFTSHIFT = 34
RIGHTSHIFT = 35
DOUBLESTAR = 36
PLUSEQUAL = 37
MINEQUAL = 38
STAREQUAL = 39
SLASHEQUAL = 40
PERCENTEQUAL = 41
AMPEREQUAL = 42
VBAREQUAL = 43
CIRCUMFLEXEQUAL = 44
LEFTSHIFTEQUAL = 45
RIGHTSHIFTEQUAL = 46
DOUBLESTAREQUAL = 47
DOUBLESLASH = 48
DOUBLESLASHEQUAL = 49
AT = 50
OP = 51
ERRORTOKEN = 52
N_TOKENS = 53
NT_OFFSET = 256
#--end constants--

tok_name = {}
for _name, _value in globals().items():
    if type(_value) is type(0):
        tok_name[_value] = _name


@h = tangler("interscript/tokenisers/felix.py")
@select(h)
import string, re
from interscript.tokenisers.felix_token import *

COMMENT = N_TOKENS
tok_name[COMMENT] = 'COMMENT'

NL = N_TOKENS + 1
tok_name[NL] = 'NL'

WHITESPACE = N_TOKENS+2
tok_name[WHITESPACE] = 'WHITESPACE'

MULTILINE_STRING_FIRST = N_TOKENS+3
tok_name[MULTILINE_STRING_FIRST]= 'MULTILINE_STRING_FIRST'

MULTILINE_STRING_MIDDLE = N_TOKENS+4
tok_name[MULTILINE_STRING_MIDDLE]= 'MULTILINE_STRING_MIDDLE'

MULTILINE_STRING_LAST = N_TOKENS+5
tok_name[MULTILINE_STRING_LAST]= 'MULTILINE_STRING_LAST'

# Changes from 1.3:
#     Ignore now accepts \f as whitespace.  Operator now includes '**'.
#     Ignore and Special now accept \n or \r\n at the end of a line.
#     Imagnumber is new.  Expfloat is corrected to reject '0e4'.
# Note: to quote a backslash in a regex, it must be doubled in a r'aw' string.

def group(*choices): return '(' + string.join(choices, '|') + ')'
def any(*choices): return apply(group, choices) + '*'
def maybe(*choices): return apply(group, choices) + '?'

Whitespace = r'[ \f\t]*'
Comment = r'#[^\r\n]*'
Ignore = Whitespace + any(r'\\\r?\n' + Whitespace) + maybe(Comment)
Name = r'[a-zA-Z_]\w*'

Hexnumber = r'0[xX][\da-fA-F]*[lL]?'
Octnumber = r'0[0-7]*[lL]?'
Decnumber = r'[1-9]\d*[lL]?'
Intnumber = group(Hexnumber, Octnumber, Decnumber)
Exponent = r'[eE][-+]?\d+'
Pointfloat = group(r'\d+\.\d*', r'\.\d+') + maybe(Exponent)
Expfloat = r'[1-9]\d*' + Exponent
Floatnumber = group(Pointfloat, Expfloat)
Imagnumber = group(r'0[jJ]', r'[1-9]\d*[jJ]', Floatnumber + r'[jJ]')
Number = group(Imagnumber, Floatnumber, Intnumber)

Single = any(r"[^'\\]", r'\\.') + "'"
Double = any(r'[^"\\]', r'\\.') + '"'
Single3 = any(r"[^'\\]",r'\\.',r"'[^'\\]",r"'\\.",r"''[^'\\]",r"''\\.") + "'''"
Double3 = any(r'[^"\\]',r'\\.',r'"[^"\\]',r'"\\.',r'""[^"\\]',r'""\\.') + '"""'
Triple = group("[rR]?'''", '[rR]?"""')
String = group("[rR]?'" + any(r"[^\n'\\]", r'\\.') + "'",
               '[rR]?"' + any(r'[^\n"\\]', r'\\.') + '"')

Operator = group('\+', '\-', '\*\*', '\*', '\^', '~', '/', '%', '&', '\|',
                 '<<', '>>', '==', '<=', '<>', '!=', '>=', '=', '<', '>')
Bracket = '[][(){}]'
Special = group(r'\r?\n', r'[:;.,`]')
Funny = group(Operator, Bracket, Special)

PlainToken = group(Number, Funny, String, Name)
Token = Ignore + PlainToken

ContStr = group("[rR]?'" + any(r'\\.', r"[^\n'\\]") + group("'", r'\\\r?\n'),
                '[rR]?"' + any(r'\\.', r'[^\n"\\]') + group('"', r'\\\r?\n'))
PseudoExtras = group(r'\\\r?\n', Comment, Triple)
PseudoToken = Whitespace + group(PseudoExtras, Number, Funny, ContStr, Name)

tokenprog, pseudoprog, single3prog, double3prog = map(
    re.compile, (Token, PseudoToken, Single3, Double3))
endprogs = {"'": re.compile(Single), '"': re.compile(Double),
            "'''": single3prog, '"""': double3prog,
            "r'''": single3prog, 'r"""': double3prog,
            "R'''": single3prog, 'R"""': double3prog, 'r': None, 'R': None}

opdict = {
  '(':LPAR,
  ')':RPAR,
  '[':LSQB,
  ']':RSQB,
  ':':COLON,
  ',':COMMA,
  ';':SEMI,
  '+':PLUS,
  '-':MINUS,
  '*':STAR,
  '/':SLASH,
  '|':VBAR,
  '&':AMPER,
  '<':LESS,
  '>':GREATER,
  '=':EQUAL,
  '.':DOT,
  '%':PERCENT,
  '`':BACKQUOTE,
  '{':LBRACE,
  '}':RBRACE,
  '==':EQEQUAL,
  '!=':NOTEQUAL,
  '<>':NOTEQUAL,
  '<=':LESSEQUAL,
  '>=':GREATEREQUAL,
  '~':TILDE,
  '^':CIRCUMFLEX,
  '<<':LEFTSHIFT,
  '>>':RIGHTSHIFT,
  '**':DOUBLESTAR
  }

tabsize = 8
TokenError = 'TokenError'
def printtoken(type, token, (srow, scol), (erow, ecol), line): # for testing
    print "%d,%d-%d,%d:\t%s\t%s" % \
        (srow, scol, erow, ecol, tok_name[type], repr(token))

def tokenise(readline,
  tokeneater=printtoken,
  squashop=1, report_comments=1, split_multiline_strings=0):
  t = felix_tokeniser(squashop, report_comments, split_multiline_strings)
  line = readline()
  while line:
    t.writeline(line)
    for token in t.tokens:
      apply(tokeneater,token)
    t.tokens = []
    line = readline()
  t.writeline('')
  for token in t.tokens:
    apply(tokeneater,token)
  t.tokens = []

namechars, numchars = string.letters + '_', string.digits

class felix_tokeniser:
  def __init__(self, squashop=0, report_comments=0, split_multiline_strings=0):
    self.squashop = squashop
    self.report_comments = report_comments
    self.split_multiline_strings = split_multiline_strings
    self.reset()

  def reset(self):
    self.lnum = self.parenlev = self.continued = 0
    self.contstr, self.needcont = '', 0
    self.contline = None
    self.indents = [0]
    self.tokens = []
    self.buffer = ''

  def get_tokens(self):
    tmp = self.tokens
    self.tokens = []
    return tmp

  def tokenize(self,data):
    self.write(data)
    return self.get_tokens()

  def tokeneater(self,*args):
    self.tokens.append(args)

  def close(self):
    if self.buffer:
      self.writeline(self.buffer)
      self.buffer = ''
    self.writeline('')
    return self.get_tokens()

  def write(self,data):
    lines = string.split(data,'\n')
    if lines:
      lines[0]=lines[0]+self.buffer
      self.buffer = ''
    for line in lines[:-1]:
      self.writeline(line+'\n')
    self.buffer = lines[-1]

  def writeline(self,line):
    lnum = self.lnum = self.lnum + 1
    pos, max = 0, len(line)
    tokeneater = self.tokeneater

    if self.contstr:                                   # continued string
        if not line:
            raise TokenError, ("EOF in multi-line string", self.strstart)
        endmatch = self.endprog.match(line)
        if endmatch:
            pos = end = endmatch.end(0)
            if self.split_multiline_strings:
              tokeneater(MULTILINE_STRING_LAST,
                line[:end], (lnum,0),(lnum,end), line)
            else:
              tokeneater(STRING, self.contstr + line[:end],
                self.strstart, (lnum, end), self.contline + line)
            self.contstr, self.needcont = '', 0
            self.contline = None
        elif self.needcont and line[-2:] != '\\\n' and line[-3:] != '\\\r\n':
            tokeneater(ERRORTOKEN, self.contstr + line,
                       self.strstart, (lnum, len(line)), self.contline)
            self.contstr = ''
            self.contline = None
            return
        else:
            self.contstr = self.contstr + line
            self.contline = self.contline + line
            if self.split_multiline_strings:
              tokeneater(MULTILINE_STRING_MIDDLE,
                line, (lnum, 0), (lnum, len(line)), line)
            return

    elif self.parenlev == 0 and not self.continued:    # new statement
        if not line: self._close(); return

        column = 0
        while pos < max:                               # measure leading whitespace
            if line[pos] == ' ': column = column + 1
            elif line[pos] == '\t': column = (column/tabsize + 1)*tabsize
            elif line[pos] == '\f': column = 0
            else: break
            pos = pos + 1
        if pos == max: self._close(); return           # omitted newline

        if line[pos] in '#\r\n':                       # skip comments or blank lines
            if self.report_comments:
              tokeneater((NL, COMMENT)[line[pos] == '#'], line[pos:],
                       (lnum, pos), (lnum, len(line)), line)
            return

        if column > self.indents[-1]:                  # count indents or dedents
            self.indents.append(column)
            tokeneater(INDENT, line[:pos], (lnum, 0), (lnum, pos), line)
        while column < self.indents[-1]:
            self.indents = self.indents[:-1]
            tokeneater(DEDENT, '', (lnum, pos), (lnum, pos), line)

    else:                                              # continued statement
        if not line:
            raise TokenError, ("EOF in multi-line statement", (lnum, 0))
        self.continued = 0

    while pos < max:
        pseudomatch = pseudoprog.match(line, pos)
        if pseudomatch:                                # scan for tokens
            start, end = pseudomatch.span(1)
            spos, epos, pos = (lnum, start), (lnum, end), end
            token, initial = line[start:end], line[start]

            if initial in numchars \
                or (initial == '.' and token != '.'):  # ordinary number
                tokeneater(NUMBER, token, spos, epos, line)
            elif initial in '\r\n':
                if self.parenlev == 0:
                  tokeneater(NEWLINE, token, spos, epos, line)
                elif self.report_comments:
                  tokeneater(NL, token, spos, epos, line)

            elif initial == '#':
                if self.report_comments:
                  tokeneater(COMMENT, token, spos, epos, line)
            elif token in ("'''", '"""',               # triple-quoted
                           "r'''", 'r"""', "R'''", 'R"""'):
                self.endprog = endprogs[token]
                endmatch = self.endprog.match(line, pos)
                if endmatch:                           # all on one line
                    pos = endmatch.end(0)
                    token = line[start:pos]
                    tokeneater(STRING, token, spos, (lnum, pos), line)
                else:
                    if self.split_multiline_strings:
                      token = line[start:]
                      tokeneater(MULTILINE_STRING_FIRST,
                        token, spos, (lnum, len(line)), line)
                    self.strstart = (lnum, start)    # multiple lines
                    self.contstr = line[start:]
                    self.contline = line
                    break
            elif initial in ("'", '"') or \
                token[:2] in ("r'", 'r"', "R'", 'R"'):
                if token[-1] == '\n':                  # continued string
                    self.strstart = (lnum, start)
                    self.endprog = endprogs[initial] or endprogs[token[1]]
                    self.contstr, self.needcont = line[start:], 1
                    self.contline = line
                    if self.split_multiline_strings:
                      tokeneater(MULTILINE_STRING_FIRST,
                        line[start:], (lnum, start), (lnum, len(line)), line)
                    break
                else:                                  # ordinary string
                    tokeneater(STRING, token, spos, epos, line)
            elif initial in namechars:                 # ordinary name
                tokeneater(NAME, token, spos, epos, line)
            elif initial == '\\':                      # continued stmt
                self.continued = 1
            else:
                if initial in '([{': self.parenlev = self.parenlev + 1
                elif initial in ')]}': self.parenlev = self.parenlev - 1
                if self.squashop:
                  tokeneater(OP, token, spos, epos, line)
                else:
                  op = opdict[token]
                  tokeneater(op, token, spos, epos, line)
        else:
            tokeneater(ERRORTOKEN, line[pos],
                       (lnum, pos), (lnum, pos+1), line)
            pos = pos + 1


  def _close(self):
      for indent in self.indents[1:]:          # pop remaining indent levels
          self.tokeneater(DEDENT, '', (self.lnum, 0), (self.lnum, 0), '')
      self.tokeneater(ENDMARKER, '', (self.lnum, 0), (self.lnum, 0), '')

@h = tangler("interscript/tanglers/flx.py")
@select(h)
#---------------------------------------------------------
# felix tangler: write to a file, insert source line numbers
# using '#line ' comments
# works for Felix
programming_language="felix"
from interscript.tanglers.base import tangler_base
import re
import string
from interscript.tokenisers.felix import felix_tokeniser
from interscript.tokenisers.felix import COMMENT, \
   MULTILINE_STRING_FIRST, \
   MULTILINE_STRING_MIDDLE, \
   MULTILINE_STRING_LAST
from interscript.tokenisers import felix_keyword
from interscript.tokenisers import felix_token

py_bracket_tokens = [
  felix_token.LPAR, felix_token.RPAR,
  felix_token.LSQB, felix_token.RSQB,
  felix_token.LBRACE, felix_token.RBRACE]

py_punct_tokens = [
  felix_token.COLON, felix_token.COMMA, felix_token.SEMI]

py_op_tokens = [
  felix_token.OP,
  felix_token.PLUS, felix_token.MINUS, felix_token.STAR, felix_token.SLASH,
  felix_token.VBAR, felix_token.AMPER,
  felix_token.LESS, felix_token.GREATER, felix_token.EQUAL,
  felix_token.DOT, felix_token.PERCENT,
  felix_token.BACKQUOTE, felix_token.EQEQUAL,
  felix_token.NOTEQUAL, felix_token.LESSEQUAL, felix_token.GREATEREQUAL,
  felix_token.TILDE, felix_token.CIRCUMFLEX,
  felix_token.LEFTSHIFT,  felix_token.RIGHTSHIFT, felix_token.DOUBLESTAR]

class argument:
  def __init__(self,
    name,
    protocol=None,
    description=None,
    default=None):
    self.name = name
    self.protocol = protocol
    self.description = description
    self.default = default

def tangle_precondition(indent, precondition):
  return ' ' * indent + 'assert ' + precondition + '\n'

def tangle_postcondition(indent, postcondition):
  return ' ' * indent + 'assert ' + postcondition + '\n'

def tangle_argument_check(indent, argument):
  code = ''
  if argument.protocol:
    code = code + ' '* indent + 'assert has_protocol('+\
      argument.name + ', '+ argument.protocol + ')\n'
  return code

def tangle_argument_checks(indent, arguments):
  code = ''
  for argument in arguments:
    code = code + tangle_argument_check(indent, argument)
  return code

def tangle_argument(argument):
  code = argument.name
  if argument.default: code = code + '='+argument.default
  return code

def tangle_arguments(indent, arguments):
  code = ''
  for argument in arguments[:-1]:
    code = code + ' '*indent + tangle_argument(argument)
    code = code + ',\n'
  code = code + ' '*indent + tangle_argument(arguments[-1])
  return code

def tangle_result(indent, results):
  code = ''
  for result in results:
    if result.protocol:
      code = code + ' '*indent + 'assert has_protocol(' +\
        result.name+', '+ result.protocol+')\n'
  code = code + ' '*indent + 'return '
  for result in results[:-1]:
    code = code + result.name + ', '
  code = code + results[-1].name+'\n'
  return code

def tangle_function(
  sink,
  source_file,
  source_line,
  indent,
  name,
  description=None,
  arguments=None,
  precondition=None,
  result=None,
  postcondition=None,
  initial=None,
  final=None,
  body=None):

  # argument list
  code = ' '* indent + 'def '+name
  if arguments:
    code = code + '(\n'
    code = code + tangle_arguments(indent+2, arguments)
    code = code + '):\n'
  else: code = code + '():\n'

  # argument checks
  if arguments:
    code = code + ' ' * (indent + 2) + '#check arguments\n'
    code = code + tangle_argument_checks(indent+2, arguments)

  # precondition
  if precondition:
    code = code + ' ' * (indent + 2) + '#precondition\n'
    code = code + tangle_precondition(indent+2, precondition)

  # begin try/finally block
  code = code + ' '* (indent+2) + 'try:\n'

  # initial
  if initial:
    code = code + ' ' * (indent + 4) + '#initially\n'
    for line in initial:
      code = code + ' ' * (indent+4) + line + '\n'

  # begin try/except block
  code = code + ' '* (indent+4) + 'try:\n'

  # body
  if body:
    code = code + ' ' * (indent + 6) + '#body\n'
    for line in body:
      code = code + ' ' * (indent+6) + line + '\n'


  # exception
  code = code + ' ' * (indent + 4) + '#transmit user exceptions\n'
  code = code + ' ' * (indent +4) + 'except: raise\n'
  code = code + ' ' * (indent +4) + 'else:\n'


  # postcondition
  if postcondition:
    code = code + ' ' * (indent + 6) + '#postcondition\n'
    code = code + tangle_postcondition(indent + 6, postcondition)


  # result
  if result:
    code = code + ' ' * (indent + 6) + '#return result\n'
    code = code + tangle_result(indent + 6, result)
  else: code = code + ' ' * (indent+6) + 'pass\n'

  # finally
  code = code + ' ' * (indent + 2) + '#cleanup\n'
  code = code + ' '* (indent+2) + 'finally:\n'
  if final:
    for line in final:
      code = code + ' ' * (indent+4) + line + '\n'
  else:
    code = code + ' ' * (indent+4) + 'pass\n'

  for line in string.split(code,'\n')[:-1]:
    sink.writeline(line)
  return code

#-------------------------------------------------
def weave_argument(weaver, indent, argument):
  weaver.write_code_fragment(' '* indent)
  weaver.write_code_fragment(argument.name, 'NAME')
  if argument.protocol:
    weaver.write_code_fragment(':', 'PUNCT')
    weaver.write_code_fragment(' ')
    weaver.write_code_fragment(argument.protocol, 'NAME')
  if argument.default:
    weaver.write_code_fragment('=', 'PUNCT')
    weaver.write_code_fragment(argument.default)
  if argument.description:
    weaver.write_code_fragment(' ')
    weaver.write_code_fragment(repr(argument.description), 'COMMENT')

def weave_arguments(weaver, indent, arguments):
  for argument in arguments[:-1]:
    weaver.start_code_line()
    weave_argument(weaver, indent, argument)
    weaver.write_code_fragment(',','PUNCT')
    weaver.end_code_line()
  weaver.start_code_line()
  weave_argument(weaver, indent, arguments[-1])

def weave_function(
  weaver,
  indent,
  name,
  description=None,
  arguments=None,
  precondition=None,
  result=None,
  postcondition=None,
  initial=None,
  final=None,
  body=None):

  weaver.start_code_line()
  weaver.write_code_fragment(' '*indent)
  weaver.write_code_fragment('function','KEYWORD')
  weaver.write_code_fragment(' ')
  weaver.write_code_fragment(name,'NAME')
  weaver.write_code_fragment(':','PUNCT')
  weaver.write_code_fragment(' ')
  weaver.write_code_fragment('# '+description,'COMMENT')
  weaver.end_code_line()

  if arguments:
    weaver.start_code_line()
    weaver.write_code_fragment(' '*(indent+2))
    weaver.write_code_fragment('accepts','KEYWORD')
    weaver.write_code_fragment(':','PUNCT')
    weaver.end_code_line()
    weave_arguments(weaver,indent+4,arguments)
    weaver.end_code_line()

  if precondition:
    weaver.start_code_line()
    weaver.write_code_fragment(' '*(indent+2),)
    weaver.write_code_fragment('precondition','KEYWORD')
    weaver.write_code_fragment(':','PUNCT')
    weaver.write_code_fragment(' ')
    weaver.write_code_fragment(precondition)
    weaver.end_code_line()

  if result:
    weaver.start_code_line()
    weaver.write_code_fragment(' '*(indent+2))
    weaver.write_code_fragment('returns','KEYWORD')
    weaver.end_code_line()
    weave_arguments(weaver,indent+4,result)
    weaver.end_code_line()

  if postcondition:
    weaver.start_code_line()
    weaver.write_code_fragment(' '*(indent+2))
    weaver.write_code_fragment('postcondition','KEYWORD')
    weaver.write_code_fragment(':','PUNCT')
    weaver.write_code_fragment(' ')
    weaver.write_code_fragment(postcondition)
    weaver.end_code_line()

  if body:
    for line in body:
      weaver.start_code_line()
      weaver.write_code_fragment(' '*(indent+2))
      weaver.write_code_fragment(line)
      weaver.end_code_line()

#-------------------------------------------------
class flx_tangler(tangler_base):
  def __init__(self,sink,weaver,nosref=0):
    tangler_base.__init__(self,sink,weaver,nosref)
    self.matchPOD = re.compile('^ *#@(.*)$')
    self.matchcomment = re.compile('^([^#]*)#.*$')
    self.excludeid = []
    self.userdict = {}
    self.tokeniser = felix_tokeniser(report_comments = 1, split_multiline_strings=1)
    self.language = 'felix'

#  def __del__(self):
#    try:
#      tokens = self.tokeniser.close()
#    except:
#        print 'Tokeniser error'
#        try:
#          print 'closing tokeniser for',self.sink.name
#        except:
#          print 'tangler sink missing in __del__ method'
#    tangler_base.__del__(self)

  def writeline(self,data,file,count,inhibit_sref=0):
    match = self.matchPOD.match(data)
    if match:
      command = match.group(1)
      py_exec(command,file,count,globals(),self.userdict)
    else:
      self.weaver.set_fc_anchor(file,count)
      # special hack to preserve leading #! line
      if self.sink.lines_written == 0 and len(data)>2:
        inhibit_sref = data[:2]=='#!'
      self._handle_sref(file,count, inhibit_sref)
      self._writeline(data)

      try:
        tokens = self.tokeniser.tokenize(data+'\n')
      except TokenError, e:
        print 'Tokeniser error',e
        print 'in file',file,'line',line
        print 'data['+data+']'


      # pretty printing
      chars_written = 0
      self.weaver.start_code_line(self.sink.lines_written)
      if tokens:
        for kind,id,lstart,lend,dummy in tokens:
          first = lstart[1]
          last = lend[1]
          self.weaver.write_code_fragment(data[chars_written:first])
          markup = None
          if kind == felix_token.NAME:
            if felix_keyword.iskeyword(id): markup = 'KEYWORD'
          elif kind == COMMENT: markup = 'COMMENT'
          elif kind in [felix_token.STRING,
            MULTILINE_STRING_FIRST,
            MULTILINE_STRING_MIDDLE,
            MULTILINE_STRING_LAST]: markup = 'STRING'
          elif kind == felix_token.NUMBER: markup = 'NUMBER'
          elif kind in py_bracket_tokens : markup = 'BRACKET'
          elif kind in py_punct_tokens : markup = 'PUNCT'
          elif kind in py_op_tokens: markup = 'OP'
          self.weaver.write_code_fragment(data[first:last], markup)
          chars_written = last
        self.weaver.write_code_fragment(data[chars_written:])
      self.weaver.end_code_line()

      dst_count = self.sink.lines_written
      dst_file = self.sink.name
      class_name = 0
      function_name = 0
      level = 0
      for kind,id,lstart,lend,dummy in tokens:
        if kind == felix_token.INDENT:
          level = level + 1
        elif kind == felix_token.DEDENT:
          level = level - 1
        if kind is felix_token.NAME:
          if not (felix_keyword.iskeyword(id) or id in self.excludeid):
            if not self.pass_frame.ids.has_key(id): self.pass_frame.ids[id]=[]
            self.pass_frame.ids[id].append((file,count,dst_file,dst_count))
            if class_name:
              #print 'class',id
              if not self.pass_frame.classes.has_key(id): self.pass_frame.classes[id]=[]
              self.pass_frame.classes[id].append((file,count,dst_file,dst_count))
              class_name = 0
            elif function_name:
              if not self.pass_frame.functions.has_key(id): self.pass_frame.functions[id]=[]
              self.pass_frame.functions[id].append((file,count,dst_file,dst_count))
              function_name = 0
          elif id == 'class':
            class_name = 1
          elif id == 'def':
            function_name = 1

  def write_comment(self,line,file,count):
    self.writeline('# '+line,file,count)

  def start_section(self, file, count):
    data = '#line '+str(count)+' '+'"'+file+'"'
    self._writeline(data)
    if self.weaver:
      self.weaver.echotangle(self.sink.lines_written,data)

  def get_comment_tangler(self):
    return script_comment_tangler(self.sink)

  def get_string_tangler(self,eol,width):
    return c_string_tangler(self.sink,self.get_weaver(),eol,width)

  def function(self,
    name,
    indent,
    source_file,
    source_line,
    description=None,
    arguments=None,
    precondition=None,
    result=None,
    postcondition=None,
    initial=None,
    final=None,
    body=None):

    tangle_function(
      self.sink,
      source_file,
      source_line,
      indent,
      name,
      description=description,
      arguments=arguments,
      precondition=precondition,
      result=result,
      postcondition=postcondition,
      initial=initial,
      final=final,
      body=body)

    weave_function(
      self.weaver,
      indent,
      name,
      description=description,
      arguments=arguments,
      precondition=precondition,
      result=result,
      postcondition=postcondition,
      initial=initial,
      final=final,
      body=body)

class script_comment_tangler(tangler_base):
  def writeline(self,data,file,count,inhibit_sref=0):
    if self.weaver:
      self.weaver.writeline(data)
    self._writeline('# '+line)

