@head(1,'Pre token filters')
@h = tangler('src/flx_lex1.ml')
@select(h)

open Flx_parse
open Flx_exceptions

(* 1: remove comments *)

let filter_comments x =
  let rec filter x' result  =
    match x' with 
    | COMMENT_NEWLINE _ :: t
    | COMMENT _ :: t 
    | NEWLINE :: t
    | WHITE _ :: t -> filter t result
    | h :: t -> filter t (h::result)
    | [] -> List.rev result
  in filter x []

let translate ts = 
  let filters = [
    (* 1 *) filter_comments
    ] 
  and reverse_apply dat fn = fn dat 
  in List.fold_left reverse_apply ts filters

@h = tangler('src/flx_lex1.mli')
@select(h)
val translate : Flx_parse.token list -> Flx_parse.token list

@head(1,'Pre token printer')
@h = tangler('src/flx_pretok.ml')
@select(h)
open Flx_parse
open Flx_prelex
;;
let pre_tokens_of_lexbuf buf state =
  let rec get lst = 
    let t = Flx_lex.pre_flx_lex state buf in 
    match t with
    | [Flx_parse.ENDMARKER] ->
      [Flx_parse.ENDMARKER] @ lst
    | _ -> 
      match state#get_condition with
      | `Processing ->
        get (List.rev_append t lst)
      | _ ->
        get lst
  in List.rev (get [])

let pre_tokens_of_filename filename dirname incdirs =
  let state = new Flx_lex.lexer_state filename dirname incdirs in
  let infile = open_in filename in
  let src = Lexing.from_channel infile in
  let toks = pre_tokens_of_lexbuf src state in
    close_in infile; 
    toks

let pre_tokens_of_string s filename =
  let state = new Flx_lex.lexer_state filename "" [] in
  pre_tokens_of_lexbuf (Lexing.from_string s) state


@h = tangler('src/flx_pretok.mli')
@select(h)
open Flx_parse
val pre_tokens_of_filename : string -> string -> string list -> token list
val pre_tokens_of_string : string -> string -> token list

@head(1, 'Tokeniser')
@h = tangler('src/flx_tok.ml')
@select(h)
let print_pre_token t = 
  let emit t = print_string (Flx_prelex.string_of_token t) in
    begin match t with
    | Flx_parse.COMMENT_NEWLINE s -> 
      print_endline ("//" ^ s); 

    | Flx_parse.NEWLINE -> 
      print_endline ""

    | Flx_parse.ENDMARKER -> print_endline "<<EOF>>" 
    | _ -> emit t
    end;
    flush stdout

let print_pre_tokens ts = 
  if (List.length ts) = 0
  then print_string "<Empty pretoken list>";
  print_string "   1: ";
  List.iter print_pre_token ts

let print_tokens ts = 
  let lineno = ref 0 in
  let indent = ref 0 in
  let emit t = 
    print_string ((Flx_prelex.string_of_token t) ^ " ") 
  and emit_eol t = 
    print_endline t;
    let s' = "    " ^ (string_of_int !lineno) in
    let n = String.length s' in
    print_string ((String.sub s' (n-4) 4) ^ ": ");
    for i=0 to !indent -1 do print_string "  " done
  in
  let print_token t =  
    begin match t with
    | Flx_parse.NEWLINE  -> 
      emit_eol ("//")
    | Flx_parse.LBRACE _ -> 
      incr indent;
      emit_eol "  {" 
    | Flx_parse.RBRACE _ -> 
      decr indent;
      emit_eol "}" 
    | Flx_parse.ENDMARKER -> emit_eol "#<<EOF>>"
    | _ -> emit t
    end;
    flush stdout
  in 
    List.iter print_token ts
;;

class tokeniser t = 
object
  val mutable tokens = []
  val mutable tokens_copy = []
  val mutable current_token_index = 0
  initializer tokens  <- t; tokens_copy <- t

  method token_src (dummy :Lexing.lexbuf) =
    let tmp = List.hd tokens in
    tokens <- List.tl tokens;
    current_token_index <- current_token_index + 1;
    tmp

  method report_syntax_error = 
    print_endline "";
    print_endline "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!";
    let n = List.length tokens_copy in
    let first = max 0 (current_token_index - 20)
    and last = min (n-1) (current_token_index + 20)
    and slist = ref [] in
    for i = first to current_token_index-1 do
      slist := List.concat [!slist; [List.nth tokens_copy i]]
    done;
    print_tokens !slist;
    print_endline "";
    
    let token = List.nth tokens_copy
      begin 
        if List.length tokens_copy = current_token_index
        then begin
          print_string "Unexpected End Of File";
          current_token_index - 1
        end else begin
          print_string "Syntax Error before token ";
          print_string (string_of_int current_token_index);
          current_token_index
        end
      end
    in 
    let file,line,scol,ecol = Flx_prelex.src_of_token token in
    print_endline 
    (
      " in " ^ file ^ 
      ", line " ^ string_of_int line ^ 
      " col " ^ string_of_int scol
    );
    
    slist := [];
    for i = current_token_index to last do
      slist := List.concat [!slist; [List.nth tokens_copy i]]
    done;
    print_tokens !slist;
    print_endline "";
    print_endline "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!";
    flush stdout
end
;;

type 'a parser_t = 
  (Lexing.lexbuf  -> Flx_parse.token) -> 
  Lexing.lexbuf -> 
  'a

let parse_tokens (parser:'a parser_t) (tokens: Flx_parse.token list) = 
  let toker = (new Flx_tok.tokeniser tokens) in
  try 
    parser (toker#token_src) (Lexing.from_string "dummy" )
  with _ ->
    toker#report_syntax_error;
    raise (Flx_exceptions.ParseError "Parsing Tokens")


@h = tangler('src/flx_tok.mli')
@select(h)
open Flx_parse

val print_pre_tokens : token list -> unit
val print_tokens : token list -> unit
class tokeniser :
  token list ->
  object
    val mutable current_token_index : int
    val mutable tokens : token list
    val mutable tokens_copy : token list
    method report_syntax_error : unit
    method token_src : Lexing.lexbuf -> token
  end

type 'a parser_t = 
  (Lexing.lexbuf  -> token) -> 
  Lexing.lexbuf -> 
  'a

val parse_tokens:
  'a parser_t ->
  token list ->
  'a


@head(1, 'Lexer test harness')
@h = tangler('src/flxl.ml')
@select(h)

(* just lex a file *)

let filename = Sys.argv.(1) ^ ".flx";;
print_endline "---------------------------------------";;
print_endline ("Lexing " ^ filename);;
print_endline "---------------------------------------";;

print_endline "Pre tokens";;
let pretokens = (Flx_pretok.pre_tokens_of_filename filename "" []);;
Flx_tok.print_pre_tokens  pretokens;;
print_endline "---------------------------------------";;

print_endline "Tokens";;
let tokens = Flx_lex1.translate pretokens;;
Flx_tok.print_tokens tokens;;
print_endline "---------------------------------------";;


