@head(1,'Pre token filters')
@h = tangler('src/flx_lex1.ml')
@select(h)

open Flx_parse
open Flx_exceptions

(* 1: remove comments *)

let filter_comments x =
  let rec filter x' result  =
    match x' with 
    | COMMENT_NEWLINE _ :: t
    | COMMENT _ :: t 
    | NEWLINE :: t
    | WHITE _ :: t -> filter t result
    | h :: t -> filter t (h::result)
    | [] -> List.rev result
  in filter x []

let translate ts = 
  let filters = [
    (* 1 *) filter_comments
    ] 
  and reverse_apply dat fn = fn dat 
  in List.fold_left reverse_apply ts filters

@h = tangler('src/flx_lex1.mli')
@select(h)
val translate : Flx_parse.token list -> Flx_parse.token list

@head(1,'Pre token printer')
@h = tangler('src/flx_pretok.ml')
@select(h)
open Flx_parse
open Flx_prelex
;;
let pre_tokens_of_lexbuf buf state =
  let rec get lst = 
    let t = Flx_lex.pre_flx_lex state buf in 
    match t with
    | [Flx_parse.ENDMARKER] ->
      [Flx_parse.ENDMARKER] @ lst
    | _ -> 
      match state#get_condition with
      | `Processing ->
        get (List.rev_append t lst)
      | _ ->
        get lst
  in List.rev (get [])

let pre_tokens_of_filename filename dirname incdirs =
  let e = Ocs_top.make_env () in
  let state = new Flx_lexstate.lexer_state filename dirname incdirs e in
  let infile = open_in filename in
  let src = Lexing.from_channel infile in
  let toks = pre_tokens_of_lexbuf src state in
    close_in infile; 
    toks

let pre_tokens_of_string s filename =
  let e = Ocs_top.make_env () in
  let state = new Flx_lexstate.lexer_state filename "" [] e in
  pre_tokens_of_lexbuf (Lexing.from_string s) state


@h = tangler('src/flx_pretok.mli')
@select(h)
open Flx_parse
val pre_tokens_of_filename : string -> string -> string list -> token list
val pre_tokens_of_string : string -> string -> token list

@head(1, 'Tokeniser')
@h = tangler('src/flx_tok.ml')
@select(h)
let print_pre_token t = 
  let emit t = print_string (Flx_prelex.string_of_token t) in
    begin match t with
    | Flx_parse.COMMENT_NEWLINE s -> 
      print_endline ("//" ^ s); 

    | Flx_parse.NEWLINE -> 
      print_endline ""

    | Flx_parse.ENDMARKER -> print_endline "<<EOF>>" 
    | _ -> emit t
    end;
    flush stdout

let print_pre_tokens ts = 
  if (List.length ts) = 0
  then print_string "<Empty pretoken list>";
  print_string "   1: ";
  List.iter print_pre_token ts

let print_tokens ts = 
  let lineno = ref 0 in
  let indent = ref 0 in
  let emit t = 
    print_string ((Flx_prelex.string_of_token t) ^ " ") 
  and emit_eol t = 
    print_endline t;
    let s' = "    " ^ (string_of_int !lineno) in
    let n = String.length s' in
    print_string ((String.sub s' (n-4) 4) ^ ": ");
    for i=0 to !indent -1 do print_string "  " done
  in
  let print_token t =  
    begin match t with
    | Flx_parse.NEWLINE  -> 
      emit_eol ("//")
    | Flx_parse.LBRACE _ -> 
      incr indent;
      emit_eol "  {" 
    | Flx_parse.RBRACE _ -> 
      decr indent;
      emit_eol "}" 
    | Flx_parse.ENDMARKER -> emit_eol "#<<EOF>>"
    | _ -> emit t
    end;
    flush stdout
  in 
    List.iter print_token ts
;;

class tokeniser t = 
object
  val mutable tokens = []
  val mutable tokens_copy = []
  val mutable current_token_index = 0
  initializer tokens  <- t; tokens_copy <- t

  method token_src (dummy :Lexing.lexbuf) =
    let tmp = List.hd tokens in
    tokens <- List.tl tokens;
    current_token_index <- current_token_index + 1;
    tmp

  method report_syntax_error = 
    print_endline "";
    print_endline "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!";
    let n = List.length tokens_copy in
    let first = max 0 (current_token_index - 20)
    and last = min (n-1) (current_token_index + 20)
    and slist = ref [] in
    for i = first to current_token_index-1 do
      slist := List.concat [!slist; [List.nth tokens_copy i]]
    done;
    print_tokens !slist;
    print_endline "";
    
    let token = List.nth tokens_copy
      begin 
        if List.length tokens_copy = current_token_index
        then begin
          print_string "Unexpected End Of File";
          current_token_index - 1
        end else begin
          print_string "Syntax Error before token ";
          print_string (string_of_int current_token_index);
          current_token_index
        end
      end
    in 
    let file,line,scol,ecol = Flx_prelex.src_of_token token in
    print_endline 
    (
      " in " ^ file ^ 
      ", line " ^ string_of_int line ^ 
      " col " ^ string_of_int scol
    );
    
    slist := [];
    for i = current_token_index to last do
      slist := List.concat [!slist; [List.nth tokens_copy i]]
    done;
    print_tokens !slist;
    print_endline "";
    print_endline "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!";
    flush stdout
end
;;

type 'a parser_t = 
  (Lexing.lexbuf  -> Flx_parse.token) -> 
  Lexing.lexbuf -> 
  'a

let parse_tokens (parser:'a parser_t) (tokens: Flx_parse.token list) = 
  let toker = (new tokeniser tokens) in
  try 
    parser (toker#token_src) (Lexing.from_string "dummy" )
  with 
  | Flx_exceptions.ClientError _ 
  | Flx_exceptions.ClientError2 _ 
  | Flx_exceptions.ClientErrorn _ as x -> 
    toker#report_syntax_error;
    raise x
  | _ ->
    toker#report_syntax_error;
    raise (Flx_exceptions.ParseError "Parsing Tokens")


@h = tangler('src/flx_tok.mli')
@select(h)
open Flx_parse

val print_pre_tokens : token list -> unit
val print_tokens : token list -> unit
class tokeniser :
  token list ->
  object
    val mutable current_token_index : int
    val mutable tokens : token list
    val mutable tokens_copy : token list
    method report_syntax_error : unit
    method token_src : Lexing.lexbuf -> token
  end

type 'a parser_t = 
  (Lexing.lexbuf  -> token) -> 
  Lexing.lexbuf -> 
  'a

val parse_tokens:
  'a parser_t ->
  token list ->
  'a


@head(1, 'Lexer test harness')
@h = tangler('src/flxl.ml')
@select(h)
open Flx_mtypes2
open Flx_flxopt
open Flx_getopt
open Flx_parse
open Flx_prelex
;;

let print_elkhound_tokens toks =
  let nt x = name_of_token x in
  List.iter
  (
    fun x -> 
      match x with
      | NAME (s,id) ->
        print_endline (nt x ^ " " ^ id)
      | INTEGER (s,t,v) ->
        print_endline (nt x ^ " " ^
          t^","^Big_int.string_of_big_int v
        )
      | FLOAT (s,t,v) -> 
        print_endline (nt x ^ " " ^ 
          t^","^v
        )
      | STRING (sr,s) -> 
        print_endline (nt x ^ " " ^
          Flx_string.c_quote_of_string s 
        )
      | CSTRING (sr,s) -> 
        print_endline (nt x ^ " " ^
          Flx_string.c_quote_of_string s 
        )
      | WSTRING (sr,s) -> 
        print_endline (nt x ^ " " ^
          Flx_string.c_quote_of_string s 
        )
      | USTRING (sr,s) -> 
        print_endline (nt x ^ " " ^
          Flx_string.c_quote_of_string s 
        )
      | _ ->
        print_endline (Flx_prelex.name_of_token x)
  )
  toks
;;

let run() = 
  let raw_options = parse_options Sys.argv in
  let compiler_options = get_felix_options raw_options in
  if compiler_options.print_flag then begin
    print_string "//Include directories = ";
    List.iter (fun d -> print_string (d ^ " "))
    compiler_options.include_dirs;
    print_endline ""
  end
  ;

  let elkhound_test = check_key raw_options "elkhound" in
  let filename = 
    match get_key_value raw_options "" with
    | Some s -> s
    | None -> exit 0
  in
  let filebase = filename in
  let input_file_name = filebase ^ ".flx" in

  if compiler_options.print_flag then begin
    print_endline "---------------------------------------";
    print_endline ("Lexing " ^ input_file_name);
    print_endline "---------------------------------------";
    print_endline "Pre tokens"
  end;

  let pretokens = 
    Flx_pretok.pre_tokens_of_filename 
    input_file_name 
    (Filename.dirname input_file_name) 
    [] 
  in
  if compiler_options.print_flag then begin
    Flx_tok.print_pre_tokens  pretokens;
    print_endline "---------------------------------------";
    print_endline "Tokens"
  end
  ;
  let tokens = Flx_lex1.translate pretokens in
  if not elkhound_test || compiler_options.print_flag then
    Flx_tok.print_tokens tokens;
  if compiler_options.print_flag then begin
    print_endline "---------------------------------------"
  end
  ;
  if elkhound_test then begin
    print_elkhound_tokens tokens;
  end

in 
  run()
;;

