@head(1,'Pre token filters')

@mwct = [
 ("signed char","tiny"),
 ("unsigned char","utiny"),

 ("signed int","int"),
 ("unsigned int","uint"),
 ("unsigned","uint"),

 ("long","long"),
 ("signed long","long"),
 ("unsigned long","ulong"),

 ("long int","long"),
 ("signed long int","long"),
 ("unsigned long int","ulong"),

 ("long long","vlong"),
 ("signed long long","vlong"),
 ("unsigned long long","uvlong"),

 ("long long int","vlong"),
 ("signed long long int","vlong"),
 ("unsigned long long int","uvlong"),

 ("float double","double"),
 ("double float","double"),
 ("long double float","ldouble"),
 ]

@mwct = [ (string.split(s,' '),t) for s,t in mwct ]
@mwct = [ (len (s), s, t) for s,t in mwct ]
@mwct.sort()
@mwct.reverse()

@h = tangler('src/compiler/flxlib/flx_lex1.ml')
@select(h)

open Flx_token
open Flx_exceptions
open List
open Flx_srcref

(*

NOTES on token filters
----------------------

These routine are all meant to be short and simple
so its easy to see what they do -- the result
of composition is inefficient!

The results of filter application may depend on
ordering! In other words, each filter has
a postcondition, but also may have several
stronger postconditions given a pre-condition.

Composition of these routines is inefficient!
*)

let dummysr = ("<eol>",0,0,0)

(* throw out C comments; replace newline terminated C++
   comment with a newline
*)
let filter_comments x =
  let rec filter x' result  =
    match x' with
    | COMMENT_NEWLINE _ :: t -> filter t (NEWLINE::result)
    | COMMENT _ :: t  -> filter t result
    | h :: t -> filter t (h::result)
    | [] -> rev result
  in filter x []

(* throw out all white space *)
let filter_white x =
  let rec filter x' result  =
    match x' with
    | WHITE _ :: t -> filter t result
    | h :: t -> filter t (h::result)
    | [] -> rev result
  in filter x []

(* throw out all newlines *)
let filter_newline x =
  let rec filter x' result  =
    match x' with
    | NEWLINE :: t -> filter t result
    | h :: t -> filter t (h::result)
    | [] -> rev result
  in filter x []

(* throw out white space not immediately following a newline *)
let filter_inner_white x =
  let rec filter x' result  =
    match x' with
    | NEWLINE :: (WHITE _ as w) :: t -> filter t (w :: NEWLINE :: result)
    | WHITE _ :: t -> filter t result
    | h :: t -> filter t (h::result)
    | [] -> rev result
  in filter x []

(* throw out whitespace immediately preceding a NEWLINE *)
let filter_trailing_white x =
  let rec filter x' result  =
    match x' with
    | WHITE _ :: NEWLINE :: t -> filter t (NEWLINE::result)
    | h :: t -> filter t (h::result)
    | [] -> rev result
  in filter x []

(* throw out duplicate NEWLINE, elides blanks lines if they're cleared
   of white space etc
*)
let filter_dup_newline x =
  let rec filter x' result  =
    match x' with
    | NEWLINE :: NEWLINE :: t -> filter (NEWLINE :: t) result
    | h :: t -> filter t (h::result)
    | [] -> rev result
  in filter x []

(* throw out duplicate SEMI *)
let filter_dup_semi x =
  let rec filter x' result  =
    match x' with
    | SEMI _ as s :: SEMI _ :: t -> filter (s :: t) result
    | h :: t -> filter t (h::result)
    | [] -> rev result
  in filter x []

let check_nowhite x =
  let rec filter x' = match x' with
    | WHITE _ :: t -> failwith "UNEXPECTED WHITESPACE"
    | h :: t -> filter t
    | [] -> x
  in filter x

let check_nonewline x =
  let rec filter x' = match x' with
    | NEWLINE :: t -> failwith "UNEXPECTED NEWLINE"
    | h :: t -> filter t
    | [] -> x
  in filter x

let check_dent x =
  let rec filter x' = match x' with
    | NEWLINE :: WHITE _  :: t -> filter t
    | WHITE _ :: t -> failwith "WHITESPACE NOT AFTER NEWLINE"
    | h :: t -> filter t
    | [] -> x
  in filter x

(* this checks canonical form, every newline has white space
   after it, even width 0 if necessary
*)
let check_full_dent x =
  let rec filter x' = match x' with
    | NEWLINE :: WHITE _  :: t -> filter t
    | WHITE _ :: t -> failwith "WHITESPACE NOT AFTER NEWLINE"
    | NEWLINE :: t -> failwith "NEWLINE NOT FOLLOWED BY WHITESPACE"
    | h :: t -> filter t
    | [] -> x
  in filter x

let check_dollardollar x =
  let rec filter x' = match x' with
    | DOLLARDOLLAR _ :: NEWLINE :: WHITE _  :: t -> filter t
    | DOLLARDOLLAR _ :: t -> failwith "$$ not followed by INDENT"
    | h :: t -> filter t
    | [] -> x
  in filter x

(* canonicalise indents, by adding whitespace width 0 if necessary
  Afterwards all newlines should be followed by white space
*)
let canonical_dent x =
  let rec filter x' result  =
    match x' with
    | NEWLINE :: (WHITE _ as w) :: t -> filter t (w::NEWLINE::result)
    | NEWLINE :: t -> filter t (WHITE 0::NEWLINE::result)
    | h :: t -> filter t (h::result)
    | [] -> rev result
  in filter x []


let dentit x =
  let rec filter x' dent result = match x' with
    (* New block start *)
    | DOLLARDOLLAR sr :: NEWLINE :: WHITE n :: t ->
      filter t (n::dent) (LBRACE sr::result)

    (* continuation line *)
    | NEWLINE :: WHITE n :: t when n > hd dent ->
      filter t dent result

    (* new statement *)
    | NEWLINE :: WHITE n :: t when n = hd dent ->
      filter t dent (SEMI dummysr :: result)

    (* dedent *)
    | NEWLINE :: WHITE n :: t when n < hd dent ->
      filter x' (tl dent) (RBRACE dummysr :: SEMI dummysr :: result)

    | ENDMARKER :: t when length dent > 1 ->
      filter x' (tl dent) (RBRACE dummysr :: SEMI dummysr :: result)

    | ENDMARKER :: t when length dent = 1 ->
      rev (ENDMARKER :: SEMI dummysr :: result)

    | h :: t -> filter t dent (h::result)
    | [] -> rev result
  in
  filter x [-1] []

(* remove comments, whitespace, newlines, trailing sloshes,
  and a trailing hash on the first line
*)
let filter_preprocessor x =
  let rec filter first_line x' result  =
    match x' with
    | WHITE _ :: t
    | COMMENT _ :: t
      -> filter first_line t result

    | COMMENT_NEWLINE _ :: t
    | NEWLINE :: t
    | SLOSH :: NEWLINE :: t
    | SLOSH :: WHITE _ :: NEWLINE :: t
      -> filter false t result

    | HASH _ :: NEWLINE :: t
    | HASH _ :: WHITE _ :: NEWLINE :: t
      when first_line  -> filter false t result

    | h :: t -> filter first_line t (h::result)
    | [] -> rev result
  in filter true x []


let compress_ctypes x =
  let rec filter x' result =
    match x' with
@for n,c,f in mwct:
  tangle('| NAME(sr,"' + string.join (c,'") :: NAME(_,"') + '") :: t -> ')
  tangle('    filter t (NAME (sr, "'+f+'") :: result)')
@#
    | h :: t -> filter t (h::result)
    | [] -> rev result
  in filter x []

let unkeyword ts =
  let rec filter inp out = match inp with
@for kw in flx_leadin_keywords:
  tangle("| ("+kw+" _ as cc) :: (USER_KEYWORD (sr,s) as u) :: tail ")
@#
  ->
    let sr = Flx_prelex.src_of_token  u in
    let s = Flx_prelex.string_of_token u in
    let u = NAME (sr,s) in
    filter tail (u :: cc :: out)
  | h :: t -> filter t (h::out)
  | [] -> rev out
  in filter ts []

let token_packer ts =
  let rec aux n o i = match i with
    | [] ->
      if n = 0 then rev o,[]
      else failwith "At end of file, unterminated token group"

    | NAME (sr,"_tok") :: t ->
      let h,t = aux (n+1) [] t in
      aux n (TOKEN_LIST h :: o) t

    | NAME (sr,"_etok") :: t ->
      if n = 0 then failwith "Unmatched _etok"
      else rev o,t

    | h :: t -> aux n (h::o) t
  in
  fst (aux 0 [] ts)

type state = {
  macs: (string * (string list * token list)) list ;
  cstack : bool list;
  cond : bool;
}

let cond ls = fold_left (fun x y -> x && y) true ls

let token_expander ts =
  let rec aux s o i = match i with
  | TOKEN_LIST ts :: t -> aux s o (ts @ t)

  | NAME (sr,name) as h :: t ->
    let err x = clierr (slift sr) x in
    begin match name with
    | "_ifdef" ->
       begin match t with
       | NAME (sr2,name) :: NAME(_,"_then" ) :: t ->
         let cs = mem_assoc name s.macs :: s.cstack in
         aux { s with cond=cond cs; cstack=cs} o t
       | _ -> err "usage: _ifdef token _then .. _endif"
       end

    | "_elifdef" ->
       begin match t with
       | NAME (sr2,name) :: NAME(_,"_then" ) :: t ->
         if length s.cstack > 0 then
           let cs = mem_assoc name s.macs :: tl s.cstack in
           aux { s with cond = cond cs; cstack=cs} o t
         else
          err "Unmatch _elif"

       | _ -> err "usage: _elifdef token _then .. _endif"
       end

    | "_endif" ->
      if length s.cstack > 0 then
        let cs = tl s.cstack in
        aux { s with cond = cond cs; cstack=cs} o t
      else
        err "Unmatch _endif"

    | "_else" ->
      if length s.cstack > 0 then
        let cs = not (hd s.cstack) :: tl s.cstack in
        aux { s with cond = cond cs; cstack=cs} o t
      else
        err "Unmatch _else"

    | _ when not (s.cond) -> aux s o t

    | "_tokdef" ->
      let rec grabdef n o i = match i with
        | NAME (sr,"_tokdef") as  h :: t ->
          grabdef (n+1) (h::o) t

        | NAME (sr,"_edef") as h :: t ->
          if n = 0 then rev o,t
          else grabdef (n-1) (h::o) t

        | NAME (sr,"_quote") :: h :: t ->
          grabdef n (h::o) t

        | h::t -> grabdef n (h::o) t
        | [] -> err "unterminated token macro substream"
      in
      begin match t with
      | NAME (sr2,name) :: t ->
        let rec grabp n o i : string list * token list  =
          if n = 0 then err "too many macro args, runaway?";
          match i with
          | [] -> err "unterminated macro definition"
          | EQUAL _ :: t -> rev o, t
          | NAME (_,s) :: t -> grabp (n-1) (s::o) t
          | _ -> err "macro arg must be identifier"
        in
        let params,t = grabp 10 [] t in
        let mac,t = grabdef 0 [] t in
         aux {s with macs=(name,(params,mac))::s.macs} o t
      | _ -> err "usage: _tokdef name = stream"
      end

    | "_undef" ->
       begin match t with
       | NAME (sr2,name) :: t ->
         let rec strip flag inp out = match inp with
         | [] -> rev out
         | (n,_) :: t when flag && n = name ->
           strip false t out
         | h :: t -> strip flag t (h::out)
         in
         let macs = strip true s.macs [] in
         aux {s with macs=macs} o t
       | _ -> err "usage: _undef name"
       end

    | "_popto" ->
       begin match t with
       | NAME (sr2,name) :: t ->
         let rec strip inp = match inp with
         | [] -> err ("_popto can't find macro " ^ name);
         | (n,_) :: t when n = name -> t
         | h :: t -> strip t
         in
         let macs = strip s.macs in
         aux {s with macs=macs} o t
       | _ -> err "usage: _popto name"
       end

    | _ when mem_assoc name s.macs ->
      let rec graba n o i =
        if n = 0 then rev o,i else
        match i with
        | [] -> err ("Not enough args for macro " ^ name)
        | h :: t -> graba (n-1) (h::o) t
      in
      let params,body = assoc name s.macs in
      let args, t = graba (length params) [] t in
      let pas =
        fold_left2
        (fun m p a -> (p,a) :: m)
        [] params args
      in
      let body =
        map
        (fun t -> match t with
          | NAME(_,s) ->
            (try assoc s pas with Not_found -> t)
          | _ -> t
        )
        body
      in
      aux s o (body @ t)
    | _ -> aux s (h::o) t
    end (* name handling *)

  | h :: t when not s.cond -> aux s o t
  | h :: t -> aux s (h::o) t
  | [] -> rev o
  in aux {macs=[]; cond=true; cstack=[]} [] ts


let translate ts =
  let filters = [
    filter_comments ;
    filter_inner_white;
    filter_trailing_white;
    filter_dup_newline;
    check_dent;
    canonical_dent;
    check_full_dent;
    check_dollardollar;
    dentit ;
    filter_dup_semi;
    check_nowhite;
    check_nonewline;
    compress_ctypes ;
    unkeyword ;
    token_packer;
    token_expander;
    ]
  and reverse_apply dat fn = fn dat
  in
  let result = List.fold_left reverse_apply ts filters in
  result

let translate_preprocessor ts =
  let filters = [
    (* 1 *) filter_preprocessor ;
    (* 2 *) compress_ctypes ;
    ]
  and reverse_apply dat fn = fn dat
  in List.fold_left reverse_apply ts filters

@h = tangler('src/compiler/flxlib/flx_lex1.mli')
@select(h)
open Flx_token
val translate : token list -> token list
val translate_preprocessor : token list -> token list

@head(1,'Pre token printer')
@h = tangler('src/compiler/flxlib/flx_pretok.ml')
@select(h)
open Flx_token
open Flx_lex
open Flx_prelex
open Flx_preproc
open List

let pre_tokens_of_lexbuf buf state =
  let rec get lst =
    let t = Flx_lex.pre_flx_lex state buf in
    match t with
    | [ENDMARKER] -> lst
    | _ ->
    match state#get_condition with
      | `Processing -> get (List.rev_append t lst)
      | _ -> get lst
   in
   let tks = ENDMARKER :: get [] in
    (*
    print_endline
    (
      "#included files are " ^
      String.concat ", " state#get_include_files
    )
    ;
    *)
  let toks = List.rev tks in
  let includes = state#get_include_files in
  HASH_INCLUDE_FILES includes :: toks

(* This routine appears to be called ONLY by the parser. It is never
   called for any #includes. So it is triggered only for each
   whole file name given to flxg, or, by a syntactic include directive
   which recursively parses.
*)

let rec pre_tokens_of_filename filename dirname incdirs cache_dir expand_expr auto_imports =
  (*
  print_endline ("tokenising " ^ filename);
  *)
  let state = new Flx_lexstate.lexer_state filename dirname incdirs cache_dir expand_expr in
  let tokss =
    (map
      (fun fn->
        (*
        print_endline (" .. Autoimporting " ^ fn);
        *)
        include_directive "import" state ("auto_import",0,0,0,0)
        fn pre_flx_lex
      )
      auto_imports
    )
  in
  let split_hs toks = match toks with
  | HASH_INCLUDE_FILES fs :: toks -> fs, toks
  | _ -> assert false
  in

  (*
  print_endline ("Actually tokenising " ^ filename);
  *)
  let infile = open_in filename in
  let src = Lexing.from_channel infile in
  let toks = pre_tokens_of_lexbuf src state in close_in infile;
  let fs,toks = split_hs toks in
  let tokss = tokss @ [toks] in
  let toks = HASH_INCLUDE_FILES fs :: concat tokss in
  (*
  print_endline ("DONE tokenising " ^ filename);
  print_endline ("Token stream is: ");
  Flx_tok.print_tokens toks;
  *)
  toks

let pre_tokens_of_string s filename expand_expr =
  let state = new Flx_lexstate.lexer_state filename "" [] None expand_expr in
  pre_tokens_of_lexbuf (Lexing.from_string s) state


@h = tangler('src/compiler/flxlib/flx_pretok.mli')
@select(h)
open Flx_token
open Flx_ast

val pre_tokens_of_filename :
  string -> string -> string list ->
  string option ->
  (string -> expr_t -> expr_t) ->
  string list -> (* auto imports *)
  token list

val pre_tokens_of_string :
  string -> string ->
  (string -> expr_t -> expr_t) ->
  token list

@head(1, 'Tokeniser')
@h = tangler('src/compiler/flxlib/flx_tok.ml')
@select(h)
open Flx_ast
open Flx_exceptions
open List
open Flx_srcref
open Flx_token

let dyphack (ls : ( 'a * Dyp.priority) list) : 'a =
  match ls with
  | [x,_] -> x
  | _ -> failwith "Dypgen parser failed"

let print_pre_token t =
  let emit t = print_string (Flx_prelex.string_of_token t) in
    begin match t with
    | COMMENT_NEWLINE s ->
      print_endline ("//" ^ s);

    | NEWLINE ->
      print_endline ""

    | ENDMARKER -> print_endline "<<EOF>>"
    | _ -> emit t
    end;
    flush stdout

let print_pre_tokens ts =
  if (length ts) = 0
  then print_string "<Empty pretoken list>";
  print_string "   1: ";
  iter print_pre_token ts

let print_tokens ts =
  let lineno = ref 0 in
  let indent = ref 0 in
  let emit t =
    print_string ((Flx_prelex.string_of_token t) ^ " ")
  and emit_eol t =
    print_endline t;
    let s' = "    " ^ (string_of_int !lineno) in
    let n = String.length s' in
    print_string ((String.sub s' (n-4) 4) ^ ": ");
    for i=0 to !indent -1 do print_string "  " done
  in
  let print_token t =
    begin match t with
    | NEWLINE  ->
      emit_eol ("//")
    | LBRACE _ ->
      incr indent;
      emit_eol "  {"
    | RBRACE _ ->
      decr indent;
      emit_eol "}"
    | ENDMARKER -> emit_eol "#<<EOF>>"
    | _ -> emit t
    end;
    flush stdout
  in
    iter print_token ts
;;

class tokeniser t =
object(self)
  val mutable tokens = []
  val mutable tokens_copy = []
  val mutable current_token_index = 0
  initializer tokens  <- t; tokens_copy <- t

  method token_peek (dummy :Lexing.lexbuf) =
    hd tokens

  method token_src (dummy :Lexing.lexbuf) =
    if List.length tokens = 0 then begin
      print_endline "Tokeniser: Run out of tokens!";
      ENDMARKER
    end else
    let tmp = hd tokens in
    tokens <- tl tokens;
    current_token_index <- current_token_index + 1;
    tmp

  method put_back (x:token) =
    tokens <- x :: tokens;
    current_token_index <- current_token_index - 1

  method get_loc =
    let token = nth tokens_copy current_token_index in
    slift (Flx_prelex.src_of_token token)

  method report_syntax_error =
    print_endline "";
    print_endline "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!";
    let n = length tokens_copy in
    let first = max 0 (current_token_index - 20)
    and last = min (n-1) (current_token_index + 20)
    and slist = ref [] in
    for i = first to current_token_index-1 do
      slist := concat [!slist; [nth tokens_copy i]]
    done;
    print_tokens !slist;
    print_endline "";

    let j =
      begin
        if length tokens_copy = current_token_index
        then begin
          print_string "Unexpected End Of File";
          current_token_index - 1
        end else begin
          print_string "Syntax Error before token ";
          print_string (string_of_int current_token_index);
          current_token_index
        end
      end
    in
    let token = nth tokens_copy j in
    let sr = ref (Flx_prelex.src_of_token token) in
    let file,line,scol,ecol = !sr in
    if line <> 0 or j = 0 then
      print_endline
      (
        " in " ^ file ^
        ", line " ^ string_of_int line ^
        " col " ^ string_of_int scol
      )
    else begin
      let token = nth tokens_copy (j-1) in
      sr := Flx_prelex.src_of_token token;
      let file,line,scol,ecol = !sr in
      print_endline
      (
        " in " ^ file ^
        ", after line " ^ string_of_int line ^
        " col " ^ string_of_int scol
      )
    end
    ;

    slist := [];
    for i = current_token_index to last do
      slist := concat [!slist; [nth tokens_copy i]]
    done;
    print_tokens !slist;
    print_endline "";
    print_endline "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!";
    flush stdout;
    (*
    clierr (slift (!sr)) "Syntax Error";
    ()
    *)

end
;;


type 'a parser_t =
  (Lexing.lexbuf  -> token) ->
  Lexing.lexbuf ->
  'a

let parse_tokens (parser:'a parser_t) (tokens: token list) =
  let toker = (new tokeniser tokens) in
  try
    parser (toker#token_src) (Lexing.from_string "dummy" )
  with
  | Flx_exceptions.ClientError _
  | Flx_exceptions.ClientError2 _
  | Flx_exceptions.ClientErrorn _ as x ->
    (*
    print_endline ("got client error from parse..");
    *)
    toker#report_syntax_error;
    raise x

  | Flx_exceptions.ParseError _ as x ->
    (*
    print_endline ("got ParseError from parse..");
    *)
    toker#report_syntax_error;
    raise x

  | Flx_exceptions.RDP_match_fail _ as x ->
    (*
    print_endline ("got RDP_match_fail from parse..");
    *)
    toker#report_syntax_error;
    raise x

  | exn ->
    print_endline "Got unknown error from parse..";
    print_endline (Printexc.to_string exn);
    toker#report_syntax_error;
    raise (Flx_exceptions.ParseError "Parsing Tokens")


@h = tangler('src/compiler/flxlib/flx_tok.mli')
@select(h)
open Flx_token
open Flx_ast

val print_pre_tokens : token list -> unit
val print_tokens : token list -> unit
class tokeniser :
  token list ->
  object
    val mutable current_token_index : int
    val mutable tokens : token list
    val mutable tokens_copy : token list
    method report_syntax_error : unit
    method put_back : token -> unit
    method get_loc: range_srcref
    method token_src : Lexing.lexbuf -> token
    method token_peek : Lexing.lexbuf -> token
  end

type 'a parser_t =
  (Lexing.lexbuf  -> token) ->
  Lexing.lexbuf ->
  'a

val parse_tokens:
  'a parser_t ->
  token list ->
  'a

@head(1, 'Lexer test harness')
@h = tangler('src/compiler/drivers/flxl.ml')
@select(h)
open Flx_mtypes2
open Flx_flxopt
open Flx_getopt
open Flx_types
open Flx_version
open Flx_prelex
open Flx_token
;;
Flx_version_hook.set_version ();;

let print_elkhound_tokens toks =
  let nt x = name_of_token x in
  List.iter
  (
    fun x ->
      match x with
      | NAME (s,id) ->
        print_endline (nt x ^ " " ^ id)
      | INTEGER (s,t,v) ->
        print_endline (nt x ^ " " ^
          t^","^Big_int.string_of_big_int v
        )
      | FLOAT (s,t,v) ->
        print_endline (nt x ^ " " ^
          t^","^v
        )
      | STRING (sr,s) ->
        print_endline (nt x ^ " " ^
          Flx_string.c_quote_of_string s
        )
      | CSTRING (sr,s) ->
        print_endline (nt x ^ " " ^
          Flx_string.c_quote_of_string s
        )
      | WSTRING (sr,s) ->
        print_endline (nt x ^ " " ^
          Flx_string.c_quote_of_string s
        )
      | USTRING (sr,s) ->
        print_endline (nt x ^ " " ^
          Flx_string.c_quote_of_string s
        )
      | _ ->
        print_endline (Flx_prelex.name_of_token x)
  )
  toks
;;

let print_help () = print_options(); exit(0)
;;

let run() =
  let raw_options = parse_options Sys.argv in
  let compiler_options = get_felix_options raw_options in

  if check_keys raw_options ["h"; "help"]
  then print_help ()
  ;
  if check_key raw_options "version"
  then (print_endline ("Felix Version " ^ !version_data.version_string))
  ;
  if compiler_options.print_flag then begin
    print_string "//Include directories = ";
    List.iter (fun d -> print_string (d ^ " "))
    compiler_options.include_dirs;
    print_endline ""
  end
  ;

  let elkhound_test = check_key raw_options "elkhound" in
  let filename =
    match get_key_value raw_options "" with
    | Some s -> s
    | None -> exit 0
  in
  let filebase = filename in
  let input_file_name = filebase ^ ".flx" in

  if compiler_options.print_flag then begin
    print_endline "---------------------------------------";
    print_endline ("Lexing " ^ input_file_name);
    print_endline "---------------------------------------";
    print_endline "Pre tokens"
  end;

  let pretokens =
    Flx_pretok.pre_tokens_of_filename
    input_file_name
    (Filename.dirname input_file_name)
    compiler_options.include_dirs
    compiler_options.cache_dir
    Flx_macro.expand_expression
    compiler_options.auto_imports
  in
  if compiler_options.print_flag then begin
    Flx_tok.print_pre_tokens  pretokens;
    print_endline "---------------------------------------";
    print_endline "Tokens"
  end
  ;
  let tokens = Flx_lex1.translate pretokens in
  if not elkhound_test || compiler_options.print_flag then
    Flx_tok.print_tokens tokens;
  if compiler_options.print_flag then begin
    print_endline "---------------------------------------"
  end
  ;
  if elkhound_test then begin
    print_elkhound_tokens tokens;
  end

in
  run()
;;

