@head(1,'Pre token filters')

@mwct = [
 ("signed char","tiny"),
 ("unsigned char","utiny"),

 ("signed int","int"),
 ("unsigned int","uint"),
 ("unsigned","uint"),

 ("long","long"),
 ("signed long","long"),
 ("unsigned long","ulong"),
 
 ("long int","long"),
 ("signed long int","long"),
 ("unsigned long int","ulong"),

 ("long long","vlong"),
 ("signed long long","vlong"),
 ("unsigned long long","uvlong"),
 
 ("long long int","vlong"),
 ("signed long long int","vlong"),
 ("unsigned long long int","uvlong"),

 ("float double","double"),
 ("double float","double"),
 ("long double float","ldouble"),
 ]

@mwct = [ (string.split(s,' '),t) for s,t in mwct ]
@mwct = [ (len (s), s, t) for s,t in mwct ]
@mwct.sort()
@mwct.reverse()

@h = tangler('src/flx_lex1.ml')
@select(h)

open Flx_parse
open Flx_exceptions
open List
open Flx_srcref

(* remove comments, whitespace, newlines *)

let filter_comments x =
  let rec filter x' result  =
    match x' with 
    | COMMENT_NEWLINE _ :: t
    | COMMENT _ :: t 
    | NEWLINE :: t
    | WHITE _ :: t -> filter t result
    | h :: t -> filter t (h::result)
    | [] -> rev result
  in filter x []

(* remove comments, whitespace, newlines, trailing sloshes,
  and a trailing hash on the first line 
*)
let filter_preprocessor x =
  let rec filter first_line x' result  =
    match x' with 
    | WHITE _ :: t
    | COMMENT _ :: t 
      -> filter first_line t result

    | COMMENT_NEWLINE _ :: t
    | NEWLINE :: t
    | SLOSH :: NEWLINE :: t 
    | SLOSH :: WHITE _ :: NEWLINE :: t
      -> filter false t result

    | HASH _ :: NEWLINE :: t
    | HASH _ :: WHITE _ :: NEWLINE :: t 
      when first_line  -> filter false t result

    | h :: t -> filter first_line t (h::result)
    | [] -> rev result
  in filter true x []


let compress_ctypes x =
  let rec filter x' result = 
    match x' with
@for n,c,f in mwct:
  tangle('| NAME(sr,"' + string.join (c,'") :: NAME(_,"') + '") :: t -> ')
  tangle('    filter t (NAME (sr, "'+f+'") :: result)')
@#    
    | h :: t -> filter t (h::result)
    | [] -> rev result
  in filter x [] 
  
let translate ts = 
  let filters = [
    (* 1 *) filter_comments ;
    (* 2 *) compress_ctypes ;
    ] 
  and reverse_apply dat fn = fn dat 
  in List.fold_left reverse_apply ts filters

let translate_preprocessor ts = 
  let filters = [
    (* 1 *) filter_preprocessor ;
    (* 2 *) compress_ctypes ;
    ] 
  and reverse_apply dat fn = fn dat 
  in List.fold_left reverse_apply ts filters

@h = tangler('src/flx_lex1.mli')
@select(h)
val translate : Flx_parse.token list -> Flx_parse.token list
val translate_preprocessor : Flx_parse.token list -> Flx_parse.token list

@head(1,'Pre token printer')
@h = tangler('src/flx_pretok.ml')
@select(h)
open Flx_parse
open Flx_prelex
open List

let pre_tokens_of_lexbuf buf state =
  let rec get lst = 
    let t = Flx_lex.pre_flx_lex state buf in 
    match t with
    | [ENDMARKER] ->
      [ENDMARKER] @ lst
    | _ -> 
      match state#get_condition with
      | `Processing ->
        get (List.rev_append t lst)
      | _ ->
        get lst
  in 
    let tks = get [] in
    (*
    print_endline 
    (
      "#included files are " ^ 
      String.concat ", " state#get_include_files
    )
    ;
    *)
    let toks = List.rev tks in
    let includes = state#get_include_files in
    HASH_INCLUDE_FILES includes :: toks

let pre_tokens_of_filename filename dirname incdirs expand_expr =
  let state = new Flx_lexstate.lexer_state filename dirname incdirs expand_expr in
  let infile = open_in filename in
  let src = Lexing.from_channel infile in
  let toks = pre_tokens_of_lexbuf src state in
    close_in infile; 
    toks

let pre_tokens_of_string s filename expand_expr =
  let state = new Flx_lexstate.lexer_state filename "" [] expand_expr in
  pre_tokens_of_lexbuf (Lexing.from_string s) state


@h = tangler('src/flx_pretok.mli')
@select(h)
open Flx_parse
open Flx_ast

val pre_tokens_of_filename : 
  string -> string -> string list -> 
  (string -> expr_t -> expr_t) -> 
  token list
  
val pre_tokens_of_string : 
  string -> string -> 
  (string -> expr_t -> expr_t) -> 
  token list

@head(1, 'Tokeniser')
@h = tangler('src/flx_tok.ml')
@select(h)
open Flx_ast
open Flx_exceptions
open List
open Flx_srcref 
open Flx_parse

let print_pre_token t = 
  let emit t = print_string (Flx_prelex.string_of_token t) in
    begin match t with
    | COMMENT_NEWLINE s -> 
      print_endline ("//" ^ s); 

    | NEWLINE -> 
      print_endline ""

    | ENDMARKER -> print_endline "<<EOF>>" 
    | _ -> emit t
    end;
    flush stdout

let print_pre_tokens ts = 
  if (length ts) = 0
  then print_string "<Empty pretoken list>";
  print_string "   1: ";
  iter print_pre_token ts

let print_tokens ts = 
  let lineno = ref 0 in
  let indent = ref 0 in
  let emit t = 
    print_string ((Flx_prelex.string_of_token t) ^ " ") 
  and emit_eol t = 
    print_endline t;
    let s' = "    " ^ (string_of_int !lineno) in
    let n = String.length s' in
    print_string ((String.sub s' (n-4) 4) ^ ": ");
    for i=0 to !indent -1 do print_string "  " done
  in
  let print_token t =  
    begin match t with
    | NEWLINE  -> 
      emit_eol ("//")
    | LBRACE _ -> 
      incr indent;
      emit_eol "  {" 
    | RBRACE _ -> 
      decr indent;
      emit_eol "}" 
    | ENDMARKER -> emit_eol "#<<EOF>>"
    | _ -> emit t
    end;
    flush stdout
  in 
    iter print_token ts
;;

class tokeniser t = 
object(self)
  val mutable tokens = []
  val mutable tokens_copy = []
  val mutable current_token_index = 0
  initializer tokens  <- t; tokens_copy <- t

  method token_peek (dummy :Lexing.lexbuf) =
    hd tokens

  method token_src (dummy :Lexing.lexbuf) =
    let tmp = hd tokens in
    tokens <- tl tokens;
    current_token_index <- current_token_index + 1;
    match tmp with
    | USER_STATEMENT_KEYWORD (sr,s,tkss,nonterminals) ->
      (*
      print_endline ("TRANSLATING USER STATEMENT KEYWORD " ^ s);
      *)
      let f = fun () -> self#parse_user_statement s (slift sr) tkss nonterminals in
      USER_STATEMENT_DRIVER (sr,s,f)
    | _ -> tmp

  method put_back (x:token) =
    tokens <- x :: tokens;
    current_token_index <- current_token_index - 1

  method report_syntax_error = 
    print_endline "";
    print_endline "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!";
    let n = length tokens_copy in
    let first = max 0 (current_token_index - 20)
    and last = min (n-1) (current_token_index + 20)
    and slist = ref [] in
    for i = first to current_token_index-1 do
      slist := concat [!slist; [nth tokens_copy i]]
    done;
    print_tokens !slist;
    print_endline "";
    
    let j =
      begin 
        if length tokens_copy = current_token_index
        then begin
          print_string "Unexpected End Of File";
          current_token_index - 1
        end else begin
          print_string "Syntax Error before token ";
          print_string (string_of_int current_token_index);
          current_token_index
        end
      end
    in 
    let token = nth tokens_copy j in
    let sr = ref (Flx_prelex.src_of_token token) in
    let file,line,scol,ecol = !sr in
    if line <> 0 or j = 0 then
      print_endline 
      (
        " in " ^ file ^ 
        ", line " ^ string_of_int line ^ 
        " col " ^ string_of_int scol
      )
    else begin
      let token = nth tokens_copy (j-1) in
      sr := Flx_prelex.src_of_token token;
      let file,line,scol,ecol = !sr in
      print_endline 
      (
        " in " ^ file ^ 
        ", after line " ^ string_of_int line ^ 
        " col " ^ string_of_int scol
      )
    end
    ; 
    
    slist := [];
    for i = current_token_index to last do
      slist := concat [!slist; [nth tokens_copy i]]
    done;
    print_tokens !slist;
    print_endline "";
    print_endline "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!";
    flush stdout;
    clierr (Flx_srcref.slift (!sr)) "Syntax Error";
    ()

  method parse_user_statement
    (name:string) 
    (sr:range_srcref)
    (tokss: (token list * ast_term_t) list)
    (nonterminals: (string,(token list * ast_term_t) list) Hashtbl.t)
  : statement_t =
    let term = self#parse_alternatives name sr tokss nonterminals in
    `AST_user_statement (sr,name,term)

  method private parse_alternatives
    (name:string) 
    (sr:range_srcref)
    (tokss:(token list * ast_term_t) list)
    (nonterminals: (string,(token list * ast_term_t) list) Hashtbl.t)
  : ast_term_t =

    (* save state for backtracking *)
    let saved_tokens = tokens in
    let saved_token_index = current_token_index in
    let rec aux tokss = match tokss with
      | (toks,term) :: tail ->
        begin 
          try 
            `Apply_term (term,self#parse_production name sr toks nonterminals)
          with _ -> 
            (* backtrack for next try *)
            tokens <- saved_tokens;
            current_token_index <- saved_token_index;
            aux tail
        end
      | [] -> clierr sr ("Syntax error matching user statement " ^ name)
    in aux tokss

  method private parse_production
    (name:string) 
    (sr:range_srcref)
    (toks:token list)
    (nonterminals: (string,(token list * ast_term_t) list) Hashtbl.t)
  : ast_term_t list =

    let dummy_lexbuf = Lexing.from_string "blah" in
    let rec aux toks res = match toks with
    | h :: t -> 
      begin match h with
      | EXPRESSION _ -> 
        (*
        print_endline "Matching expression ..";
        *)
        let e,tk = 
          exprx self#token_src dummy_lexbuf 
        in
        (*
        print_endline (
          "Expression matched, stopped by " ^
          Flx_prelex.string_of_token tk
        );
        *)
        self#put_back tk;
        aux t (`Expression_term e :: res)
        
      | STATEMENT _ ->
        (*
        print_endline "Matching statement ..";
        *)
        let s = 
          statement self#token_src dummy_lexbuf 
        in
        aux t (`Statement_term s :: res)
      
      | STATEMENTS _ ->
        (*
        print_endline "Matching statements ..";
        *)
        let s,tk = 
          statementsx self#token_src dummy_lexbuf 
        in
        self#put_back tk;
        aux t (`Statements_term s :: res)

      | IDENT _ ->
        (*
        print_endline "Matching ident ..";
        *)
        let tok' = self#token_src dummy_lexbuf in
        begin match tok' with
        | NAME (sr,s) ->
          aux t (`Identifier_term s :: res)
        | _ ->
          Flx_exceptions.clierr sr
          (
            "User statement: identifier requires, got " ^
            Flx_prelex.string_of_token tok'
          )
        end
          

      | INTEGER_LITERAL _ ->
        let tok' = self#token_src dummy_lexbuf in
        begin match tok' with
        | INTEGER (sr,kind,vl) ->
          let j = `AST_literal (slift sr,`AST_int (kind,vl)) in
          aux t (`Expression_term j :: res)
        | _ ->
          Flx_exceptions.clierr sr
          (
            "User statement: integer required, got " ^
            Flx_prelex.string_of_token tok'
          )
        end

      | FLOAT_LITERAL _ ->
        let tok' = self#token_src dummy_lexbuf in
        begin match tok' with
        | FLOAT (sr,kind,vl) ->
          let j = `AST_literal (slift sr,`AST_float (kind,vl)) in
          aux t (`Expression_term j :: res)
        | _ ->
          Flx_exceptions.clierr sr
          (
            "User statement: integer required, got " ^
            Flx_prelex.string_of_token tok'
          )
        end

      | STRING_LITERAL _ ->
        let tok' = self#token_src dummy_lexbuf in
        begin match tok' with
        | STRING (sr,s) ->
          let j = `AST_literal (slift sr,`AST_string s) in
          aux t (`Expression_term j :: res)
        | CSTRING (sr,s) ->
          let j = `AST_literal (slift sr,`AST_cstring s) in
          aux t (`Expression_term j :: res)
        | WSTRING (sr,s) ->
          let j = `AST_literal (slift sr,`AST_wstring s) in
          aux t (`Expression_term j :: res)
        | USTRING (sr,s) ->
          let j = `AST_literal (slift sr,`AST_ustring s) in
          aux t (`Expression_term j :: res)
        | _ ->
          Flx_exceptions.clierr sr
          (
            "User statement: integer required, got " ^
            Flx_prelex.string_of_token tok'
          )
        end

      | tok -> 
        let s = Flx_prelex.string_of_token tok in
        (*
        print_endline ("Checking if " ^ s ^ " is a nonterminal");
        *)
        let alts = 
          try Some (Hashtbl.find nonterminals s) 
          with Not_found -> None
        in
        begin match alts with
        | Some productions -> 
          (*
          print_endline ("FOUND NONTERMINAL " ^ s);
          *)
          let result = self#parse_alternatives s sr productions nonterminals in
          aux t (result :: res)

        | None  ->
        (*
        print_endline "Nope, not a non-terminal";
        *)
        let tok' = self#token_src dummy_lexbuf in
        let s' = Flx_prelex.string_of_token tok' in
        (*
        print_endline ("Matching other token " ^ s ^ " with " ^ s');
        *)
        if s = s' then
          aux t (`Keyword_term s :: res)
        else Flx_exceptions.clierr sr 
        (
          "Syntax Error in user statement: " ^
          "Failed to match keyword or symbol " ^
          s ^ ", got " ^ s' ^ " instead"
        )
        end
      end

    | [] -> rev res
    in aux toks []

end
;;


type 'a parser_t = 
  (Lexing.lexbuf  -> Flx_parse.token) -> 
  Lexing.lexbuf -> 
  'a

let parse_tokens (parser:'a parser_t) (tokens: Flx_parse.token list) = 
  let toker = (new tokeniser tokens) in
  try 
    parser (toker#token_src) (Lexing.from_string "dummy" )
  with 
  | Flx_exceptions.ClientError _ 
  | Flx_exceptions.ClientError2 _ 
  | Flx_exceptions.ClientErrorn _ as x -> 
    toker#report_syntax_error;
    raise x
  | _ ->
    toker#report_syntax_error;
    raise (Flx_exceptions.ParseError "Parsing Tokens")


@h = tangler('src/flx_tok.mli')
@select(h)
open Flx_parse
open Flx_ast

val print_pre_tokens : token list -> unit
val print_tokens : token list -> unit
class tokeniser :
  token list ->
  object
    val mutable current_token_index : int
    val mutable tokens : token list
    val mutable tokens_copy : token list
    method report_syntax_error : unit
    method put_back : token -> unit
    method token_src : Lexing.lexbuf -> token
    method token_peek : Lexing.lexbuf -> token
    method parse_user_statement: 
      string -> 
      range_srcref -> 
      (token list * ast_term_t) list -> 
      (string, (token list * ast_term_t) list) Hashtbl.t ->
      statement_t
  end

type 'a parser_t = 
  (Lexing.lexbuf  -> token) -> 
  Lexing.lexbuf -> 
  'a

val parse_tokens:
  'a parser_t ->
  token list ->
  'a

@head(1, 'Lexer test harness')
@h = tangler('src/flxl.ml')
@select(h)
open Flx_mtypes2
open Flx_flxopt
open Flx_getopt
open Flx_types
open Flx_version
open Flx_parse
open Flx_prelex
;;

let print_elkhound_tokens toks =
  let nt x = name_of_token x in
  List.iter
  (
    fun x -> 
      match x with
      | NAME (s,id) ->
        print_endline (nt x ^ " " ^ id)
      | INTEGER (s,t,v) ->
        print_endline (nt x ^ " " ^
          t^","^Big_int.string_of_big_int v
        )
      | FLOAT (s,t,v) -> 
        print_endline (nt x ^ " " ^ 
          t^","^v
        )
      | STRING (sr,s) -> 
        print_endline (nt x ^ " " ^
          Flx_string.c_quote_of_string s 
        )
      | CSTRING (sr,s) -> 
        print_endline (nt x ^ " " ^
          Flx_string.c_quote_of_string s 
        )
      | WSTRING (sr,s) -> 
        print_endline (nt x ^ " " ^
          Flx_string.c_quote_of_string s 
        )
      | USTRING (sr,s) -> 
        print_endline (nt x ^ " " ^
          Flx_string.c_quote_of_string s 
        )
      | _ ->
        print_endline (Flx_prelex.name_of_token x)
  )
  toks
;;

let print_help () = print_options(); exit(0)
;;

let run() = 
  let raw_options = parse_options Sys.argv in
  let compiler_options = get_felix_options raw_options in

  if check_keys raw_options ["h"; "help"]
  then print_help ()
  ;
  if check_key raw_options "version" 
  then (print_endline ("Felix Version " ^ !version_data.version_string))
  ;
  if compiler_options.print_flag then begin
    print_string "//Include directories = ";
    List.iter (fun d -> print_string (d ^ " "))
    compiler_options.include_dirs;
    print_endline ""
  end
  ;

  let elkhound_test = check_key raw_options "elkhound" in
  let filename = 
    match get_key_value raw_options "" with
    | Some s -> s
    | None -> exit 0
  in
  let filebase = filename in
  let input_file_name = filebase ^ ".flx" in

  if compiler_options.print_flag then begin
    print_endline "---------------------------------------";
    print_endline ("Lexing " ^ input_file_name);
    print_endline "---------------------------------------";
    print_endline "Pre tokens"
  end;

  let pretokens = 
    Flx_pretok.pre_tokens_of_filename 
    input_file_name 
    (Filename.dirname input_file_name) 
    compiler_options.include_dirs
    Flx_macro.expand_expression
  in
  if compiler_options.print_flag then begin
    Flx_tok.print_pre_tokens  pretokens;
    print_endline "---------------------------------------";
    print_endline "Tokens"
  end
  ;
  let tokens = Flx_lex1.translate pretokens in
  if not elkhound_test || compiler_options.print_flag then
    Flx_tok.print_tokens tokens;
  if compiler_options.print_flag then begin
    print_endline "---------------------------------------"
  end
  ;
  if elkhound_test then begin
    print_elkhound_tokens tokens;
  end

in 
  run()
;;

