@head(1,'Lexer')
@h = tangler('src/flx_prelex.mli')
@select(h)
val src_of_token : Flx_parse.token -> Flx_ast.srcref
val string_of_token : Flx_parse.token -> string
val name_of_token : Flx_parse.token -> string

@h = tangler('src/flx_prelex.ml')
@select(h)
open Flx_parse

let string_of_string s = "\"" ^  Flx_string.c_quote_of_string s ^ "\""

let string_of_token (tok :Flx_parse.token): string =
  match tok with
  | NAME (sr,s) -> s
  | INTEGER (sr,t,i) -> Big_int.string_of_big_int i
  | FLOAT (sr,t,v) -> v
  | STRING (sr,s) -> Flx_string.c_quote_of_string s 
  | CSTRING (sr,s) -> Flx_string.c_quote_of_string s 
  | FSTRING (sr,s) -> Flx_string.c_quote_of_string s 
  | QSTRING (sr,s) -> Flx_string.c_quote_of_string s 
  | WSTRING (sr,s) -> Flx_string.c_quote_of_string s 
  | USTRING (sr,s) -> Flx_string.c_quote_of_string s 
  | USER10 (sr,op,fn) -> "op10 " ^ op
  | USERLB (sr,_,lb) -> lb
  | USERRB (sr,rb) -> rb
  | USER_KEYWORD (sr,s) -> s
  | USER_STATEMENT_KEYWORD (sr,s,_,_) -> s
  | USER_STATEMENT_DRIVER (sr,s,_) -> s
  | HASH_INCLUDE_FILES fs -> "include_files(" ^ String.concat "," fs ^ ")"
  | TOKEN_LIST ts -> "<<token list>>"
  (*
  | PARSE_ACTION sr -> "=>#"
  *)
  
@for k,s in flx_1_char_syms:
  tangle('  | '+k+' _ -> "'+s+'"',inhibit_sref=1)

@for k,s in flx_2_char_syms:
  tangle('  | '+k+' _ -> "'+s+'"',inhibit_sref=1)

@for k,s in flx_3_char_syms:
  tangle('  | '+k+' _ -> "'+s+'"',inhibit_sref=1)

@for n,t in flx_keywords + flx_parser_keywords: 
  tangle('  |  '+t+' _ -> "'+n+'"',inhibit_sref=1)
@#
  | COMMENT s -> s (* C style comment, includes the /* */ pair *)
  | COMMENT_NEWLINE s -> "// " ^ s ^ "<NEWLINE>"
  | WHITE i -> String.make i ' '
  | NEWLINE -> "<NEWLINE>"
  | ENDMARKER -> "<<EOF>>"
  | ERRORTOKEN (sref,s) -> "<<ERROR '"^ s ^"'>>"
  | SLOSH -> "\\"

let name_of_token (tok :Flx_parse.token): string =
  match tok with
  | NAME (sr,s) -> "NAME"
  | INTEGER (sr,t,i) -> "INTEGER"
  | FLOAT (sr,t,v) -> "FLOAT"
  | STRING (sr,s) -> "STRING"
  | CSTRING (sr,s) -> "CSTRING"
  | FSTRING (sr,s) -> "FSTRING"
  | QSTRING (sr,s) -> "QSTRING"
  | WSTRING (sr,s) -> "WSTRING"
  | USTRING (sr,s) -> "USTRING"
  | USER10 (sr,op,f) -> "USER10"
  | USERLB _ -> "USERLB"
  | USERRB _ -> "USERRB"
  | USER_KEYWORD (sr,s) -> s
  | USER_STATEMENT_KEYWORD (sr,s,_,_) -> s
  | USER_STATEMENT_DRIVER (sr,s,_) -> s
  | HASH_INCLUDE_FILES _ -> "HASH_INCLUDE_FILES"
  | TOKEN_LIST _ -> "TOKEN_LIST"
  (*
  | PARSE_ACTION sr -> "PARSE_ACTION"
  *)
@for k,s in flx_1_char_syms:
  tangle('  | '+k+' _ -> "'+k+'"',inhibit_sref=1)

@for k,s in flx_2_char_syms:
  tangle('  | '+k+' _ -> "'+k+'"',inhibit_sref=1)

@for k,s in flx_3_char_syms:
  tangle('  | '+k+' _ -> "'+k+'"',inhibit_sref=1)

@for n,t in flx_keywords + flx_parser_keywords: 
  tangle('  |  '+t+' _ -> "'+t+'"',inhibit_sref=1)
@#

  | COMMENT s -> "COMMENT"
  | COMMENT_NEWLINE s -> "COMMENT_NEWLINE"
  | WHITE i -> "WHITE"
  | NEWLINE -> "NEWLINE"
  | ENDMARKER -> "ENDMARKER"
  | ERRORTOKEN (sref,s) -> "ERRORTOKEN"
  | SLOSH -> "SLOSH"

let src_of_token t = match t with
  | NEWLINE 
  | COMMENT _
  | COMMENT_NEWLINE _
  | WHITE _ 
  | ENDMARKER
  | SLOSH 
  | HASH_INCLUDE_FILES _
  | TOKEN_LIST _
    -> ("",0,0,0)

  | NAME    (s,_)
  | INTEGER (s,_,_)
  | FLOAT   (s,_,_)
  | STRING  (s,_)
  | CSTRING  (s,_)
  | FSTRING  (s,_)
  | QSTRING  (s,_)
  | WSTRING  (s,_)
  | USTRING  (s,_)
  | USER10 (s,_,_)
  | USERLB (s,_,_)
  | USERRB (s,_)
  | USER_KEYWORD (s,_)
  | USER_STATEMENT_KEYWORD (s,_,_,_)
  | USER_STATEMENT_DRIVER (s,_,_)
  (*
  | PARSE_ACTION s
  *)
  | ERRORTOKEN (s,_) 

@for k,s in flx_1_char_syms:
  tangle('  | '+k+' s ',inhibit_sref=1)

@for k,s in flx_2_char_syms:
  tangle('  | '+k+' s ',inhibit_sref=1)

@for k,s in flx_3_char_syms:
  tangle('  | '+k+' s ',inhibit_sref=1)

@for n,t in flx_keywords + flx_parser_keywords: 
  tangle('  | '+t+' s',inhibit_sref=1)
@#
    -> s

@h = tangler('src/flx_lexstate.ml')
@select(h)
open Flx_util
open Flx_parse
open Flx_string
open Big_int
open Flx_exceptions
open Flx_ast
open List

let special_tokens = 
  [
@for k,s in flx_1_char_syms:
  tangle('    ("'+s+'",(fun (sr,s)-> '+k+' sr));',inhibit_sref=1)

@for k,s in flx_2_char_syms:
  tangle('    ("'+s+'",(fun (sr,s)-> '+k+' sr));',inhibit_sref=1)

@for k,s in flx_3_char_syms:
  tangle('    ("'+s+'",(fun (sr,s)-> '+k+' sr));',inhibit_sref=1)
@#
  ]

let mk_std_tokens () =
  let tk = Array.make 4 [] in
  iter  (fun (s,f) -> 
    let n = String.length s in
    assert (n >0 && n <= 3);
    tk.(n) <- (s,f) :: tk.(n)
  )
  special_tokens
  ;
  tk

exception Duplicate_macro of string

class comment_control = 
  object (self)
    val mutable nesting_level = 0
    val mutable text = ""

    method set_text s = text <- s; nesting_level <- 1
    method append s = text <- text ^ s
    method get_comment = text

    method incr = nesting_level <- nesting_level + 1
    method decr = nesting_level <- nesting_level - 1
    method get_nesting_level = nesting_level
  end

exception Found_file of string

type condition_t = [
 | `Processing
 | `Skip_to_endif
 | `Skip_to_else
 | `Subscan
]

type location = {    
    mutable buf_pos : int;
    mutable last_buf_pos : int;
    mutable line_no : int;
    mutable original_line_no : int;
}

class file_control 
  (filename' : string) 
  (basedir': string) 
  (incdirs': string list) 
= 
  object(self)
    val mutable loc : location = { buf_pos = 0; last_buf_pos = 0; line_no = 1; original_line_no = 1; }
    method get_loc = loc
    method set_loc loc' = loc <- loc'

    (* this is the physical filename *)
    val original_filename = filename'
    val incdirs = incdirs'
    val basedir = basedir'

    (* this is the generator file name, can be set with #line directive *)
    val mutable filename = filename'
    val mutable condition:condition_t list = [`Processing]
    val macros : (string,string list * Flx_parse.token list) Hashtbl.t = Hashtbl.create 97
   
    method incr_lex_counters lexbuf =
      loc.line_no <- loc.line_no + 1;
      loc.original_line_no <- loc.original_line_no + 1;
      loc.last_buf_pos <- loc.buf_pos;
      loc.buf_pos <- Lexing.lexeme_end lexbuf

    method set_buf_pos x = loc.buf_pos <- x
    method get_buf_pos = loc.buf_pos
    method get_srcref lexbuf = 
      filename, 
      loc.line_no, 
      Lexing.lexeme_start lexbuf - loc.buf_pos + 1, 
      Lexing.lexeme_end lexbuf - loc.buf_pos 

    method get_physical_srcref lexbuf = 
      original_filename, 
      loc.original_line_no, 
      Lexing.lexeme_start lexbuf - loc.buf_pos + 1, 
      Lexing.lexeme_end lexbuf - loc.buf_pos 

    method incr n = 
      loc.line_no <- loc.line_no + n;
      loc.original_line_no <- loc.original_line_no + n

    method set_line n lexbuf = 
      loc.line_no <- n;
      loc.last_buf_pos <- loc.buf_pos;
      loc.buf_pos <- Lexing.lexeme_end lexbuf;
      (* this is a hack .. *)
      loc.original_line_no <- loc.original_line_no + 1

    method set_filename f = filename <- f
    method get_relative f = 
      let fn = Filename.concat basedir f in
      if not (Sys.file_exists fn) then
        failwith ("Relative include file \""^f^ "\" not found in "^basedir)
      else fn
      
    method get_absolute f = 
      try
        List.iter
        (fun d -> 
          let f = Filename.concat d f in 
          if Sys.file_exists f 
          then raise (Found_file f)
        )
        incdirs
        ;
        failwith ("Library File <" ^ f ^ "> not found in path")
      with Found_file s -> s 

    method store_macro name params body = 
      Hashtbl.add macros name (params,body)
      
    method undef_macro name = Hashtbl.remove macros name

    method get_macro name = 
      try Some (Hashtbl.find macros name)
      with Not_found -> None
    
    method get_macros = macros

    method get_incdirs = incdirs
    method get_condition = List.hd condition
    method push_condition c =  condition <- (c :: condition)
    method pop_condition = condition <- List.tl condition
    method set_condition c = condition <- (c :: List.tl condition)
    method condition_stack_length = List.length condition
  end

class lexer_state filename basedir incdirs expand_expr' = 
  object (self)
    val expand_expr: string -> expr_t -> expr_t = expand_expr'

    val mutable include_files: string list = []

    val comment_ctrl = new comment_control
    val file_ctrl = new file_control filename basedir incdirs 
    val mutable at_line_start = true

    val mutable keywords:
      (string * (srcref * string -> Flx_parse.token)) list array
      = [| [] |]

    val mutable symbols:
      (string * (srcref * string -> Flx_parse.token)) list array
      = mk_std_tokens () 

    val nonterminals: 
      (string, (token list * ast_term_t) list) Hashtbl.t 
      = Hashtbl.create 97

    val mutable brackets: ((string * string) * string) list = []

    method get_expand_expr = expand_expr
    method get_include_files = include_files
    method add_include_file f = include_files <- f :: include_files

    method get_symbols = symbols
    method get_nonterminals = nonterminals
    method get_brackets = brackets

    method is_at_line_start = at_line_start

    method inbody = at_line_start <- false
    method get_srcref lexbuf = file_ctrl#get_srcref lexbuf
    method get_physical_srcref lexbuf = file_ctrl#get_physical_srcref lexbuf
    method string_of_srcref lexbuf =
      match self#get_srcref lexbuf with
      (filename, lineno, scol,ecol) ->
      "File \"" ^ filename ^ "\"" ^
      ", Line " ^ string_of_int lineno ^
      ", Columns " ^ string_of_int scol ^
      "-" ^ string_of_int ecol

    (* comments *)
    method comment_level = comment_ctrl#get_nesting_level
    method incr_comment = comment_ctrl#incr
    method decr_comment = comment_ctrl#decr

    method set_comment text = comment_ctrl#set_text text
    method append_comment text = comment_ctrl#append text
    method get_comment = comment_ctrl#get_comment

    (* line counting *)
    method newline lexbuf = 
      at_line_start <- true;
      file_ctrl#incr_lex_counters lexbuf

    (* string decoders *)
    method decode decoder (s : string) : string = 
      let lfcount s = 
        let n = ref 0 in
        for i = 0 to (String.length s) - 1 do
          if s.[i] = '\n' then incr n
        done;
        !n
      in 
        file_ctrl#incr (lfcount s); 
        decoder s

    method set_line n lexbuf = 
      file_ctrl#set_line n lexbuf;
      at_line_start <- true

    method set_filename f = file_ctrl#set_filename f

    method get_loc = file_ctrl#get_loc
    method set_loc loc' = file_ctrl#set_loc loc'
    method get_incdirs = file_ctrl#get_incdirs
    method get_relative f = file_ctrl#get_relative f
    method get_absolute f = file_ctrl#get_absolute f
    
    method get_condition = file_ctrl#get_condition 
    method push_condition c = file_ctrl#push_condition c
    method pop_condition = file_ctrl#pop_condition
    method set_condition c = file_ctrl#set_condition c
    method condition_stack_length = file_ctrl#condition_stack_length
    
    method store_macro name parms body = file_ctrl#store_macro name parms body
    method undef_macro name = file_ctrl#undef_macro name
    method get_macro name = file_ctrl#get_macro name
    method get_macros = file_ctrl#get_macros

    method add_macros (s:lexer_state) = 
      let h = self#get_macros in
      Hashtbl.iter 
      (fun k v -> 
        if Hashtbl.mem h k 
        then raise (Duplicate_macro k) 
        else Hashtbl.add h k v 
      )
      s#get_macros
      ;

     (* append new keywords *)
     let new_keywords = s#get_keywords in
     let n = Array.length new_keywords in
     if n > Array.length keywords then begin
       let old_keywords = keywords in
       keywords <- Array.make n [];
       Array.blit old_keywords 0 keywords 0 (Array.length old_keywords)
     end;
     for i = 0 to Array.length new_keywords - 1 do
       keywords.(i) <- new_keywords.(i) @ keywords.(i)
     done
     ;

     (* append new symbols *)
     let new_symbols = s#get_symbols in
     let n = Array.length new_symbols in
     if n > Array.length symbols then begin
       let old_symbols = symbols in
       symbols <- Array.make n [];
       Array.blit old_symbols 0 symbols 0 (Array.length old_symbols)
     end;
     for i = 0 to Array.length new_symbols - 1 do
       symbols.(i) <- new_symbols.(i) @ symbols.(i)
     done
     ;

     brackets <- s#get_brackets @ brackets 

     ;
     Hashtbl.iter
     (fun k ls ->
       let old = try Hashtbl.find nonterminals k with Not_found -> [] in
       Hashtbl.replace nonterminals k (ls @ old)
     )
     s#get_nonterminals

    method get_keywords = keywords

    method adjust_keyword_array n =
      let m = Array.length keywords in
      if m <= n then begin
        let a = Array.make (n+1) [] in
        Array.blit keywords 0 a 0 m;
        keywords <- a
      end

    method adjust_symbol_array n =
      let m = Array.length symbols in
      if m <= n then begin
        let a = Array.make (n+1) [] in
        Array.blit symbols 0 a 0 m;
        symbols <- a
      end

    method add_infix_symbol (prec:int) s f =
      let n = String.length s in
      self#adjust_symbol_array n;
      let elt = s,(fun (sr,_) -> Flx_parse.USER10 (sr,s,f)) in
      symbols.(n) <- elt :: symbols.(n)

    method add_infix_keyword (prec:int) s f =
      let n = String.length s in
      self#adjust_keyword_array n;
      let elt = s,(fun (sr,_) -> Flx_parse.USER10 (sr,s,f)) in
      keywords.(n) <- elt :: keywords.(n)
    
    method add_keyword (s:string) =
      let n = String.length s in
      self#adjust_keyword_array n;
      let elt = s,(fun (sr,_) -> Flx_parse.USER_KEYWORD (sr,s)) in
      keywords.(n) <- elt :: keywords.(n)
     
    method add_statement_keyword (s:string) (sr:range_srcref) (toks: Flx_parse.token list) (term:ast_term_t) =
      let n = String.length s in
      self#adjust_keyword_array n;
      let tokss =
        try match (assoc s keywords.(n)) (("",0,0,0), "")  with
          | Flx_parse.USER_STATEMENT_KEYWORD (_,_,tokss,_) -> (toks,term) :: tokss
          | _ -> clierr sr "Conflicting meaning of keyword s"
        with Not_found -> [toks,term]
      in
      let elt = s,(fun (sr,_) -> Flx_parse.USER_STATEMENT_KEYWORD (sr,s,tokss,nonterminals)) in
      keywords.(n) <- elt :: remove_assoc s keywords.(n)


    method add_nonterminal (s:string) (sr:range_srcref) (toks: Flx_parse.token list) (term:ast_term_t) =
      let productions = try Hashtbl.find nonterminals s with Not_found -> [] in
      Hashtbl.replace nonterminals s ((toks,term)::productions)

    method add_brackets tok1 tok2 f =
      let n1 = String.length tok1 in
      let n2 = String.length tok2 in
      let n = max n1 n2 in
      self#adjust_symbol_array n;
      brackets <- ((tok1,tok2),f) :: brackets;
      let rbs = 
        let rec aux fnmap brs = match brs with 
          | [] -> rev fnmap
          | ((l,r),f) :: t -> 
            if l = tok1 then aux ((r,f)::fnmap) t
            else aux fnmap t
        in aux [] brackets
      in
      let elt = tok1,(fun (sr,_) -> Flx_parse.USERLB (sr,rbs,tok1)) in
      symbols.(n1) <- elt :: symbols.(n1)
      ;
      let elt = tok2,(fun (sr,_) -> Flx_parse.USERRB (sr,tok2)) in
      symbols.(n1) <- elt :: symbols.(n2)
        
    method tokenise_symbols lexbuf (s:string) : token list =
      (* temporary hack *)
      let sr = self#get_srcref lexbuf in
      let rec tk tks s =
        let m = String.length s in
        let rec aux n =
          if n = 0 then (* cannot match even first char *)
           tk (ERRORTOKEN (sr,String.sub s 0 1)::tks) (String.sub s 1 (m-1)) 
          else
          let f = 
            try Some (assoc (String.sub s 0 n) symbols.(n))
            with Not_found -> None
          in
          match f with
          | None -> aux (n-1)
          | Some f -> 
            (* next token *)
            tk (f (sr,String.sub s 0 n) :: tks) (String.sub s n (m-n))
        in 
        let n = String.length s in
        if n = 0 then rev tks
        else aux (min n (Array.length symbols - 1))
      in 
        tk [] s
end

@h = tangler('src/flx_lexstate.mli')
@select(h)
open Flx_ast
open Flx_string
open Flx_parse

exception Duplicate_macro of string

class comment_control :
  object
    val mutable nesting_level : int
    val mutable text : string
    method append : string -> unit
    method decr : unit
    method get_comment : string
    method get_nesting_level : int
    method incr : unit
    method set_text : string -> unit
  end

type condition_t = [
 | `Processing
 | `Skip_to_endif
 | `Skip_to_else
 | `Subscan
]

type location = {    
    mutable buf_pos : int;
    mutable last_buf_pos : int;
    mutable line_no : int;
    mutable original_line_no : int;
}


class file_control :
  string ->
  string ->
  string list ->
  object
    val mutable loc: location
    val filename : string
    val mutable condition : condition_t list
    val macros : (string,string list * token list) Hashtbl.t 
    
    method get_loc : location
    method set_loc : location -> unit

    method get_buf_pos : int
    method get_srcref : Lexing.lexbuf -> srcref
    method get_physical_srcref : Lexing.lexbuf -> srcref
    method incr : int -> unit
    method incr_lex_counters : Lexing.lexbuf -> unit
    method set_buf_pos : int -> unit
    method set_line : int -> Lexing.lexbuf -> unit
    method set_filename : string -> unit
    method get_relative : string -> string
    method get_incdirs : string list
    method get_absolute : string -> string

    method get_condition : condition_t 
    method push_condition : condition_t -> unit
    method pop_condition : unit
    method set_condition : condition_t -> unit
    method condition_stack_length : int

    method store_macro : string -> string list -> token list -> unit
    method undef_macro : string -> unit
    method get_macro : string -> (string list * token list) option
    method get_macros : (string,string list * token list) Hashtbl.t 
  end

class lexer_state :
  string ->
  string ->
  string list ->
  (string -> expr_t->expr_t) ->
  object
    val expand_expr : string -> expr_t -> expr_t
    val comment_ctrl : comment_control
    val file_ctrl : file_control

    val mutable symbols :
      (string * (srcref * string -> token)) list array
    val mutable keywords:
      (string * (srcref * string -> token)) list array
    val mutable brackets: ((string * string) * string) list
    val nonterminals: (string, (token list *ast_term_t) list) Hashtbl.t
    val mutable include_files : string list

    method get_expand_expr : string -> expr_t -> expr_t

    method add_include_file : string -> unit
    method get_include_files : string list

    method append_comment : string -> unit
    method comment_level : int
    method decode : (string -> string) -> string -> string
    method decr_comment : unit
    method get_comment : string
    method get_srcref : Lexing.lexbuf -> srcref
    method get_physical_srcref : Lexing.lexbuf -> srcref
    method incr_comment : unit
    method newline : Lexing.lexbuf -> unit
    method set_comment : string -> unit
    method is_at_line_start : bool
    method inbody: unit
    method string_of_srcref : Lexing.lexbuf -> string
    method set_line : int -> Lexing.lexbuf-> unit
    method set_filename : string -> unit
    method get_incdirs : string list
    method get_relative : string -> string
    method get_absolute : string -> string

    method get_condition : condition_t 
    method push_condition : condition_t -> unit
    method pop_condition : unit
    method set_condition : condition_t -> unit
    method condition_stack_length : int
    method get_loc : location
    method set_loc : location -> unit

    method store_macro : string -> string list -> token list -> unit
    method undef_macro : string -> unit
    method get_macro : string -> (string list * token list) option
    method get_macros : (string,string list * token list) Hashtbl.t 
    method add_macros : lexer_state -> unit
    method adjust_symbol_array : int -> unit
    method add_infix_symbol:
      int -> string -> string -> unit

    method get_keywords: 
      (string * (srcref * string -> token)) list array

    method adjust_keyword_array : int -> unit

    method add_infix_keyword:
      int -> string -> string -> unit
    method add_keyword:
      string -> unit

    method get_brackets:
      ((string * string) * string) list

    method get_nonterminals:
      (string, (token list *ast_term_t) list) Hashtbl.t
    
    method get_symbols:
      (string * (srcref * string -> token)) list array
    
    method add_statement_keyword:
      string -> range_srcref -> token list -> ast_term_t -> unit

    method add_nonterminal:
      string -> range_srcref -> token list -> ast_term_t -> unit

    method add_brackets: string -> string -> string -> unit

    method tokenise_symbols : Lexing.lexbuf -> string -> token list
end

@h = tangler('src/flx_preproc.mli')
@select(h)
open Flx_ast
open Flx_parse
open Flx_lexstate
open Lexing

val is_in_string : string -> char -> bool
val is_white : char -> bool
val is_digit : char -> bool
val strip_us : string -> string

val pre_tokens_of_lexbuf : 
   (lexer_state -> lexbuf -> token list) ->
  lexbuf -> lexer_state -> 
  token list

val pre_tokens_of_string :
  (lexer_state -> lexbuf -> token list) ->
  string -> string -> 
  (string -> expr_t -> expr_t) ->
  token list

val line_directive :
  lexer_state -> range_srcref -> string ->  lexbuf ->
  token list

val include_directive :
  bool ->
  lexer_state -> range_srcref -> string ->
  (lexer_state -> lexbuf -> token list) ->
  token list

val handle_preprocessor :
  lexer_state -> lexbuf -> string ->
  (lexer_state -> lexbuf -> token list) ->
  location ->
  Lexing.position ->
  token list

@h = tangler('src/flx_preproc.ml')
@select(h)
open Flx_util
open Flx_parse
open Flx_string
open Big_int
open Flx_exceptions
open Flx_lexstate
open List

@if USE_DYPGEN:
  tangle(" (* only with dypgen! *) ")
  tangle("let dyphack (ls : ( 'a * Dyp.priority) list) : 'a =")
  tangle("  match ls with")
  tangle("  | [x,_] -> x")
  tangle('  | _ -> failwith "Dypgen parser failed"')
 else:
  tangle("let dyphack x = x")
@#

let dyphack_parser f tk lb = dyphack (f tk lb)

let substr = String.sub
let len = String.length

let is_in_string s ch =
  try 
    ignore(String.index s ch);
    true 
  with Not_found -> 
    false

let is_white = is_in_string " \t"
let is_digit = is_in_string "0123456789"

let strip_us s = 
  let n = String.length s in
  let x = Buffer.create n in
  for i=0 to n - 1 do
    match s.[i] with 
    | '_' -> ()
    | c -> Buffer.add_char x c
  done;
  Buffer.contents x


let pre_tokens_of_lexbuf lexer buf state =
  let rec get lst = 
    let ts = lexer state buf in 
    match ts with
    | [Flx_parse.ENDMARKER] -> lst
    | _ -> 
      match state#get_condition with
      | `Processing | `Subscan ->
        get (rev_append ts lst)
      | _ ->
        get lst
  in 
    rev (get [])

let pre_tokens_of_string lexer s filename expand_expr =
  let state = new lexer_state filename "" [] expand_expr in
  pre_tokens_of_lexbuf lexer (Lexing.from_string s) state

let line_directive state sr s lexbuf =
  let i = ref 0 in
  let a = 
    let a = ref 0 in
    while is_digit s.[!i] do
      a := !a * 10 + dec_char2int s.[!i];
      incr i
    done;
    !a
  in
  if !i = 0 
  then clierr sr "digits required after #line"
  else begin
    while is_white s.[!i] do incr i done;
    if s.[!i] <> '\n'
    then begin
      if s.[!i]<>'"'
      then clierr sr "double quote required after line number in #line"
      else begin
        incr i;
        let j = !i in
        while s.[!i]<>'"' && s.[!i]<>'\n' do incr i done;

        if s.[!i]='\n'
        then clierr sr "double quote required after filename in #line directive"
        else begin
          let filename = String.sub s j (!i-j) in
          state#set_filename filename;
          state#set_line a lexbuf
        end
      end
    end else begin
      (* print_endline ("SETTING LINE " ^ string_of_int a); *)
      state#set_line a lexbuf
    end
  end;
  [NEWLINE]


(* output expansion of input in reverse order with exclusions *)
let rec expand' state exclude toks =
  (* output expansion of input 
    in reverse order 
    with bindings and 
    with exclusions,
    this function is tail rec and used as a loop
  *)
  let rec aux exclude inp out bindings = 
    match inp with
    | [] -> out
    | h :: ts ->
      (* do not expand a symbol recursively *)
      if mem h exclude
      then aux exclude ts (h :: out) bindings
      else
        (* if it is a parameter name, replace by argument *)
        let b = 
          try Some (assoc h bindings)
          with Not_found -> None
        in match b with
        | Some x ->
          (* note binding body is in reverse order *)
          aux exclude ts (x @ out) bindings
        
        | None ->
        match h with
        | Flx_parse.NAME (sr,s) ->
          begin match state#get_macro s with
          (* not a macro : output it *)
          | None -> aux exclude ts (h :: out) bindings

          (* argumentless macro : output expansion of body,
            current bindings are ignored
          *)
          | Some ([], body) -> 
            let body = expand' state (h::exclude) body
            in aux exclude ts (body @ out) bindings
            
          | Some (params,body) ->
            failwith "Can't handle macros with arguments yet"
          end
        | _ -> aux exclude ts (h :: out) bindings

  in aux [] toks [] []

let eval state toks =
  let e = Flx_tok.parse_tokens (dyphack_parser Flx_parse.expression) (toks @ [ENDMARKER]) in
  let e = state#get_expand_expr "PREPROC_EVAL" e in
  e
  
let expand state toks = rev (expand' state [] toks)

let eval_bool state sr toks =
  let toks = expand state toks in
  let e = eval state toks in
  match e with
  | `AST_typed_case (sr,v,`TYP_unitsum 2) ->
    v = 1

  | x -> 
    clierr sr 
    (
      "Preprocessor constant expression of boolean type required\n" ^
      "Actually got:\n" ^
      Flx_print.string_of_expr x
    )

let rec parse_params sr toks = match toks with
  | NAME (_,id) :: COMMA _ :: ts ->
    let args, body = parse_params sr toks in
    id :: args, body

  | NAME (_,id) :: RPAR _ :: ts ->
    [id], ts

  | RPAR _ :: ts -> [], ts

  | h :: _ -> 
    let sr = Flx_srcref.slift (Flx_prelex.src_of_token h) in
    clierr sr "Malformed #define directive"
  | [] ->
    clierr sr "Malformed #define directive"
   
let parse_macro_function state sr name toks =
  let args, body = parse_params sr toks in
  state#store_macro name args body

let parse_macro_body state sr name toks =
  match toks with
  | LPAR _ :: ts -> parse_macro_function state sr name ts
  | _ -> state#store_macro name [] toks
  
let undef_directive state sr toks =
  iter
  begin function
  | NAME (sr,name) -> state#undef_macro name
  | h ->
    let sr = Flx_srcref.slift (Flx_prelex.src_of_token h) in
    clierr sr "#define requires identifier"
  end
  toks
  ;
  []
 
let define_directive state sr toks =
  match toks with
  | NAME (sr,name) :: ts -> 
    let sr = Flx_srcref.slift sr in
    begin match state#get_macro name with
    | None ->
      parse_macro_body state sr name ts; 
      []
    | Some _ -> clierr sr ("Duplicate Macro definition for " ^ name)
    end

  | h :: _ ->
    let sr = Flx_srcref.slift (Flx_prelex.src_of_token h) in
    clierr sr "#define requires identifier"
  | [] ->
    clierr sr "#define requires identifier"

let infix_directive state sr toks =
  match toks with
  | [INTEGER (sr1,kind,v); STRING (sr2,tok); NAME (sr3,fn)] ->
    if kind <> "int" then
      clierr sr "#infix directive requires plain integer precedence"
    ;
    let j = Big_int.int_of_big_int v in
    state#add_infix_symbol j tok fn;
    []

  | [INTEGER (sr1,kind,v); NAME (sr2,tok); NAME (sr3,fn)] ->
    if kind <> "int" then
      clierr sr "#infix directive requires plain integer precedence"
    ;
    let j = Big_int.int_of_big_int v in
    state#add_infix_keyword j tok fn;
    []

  (* as above, without the precedence integer -- we set it to 10.
     in the dypgen system it isn't relevant anyhow
  *)
  | [STRING (sr2,tok); NAME (sr3,fn)] ->
    state#add_infix_symbol 10 tok fn;
    []

  | [ NAME (sr2,tok); NAME (sr3,fn)] ->
    state#add_infix_keyword 10 tok fn;
    []


  (* as above, without the precedence integer or mapping function
     in the dypgen system it isn't relevant if the symbol is
     used as a keyword other than an operator.

     NOTE: such symbols are USER10 encoded not USER_KEYWORD encoded 
  *)
  | [STRING (sr2,tok)] ->
    state#add_infix_symbol 10 tok tok;
    []

  | [NAME (sr2,tok)] ->
    state#add_infix_keyword 10 tok tok;
    []

  | _ ->
    clierr sr "#infix directive has syntax #infix 99 \"..\" fname"

let keyword_directive state sr toks =
  let rec aux toks = match toks with
  | NAME (sr,tok) :: t ->
    state#add_keyword tok;
    aux t

  | [] -> []
  | _ ->
    clierr sr "#keyword directive has syntax #keyword id1 id2 ..."
  in aux toks

let action_split t =
  let rec aux inp out = match inp with
  | [] -> rev out, []
  | PARSE_ACTION _ :: tail -> rev out, tail
  | h :: t -> aux t (h::out)
  in aux t []

let statement_directive state sr toks =
  let toks = Flx_keywords.retok_parser_tokens toks in
  match toks with
  | NAME (sr,tok) :: t 
  | USER_STATEMENT_KEYWORD (sr,tok,_,_) :: t ->
    (*
    print_endline ("Statement directive " ^ tok);
    *)
    let t1,t2 = action_split t in
    let sts,_ =
      match t2 with 
      | [] -> [],ENDMARKER
      | _ -> Flx_tok.parse_tokens (dyphack_parser Flx_parse.statementsx) (t2 @ [ENDMARKER])
    in
    (*
    print_endline ("Action Statements " ^ catmap "\n" (Flx_print.string_of_statement 0) sts);
    *)
    state#add_statement_keyword tok (Flx_srcref.slift sr) t1 (`Statements_term sts);
    []

  | _ ->
    clierr sr "#statement directive has syntax #statement kw production"

let nonterminal_directive state sr toks =
  let toks = Flx_keywords.retok_parser_tokens toks in
  match toks with
  | NAME (sr,tok) :: t ->
    (*
    print_endline ("Adding nonterminal .." ^ tok);
    *)
    let t1,t2 = action_split t in
    (*
    print_endline ("Action Tokens: " ^ catmap ", " Flx_prelex.string_of_token t2);
    *)
    let expr = Flx_tok.parse_tokens (dyphack_parser Flx_parse.expression) (t2 @ [ENDMARKER]) in
    state#add_nonterminal tok (Flx_srcref.slift sr) t1 (`Expression_term expr);
    []

  | _ ->
    clierr sr "#nonterminal has syntax #nonterminal name production"

let bracket_directive state sr toks =
  match toks with
  | [STRING (sr1,tok1); STRING (sr2,tok2); NAME (sr3,fn)] ->
    state#add_brackets tok1 tok2 fn;
    []

  | _ ->
    clierr sr "#bracket directive has syntax #bracket \"lb\" \"rb\" fname"
  
let if_directive state sr toks =
  state#push_condition 
  (
    match eval_bool state sr toks with
    | true -> `Processing
    | false -> `Skip_to_else
  )
  ;
  []

let ifdef_directive state sr toks =
  begin match toks with
  | NAME (sr,s) :: _ ->
    begin match state#get_macro s with
    | None -> state#push_condition `Skip_to_else
    | Some _ -> state#push_condition `Processing
    end
  | _ -> clierr sr "#ifdef requires identifier"
  end
  ;
  []

let ifndef_directive state sr toks =
  begin match toks with
  | NAME (sr,s) :: _ ->
    begin match state#get_macro s with
    | None -> state#push_condition `Processing
    | Some _ -> state#push_condition `Skip_to_else
    end
  | _ -> clierr sr "#ifndef requires identifier"
  end
  ; 
  []

let else_directive state sr =
  begin match state#get_condition with
  | `Processing -> state#set_condition `Skip_to_endif
  | `Skip_to_endif -> ()
  | `Skip_to_else -> state#set_condition `Processing
  | `Subscan -> syserr sr "unexpected else while subscanning"
  end
  ;
  []

let elif_directive state sr toks =
  begin match state#get_condition with
  | `Processing -> state#set_condition `Skip_to_endif
  | `Skip_to_endif -> ()
  | `Skip_to_else -> 
    state#set_condition 
    (
      match eval_bool state sr toks with
      | true -> `Processing
      | false -> `Skip_to_else
    )
  | `Subscan -> syserr sr "unexpected elif while subscanning"
  end
  ;
  []


let endif_directive state sr =
  if state#condition_stack_length < 2
  then 
    clierr sr "Unmatched endif"
  else
    state#pop_condition;
    []

let find_include_file state s sr =
  if s.[0]<>'"' && s.[0]<>'<'
  then clierr sr "'\"' or '<' required after #include"
  ;
  let rquote = if s.[0]='"' then '"' else '>' in
  let i = ref 1 in
  let j = !i in
  while s.[!i]<>rquote && s.[!i]<>'\n' do incr i done
  ;

  if s.[!i]='\n'
  then clierr sr "double quote required after filename in #include directive"
  ;
  let filename = String.sub s j (!i-j) in
  let filename=
    if rquote = '"'
    then state#get_relative filename 
    else state#get_absolute filename
  in
    (* 
      print_endline (
      "//Resolved in path: \"" ^ filename ^ "\""
    );
    *)
    filename

let include_directive is_import state sr s pre_flx_lex =
  let filename = find_include_file state s sr in
  state#add_include_file filename;
  let pre_tokens_of_filename filename =
    let incdirs = state#get_incdirs in
    let basedir = Filename.dirname filename in
    let state' = new lexer_state filename basedir incdirs state#get_expand_expr in
    let infile = open_in filename in
    let src = Lexing.from_channel infile in
    let toks = pre_tokens_of_lexbuf pre_flx_lex src state' in
      close_in infile; 
      if is_import then begin
        try state#add_macros state'
        with Duplicate_macro k -> clierr sr 
        ("Duplicate Macro " ^ k ^ " imported")
      end;
      iter state#add_include_file state'#get_include_files;
      toks
   in
   pre_tokens_of_filename filename

let count_newlines s =
  let n = ref 0 in
  let len = ref 0 in
  let last_len = ref 0 in
  for i = 0 to String.length s - 1 do 
    if s.[i] = '\n' then begin incr n; last_len := !len; len := 0; end
    else incr len
  done;
  !n,!last_len

let handle_preprocessor state lexbuf s pre_flx_lex start_location start_position =
  let linecount,last_line_len = count_newlines s in
  let file,line1,col1,_ = state#get_srcref lexbuf in
  let file',line1',_,_ = state#get_physical_srcref lexbuf in

  let next_line = line1+linecount in
  let next_line' = line1'+linecount in
  let sr = file,line1,col1,next_line-1,last_line_len+1 in
  let sr' = file',line1',col1,next_line'-1,last_line_len+1 in
  let saved_buf_pos = Lexing.lexeme_end lexbuf in
  (*
  print_endline ("PREPROCESSING: " ^ Flx_srcref.long_string_of_src sr);
  print_endline ("Trailing buf pos = " ^ si saved_buf_pos);
  *)
  let ident,s' = 

    (* .. note the string WILL end with a newline .. *)

    (* skip spaces *)
    let i = ref 0 in
    while is_white s.[!i] && (s.[!i] <> '\n') do incr i done; 

    (* scan non-spaces, stop at #, white, or newline *)
    let n = ref 0 in 
    while 
      not (is_white s.[!i + !n]) &&       
      not (s.[!i + !n]='\n') &&
      not (s.[!i + !n]='#')
    do incr n done;

    (* grab the preprocessor directive name *)
    let ident = String.sub s !i !n in     

    (* scan for next non-white *)
    let j = ref (!i + !n) in
    while is_white s.[!j] && (s.[!j] <> '\n') do incr j done;

    (* scan back from end of text for last non-white *)
    n := String.length s - 1;
    while !n > !j && is_white(s.[!n-1]) do decr n done;

    (* grab the text from after the directive name to the end *)
    let ssl = !n - !j in
    let rest = String.sub s !j ssl in
    ident,rest
  in

  (*
  print_endline ("PREPRO i=" ^ ident^", t='"^s'^"',\ns='"^s^"'");
  *)
  match ident with

  (* THESE COMMANDS ARE WEIRD HANGOVERS FROM C WHICH
     CANNOT HANDLE NORMAL TOKENISATION
  *)
  (* print a warning *)
  | "error" -> 
    begin match state#get_condition with
    | `Processing ->
      print_endline ("#error " ^ s');
      clierr2 sr sr' ("#error " ^ s')
    | _ -> []
    end

  | "warn" -> 
    let result =
      match state#get_condition with
      | `Processing ->
        let desc = Flx_srcref.short_string_of_src sr in
          print_endline desc
        ;
        if sr <> sr' then begin
          let desc = Flx_srcref.short_string_of_src sr' in
          print_endline ("Physical File:\n" ^ desc)
        end
        ;
        print_endline ("#warn " ^ s');
        print_endline "";
        [NEWLINE]
      | _ -> []
    in 
      for i = 1 to linecount do state#newline lexbuf done;
      result
     
  | "line" ->
    line_directive state sr s' lexbuf

  | "include" 
  | "import" -> 
    let result =
      let is_import = ident = "import" in
      match state#get_condition with
      | `Processing ->
        include_directive is_import state sr s' pre_flx_lex
      | _ -> []
    in
     for i = 1 to linecount do state#newline lexbuf done;
     result

  (* THESE ONES USE ORDINARY TOKEN STREAM *)
  | _ ->
  let result =
    let src = Lexing.from_string s in
    (*
    print_endline ("Start buf pos = " ^ si (start_position.Lexing.pos_cnum)); 
    print_endline ("Start loc = " ^ si (start_location.buf_pos));
    *)
    state#push_condition `Subscan;

    (* hack the location to the start of the line *)
    let b = start_location.buf_pos - start_position.Lexing.pos_cnum in
    (*
    print_endline ("Hacking column position to " ^ si b);
    *)
    state#set_loc { 
      buf_pos = b; 
      last_buf_pos = b; 
      line_no = line1;
      original_line_no = line1';
    };

    let toks = pre_tokens_of_lexbuf pre_flx_lex src state in
   
    state#pop_condition;

    (* use the special preprocessor token filter *)
    let toks = Flx_lex1.translate_preprocessor toks in

    (*
    iter (fun tok ->
      let sr = Flx_srcref.slift (Flx_prelex.src_of_token tok) in
      print_endline (Flx_srcref.long_string_of_src sr)
    )
    toks;
    *)
   
    match toks with
    | [] -> [] (* DUMMY *)
    | h :: toks ->
    let h = Flx_prelex.string_of_token h in
    if h <> ident then
      failwith (
        "WOOPS, mismatch on directive name: ident=" ^ 
        ident ^ ", head token = " ^ 
        h
      )
    ;
    match h with

    (* conditional compilation *)
    | "if" -> if_directive state sr toks
    | "ifdef" -> ifdef_directive state sr toks
    | "ifndef" -> ifndef_directive state sr toks
    | "else" -> else_directive state sr
    | "elif" -> elif_directive state sr toks
    | "endif" -> endif_directive state sr
     
    | _ -> match state#get_condition with
    | `Skip_to_else
    | `Skip_to_endif -> []
    | `Subscan -> syserr sr "Unexpected preprocessor directive in subscan"

    (* these ones are only done if in processing mode *)
    | `Processing ->
    match h with

    | "define" -> 
        define_directive state sr toks

    | "undef" -> 
        undef_directive state sr toks

   
    | "symbol" 
    | "infix" ->
        infix_directive state sr toks

    | "keyword" ->
        keyword_directive state sr toks

    | "statement" ->
        statement_directive state sr toks

    | "nonterminal" ->
        nonterminal_directive state sr toks

    | "bracket" ->
        bracket_directive state sr toks

    | _ -> 
      print_endline (state#string_of_srcref lexbuf);
      print_endline 
      (
        "LEXICAL ERROR: IGNORING UNKNOWN PREPROCESSOR DIRECTIVE \"" ^
        ident ^ "\""
      );
      [NEWLINE]
  in 

  (* restore the location to the start of the next line *)
  state#set_loc { 
    buf_pos = saved_buf_pos; 
    last_buf_pos = saved_buf_pos; 
    line_no = next_line;
    original_line_no = next_line'
  };
  result


@h = tangler('src/flx_lex.mll','data')
@select(h)
{
open Flx_util
open Flx_parse
open Flx_string
open Big_int
open Flx_exceptions
open Flx_lexstate
open Flx_preproc

let lexeme = Lexing.lexeme
let lexeme_start = Lexing.lexeme_start
let lexeme_end = Lexing.lexeme_end

let substr = String.sub
let len = String.length

(* string parsers *)
let decode_qstring s = let n = len s in unescape (substr s 0 (n-1)) 
let decode_dstring s = let n = len s in unescape (substr s 0 (n-1)) 
let decode_qqqstring s = let n = len s in unescape (substr s 0 (n-3)) 
let decode_dddstring s = let n = len s in unescape (substr s 0 (n-3)) 

let decode_raw_qstring s = let n = len s in substr s 0 (n-1) 
let decode_raw_dstring s = let n = len s in substr s 0 (n-1) 
let decode_raw_qqqstring s = let n = len s in substr s 0 (n-3) 
let decode_raw_dddstring s = let n = len s in substr s 0 (n-3)

exception Ok of int
exception SlashSlash of int
exception SlashAst of int

(* WARNING: hackery: adjust this when lex expression 'white'
   is adjutsed
*)

} 

(* ====================== REGULAR DEFINITIONS ============================ *)
(* special characters *)
let quote = '\''
let dquote = '"'
let slosh = '\\'
let linefeed = '\n'
let tab = '\t'
let space = ' '
let formfeed = '\012'
let vtab = '\011'
let carriage_return = '\013'
let underscore = '_'

(* character sets *)
let bindigit = ['0'-'1']
let octdigit = ['0'-'7'] 
let digit = ['0'-'9']
let hexdigit = digit | ['A'-'F'] | ['a'-'f']
let lower = ['a'-'z']
let upper = ['A'-'Z']
(* let letter = lower | upper *)
let letter = lower | upper
let hichar = ['\128'-'\255']
let white = space | tab

(* nasty: form control characters *)
let form_control = linefeed | carriage_return | vtab | formfeed
let newline_prefix = linefeed | carriage_return
let newline = formfeed | linefeed  | carriage_return linefeed
let hash = '#'

let ordinary = letter | digit | hichar |
  '!' | '$' | '%' | '&' | '(' | ')' | '*' |
  '+' | ',' | '-' | '.' | '/' | ':' | ';' | '<' |
  '=' | '>' | '?' | '@' | '[' | ']' | '^' | '_' |
  '`' | '{' | '|' | '}' | '~'

(* any sequence of these characters makes one or more tokens *)
(* MISSING: # should be in here, but can't be supported atm
  because preprocessor # uses a conditional, and just errors
  out if the # isn't at the start of a line .. needs fixing,
  not sure how to fix it
*)

let symchar =
  '!' | '$' | '%' | '&' | '(' | ')' | '*' |
  '+' | ',' | '-' | '.' | '/' | ':' | ';' | '<' |
  '=' | '>' | '?' | '@' | '[' | ']' | '^' |
  '`' | '{' | '|' | '}' | '~' | '#' | '\\'

let printable = ordinary | quote | dquote | slosh | hash

(* identifiers *)
let ucn = 
    "\\u" hexdigit hexdigit hexdigit hexdigit 
  | "\\U" hexdigit hexdigit hexdigit hexdigit hexdigit hexdigit hexdigit hexdigit

let prime = '\''
let idletter = letter | underscore | hichar | ucn
let identifier = idletter (idletter | digit | prime )* 

(* integers *)
let bin_lit  = '0' ('b' | 'B') (underscore? bindigit) +
let oct_lit  = '0' ('o' | 'O') (underscore? octdigit) +
let dec_lit  = ('0' ('d' | 'D'| "d_" | "D_"))? digit (underscore? digit) *
let hex_lit  = '0' ('x' | 'X') (underscore? hexdigit)  +
let fastint_type_suffix = 't'|'T'|'s'|'S'|'i'|'I'|'l'|'L'|'v'|'V'|"ll"|"LL"
let exactint_type_suffix =
    "i8" | "i16" | "i32" | "i64"
  | "u8" | "u16" | "u32" | "u64"
  | "I8" | "I16" | "I32" | "I64"
  | "U8" | "U16" | "U32" | "U64"

let signind = 'u' | 'U'

let suffix = 
    '_'? exactint_type_suffix 
  | ('_'? fastint_type_suffix)? ('_'? signind)? 
  | ('_'? signind)? ('_'? fastint_type_suffix)?

let int_lit = (bin_lit | oct_lit | dec_lit | hex_lit) suffix

(* floats: Follows ISO C89, except that we allow underscores *)
let decimal_string = digit (underscore? digit) *
let hexadecimal_string = hexdigit (underscore? hexdigit) *

let decimal_fractional_constant = 
  decimal_string '.' decimal_string?
  | '.' decimal_string
  
let hexadecimal_fractional_constant = 
  ("0x" |"0X")
  (hexadecimal_string '.' hexadecimal_string?
  | '.' hexadecimal_string)

let decimal_exponent = ('E'|'e') ('+'|'-')? decimal_string
let binary_exponent = ('P'|'p') ('+'|'-')? decimal_string

let floating_suffix = 'L' | 'l' | 'F' | 'f' | 'D' | 'd'
let floating_literal = 
  (
    decimal_fractional_constant decimal_exponent? |
    hexadecimal_fractional_constant binary_exponent?
  )
  floating_suffix?

(* Python strings *)
let qqq = quote quote quote
let ddd = dquote dquote dquote 

let escape = slosh _ 

let dddnormal = ordinary | hash | quote | escape | white | newline
let dddspecial = dddnormal | dquote dddnormal | dquote dquote dddnormal

let qqqnormal = ordinary | hash | dquote | escape | white | newline
let qqqspecial = qqqnormal | quote qqqnormal | quote quote qqqnormal

let raw_dddnormal = ordinary | hash | quote | slosh | white | newline
let raw_dddspecial = raw_dddnormal | dquote raw_dddnormal | dquote dquote raw_dddnormal

let raw_qqqnormal = ordinary | hash | dquote | slosh | space | newline
let raw_qqqspecial = raw_qqqnormal | quote raw_qqqnormal | quote quote raw_qqqnormal

let qstring = (ordinary | hash | dquote | escape | white) * quote
let dstring = (ordinary | hash | quote | escape | white) * dquote
let qqqstring = qqqspecial * qqq
let dddstring = dddspecial * ddd

let raw = 'r' | 'R'
let see = 'c' | 'C'
let rqc = raw see | see raw

let raw_qstring = (ordinary | hash | dquote | escape | white) * quote
let raw_dstring =  (ordinary | hash | quote | escape | white) * dquote

let raw_qqqstring = raw_qqqspecial * qqq
let raw_dddstring = raw_dddspecial * ddd

let not_hash_or_newline = ordinary | quote | dquote | white | slosh
let not_newline = not_hash_or_newline | hash
let quoted_filename = dquote (ordinary | hash | quote | white | slosh)+ dquote

(* ====================== PARSERS ============================ *)
(* string lexers *)

(* ----------- BASIC STRING -----------------------------------*)

rule parse_qstring state = parse
| qstring { 
      state#inbody;
      [STRING (
        state#get_srcref lexbuf, 
        state#decode decode_qstring (lexeme lexbuf)
      )] 
  }
| _ { 
    [ERRORTOKEN (
      state#get_srcref lexbuf, 
      "' string"
    )] 
  }

and parse_dstring state = parse
| dstring {
      state#inbody;
      [STRING (
        state#get_srcref lexbuf, 
        state#decode decode_dstring (lexeme lexbuf)
      )] 
  }
| _ { 
    state#inbody; 
    [ERRORTOKEN (
      state#get_srcref lexbuf, 
      "\" string"
    )]
  }

and parse_qqqstring state = parse
| qqqstring { 
      state#inbody;
      [STRING (
        state#get_srcref lexbuf, 
        state#decode decode_qqqstring (lexeme lexbuf)
      )] 
  }
| _ { 
    state#inbody;
    [ERRORTOKEN (
      state#get_srcref lexbuf, 
      "''' string"
    )] 
  }

and parse_dddstring state = parse
| dddstring { 
      state#inbody;
      [STRING (
        state#get_srcref lexbuf,
        state#decode decode_dddstring (lexeme lexbuf)
      )] 
  }
| _ { 
    state#inbody;
    [ERRORTOKEN (
      state#get_srcref lexbuf,
      "\"\"\" string"
    )] 
  }

(* ----------- FORMAT STRING -----------------------------------*)
and parse_fqstring state = parse
| qstring { 
      state#inbody;
      [FSTRING (
        state#get_srcref lexbuf, 
        state#decode decode_qstring (lexeme lexbuf)
      )] 
  }
| _ { 
    [ERRORTOKEN (
      state#get_srcref lexbuf, 
      "' string"
    )] 
  }

and parse_fdstring state = parse
| dstring {
      state#inbody;
      [FSTRING (
        state#get_srcref lexbuf, 
        state#decode decode_dstring (lexeme lexbuf)
      )] 
  }
| _ { 
    state#inbody; 
    [ERRORTOKEN (
      state#get_srcref lexbuf, 
      "\" string"
    )]
  }

and parse_fqqqstring state = parse
| qqqstring { 
      state#inbody;
      [FSTRING (
        state#get_srcref lexbuf, 
        state#decode decode_qqqstring (lexeme lexbuf)
      )] 
  }
| _ { 
    state#inbody;
    [ERRORTOKEN (
      state#get_srcref lexbuf, 
      "''' string"
    )] 
  }

and parse_fdddstring state = parse
| dddstring { 
      state#inbody;
      [FSTRING (
        state#get_srcref lexbuf,
        state#decode decode_dddstring (lexeme lexbuf)
      )] 
  }
| _ { 
    state#inbody;
    [ERRORTOKEN (
      state#get_srcref lexbuf,
      "\"\"\" string"
    )] 
  }

(* ----------- INTERPOLATION STRING -----------------------------------*)
and parse_Qqstring state = parse
| qstring { 
      state#inbody;
      [QSTRING (
        state#get_srcref lexbuf, 
        state#decode decode_qstring (lexeme lexbuf)
      )] 
  }
| _ { 
    [ERRORTOKEN (
      state#get_srcref lexbuf, 
      "' string"
    )] 
  }

and parse_Qdstring state = parse
| dstring {
      state#inbody;
      [QSTRING (
        state#get_srcref lexbuf, 
        state#decode decode_dstring (lexeme lexbuf)
      )] 
  }
| _ { 
    state#inbody; 
    [ERRORTOKEN (
      state#get_srcref lexbuf, 
      "\" string"
    )]
  }

and parse_Qqqqstring state = parse
| qqqstring { 
      state#inbody;
      [QSTRING (
        state#get_srcref lexbuf, 
        state#decode decode_qqqstring (lexeme lexbuf)
      )] 
  }
| _ { 
    state#inbody;
    [ERRORTOKEN (
      state#get_srcref lexbuf, 
      "''' string"
    )] 
  }

and parse_Qdddstring state = parse
| dddstring { 
      state#inbody;
      [QSTRING (
        state#get_srcref lexbuf,
        state#decode decode_dddstring (lexeme lexbuf)
      )] 
  }
| _ { 
    state#inbody;
    [ERRORTOKEN (
      state#get_srcref lexbuf,
      "\"\"\" string"
    )] 
  }

(* ----------- C STRING -----------------------------------*)
and parse_cqstring state = parse
| qstring { 
      state#inbody;
      [CSTRING (
        state#get_srcref lexbuf, 
        state#decode decode_qstring (lexeme lexbuf)
      )] 
  }
| _ { 
    [ERRORTOKEN (
      state#get_srcref lexbuf, 
      "' string"
    )] 
  }

and parse_cdstring state = parse
| dstring {
      state#inbody;
      [CSTRING (
        state#get_srcref lexbuf, 
        state#decode decode_dstring (lexeme lexbuf)
      )] 
  }
| _ { 
    state#inbody; 
    [ERRORTOKEN (
      state#get_srcref lexbuf, 
      "\" string"
    )]
  }

and parse_cqqqstring state = parse
| qqqstring { 
      state#inbody;
      [CSTRING (
        state#get_srcref lexbuf, 
        state#decode decode_qqqstring (lexeme lexbuf)
      )] 
  }
| _ { 
    state#inbody;
    [ERRORTOKEN (
      state#get_srcref lexbuf, 
      "''' string"
    )] 
  }

and parse_cdddstring state = parse
| dddstring { 
      state#inbody;
      [CSTRING (
        state#get_srcref lexbuf,
        state#decode decode_dddstring (lexeme lexbuf)
      )] 
  }
| _ { 
    state#inbody;
    [ERRORTOKEN (
      state#get_srcref lexbuf,
      "\"\"\" string"
    )] 
  }

(* ----------- WIDE STRING -----------------------------------*)
and parse_wqstring state = parse
| qstring { 
      state#inbody;
      [WSTRING (
        state#get_srcref lexbuf, 
        state#decode decode_qstring (lexeme lexbuf)
      )] 
  }
| _ { 
    [ERRORTOKEN (
      state#get_srcref lexbuf, 
      "' string"
    )] 
  }

and parse_wdstring state = parse
| dstring {
      state#inbody;
      [WSTRING (
        state#get_srcref lexbuf, 
        state#decode decode_dstring (lexeme lexbuf)
      )] 
  }
| _ { 
    state#inbody; 
    [ERRORTOKEN (
      state#get_srcref lexbuf, 
      "\" string"
    )]
  }

and parse_wqqqstring state = parse
| qqqstring { 
      state#inbody;
      [WSTRING (
        state#get_srcref lexbuf, 
        state#decode decode_qqqstring (lexeme lexbuf)
      )] 
  }
| _ { 
    state#inbody;
    [ERRORTOKEN (
      state#get_srcref lexbuf, 
      "''' string"
    )] 
  }

and parse_wdddstring state = parse
| dddstring { 
      state#inbody;
      [WSTRING (
        state#get_srcref lexbuf,
        state#decode decode_dddstring (lexeme lexbuf)
      )] 
  }
| _ { 
    state#inbody;
    [ERRORTOKEN (
      state#get_srcref lexbuf,
      "\"\"\" string"
    )] 
  }

(* ----------- UNICODE STRING -----------------------------------*)
and parse_uqstring state = parse
| qstring { 
      state#inbody;
      [WSTRING (
        state#get_srcref lexbuf, 
        state#decode decode_qstring (lexeme lexbuf)
      )] 
  }
| _ { 
    [ERRORTOKEN (
      state#get_srcref lexbuf, 
      "' string"
    )] 
  }

and parse_udstring state = parse
| dstring {
      state#inbody;
      [USTRING (
        state#get_srcref lexbuf, 
        state#decode decode_dstring (lexeme lexbuf)
      )] 
  }
| _ { 
    state#inbody; 
    [ERRORTOKEN (
      state#get_srcref lexbuf, 
      "\" string"
    )]
  }

and parse_uqqqstring state = parse
| qqqstring { 
      state#inbody;
      [USTRING (
        state#get_srcref lexbuf, 
        state#decode decode_qqqstring (lexeme lexbuf)
      )] 
  }
| _ { 
    state#inbody;
    [ERRORTOKEN (
      state#get_srcref lexbuf, 
      "''' string"
    )] 
  }

and parse_udddstring state = parse
| dddstring { 
      state#inbody;
      [USTRING (
        state#get_srcref lexbuf,
        state#decode decode_dddstring (lexeme lexbuf)
      )] 
  }
| _ { 
    state#inbody;
    [ERRORTOKEN (
      state#get_srcref lexbuf,
      "\"\"\" string"
    )] 
  }

(* ----------- RAW STRING -----------------------------------*)
and parse_raw_qstring state = parse
| raw_qstring { 
      state#inbody;
      [STRING (
        state#get_srcref lexbuf,
        state#decode decode_raw_qstring (lexeme lexbuf)
      )] 
  }
| _ { 
    state#inbody;
    [ERRORTOKEN (
     state#get_srcref lexbuf,
    "raw ' string")] 
  }

and parse_raw_dstring state = parse
| raw_dstring { 
      state#inbody;
      [STRING (
        state#get_srcref lexbuf,
        state#decode decode_raw_dstring (lexeme lexbuf)
      )]
  }
| _ { 
    state#inbody;
    [ERRORTOKEN (
      state#get_srcref lexbuf,
        "raw \" string"
    )]
  }

and parse_raw_qqqstring state = parse
| raw_qqqstring { 
      state#inbody;
      [STRING (
        state#get_srcref lexbuf,
        state#decode decode_raw_qqqstring (lexeme lexbuf)
      )]
  }
| _ { state#inbody; 
  [ERRORTOKEN (
    state#get_srcref lexbuf,
    "raw ''' string")] }

and parse_raw_dddstring state = parse
| raw_dddstring { 
      state#inbody;
      [STRING (
        state#get_srcref lexbuf,
        state#decode decode_raw_dddstring (lexeme lexbuf)
      )] 
  }
| _ { 
     [ERRORTOKEN (
       state#get_srcref lexbuf,
       lexeme lexbuf)
     ] 
   }

and parse_raw_cqstring state = parse
| raw_qstring { 
      state#inbody;
      [CSTRING (
        state#get_srcref lexbuf,
        state#decode decode_raw_qstring (lexeme lexbuf)
      )] 
  }
| _ { 
    state#inbody;
    [ERRORTOKEN (
     state#get_srcref lexbuf,
    "raw ' cstring")] 
  }

and parse_raw_cdstring state = parse
| raw_dstring { 
      state#inbody;
      [STRING (
        state#get_srcref lexbuf,
        state#decode decode_raw_dstring (lexeme lexbuf)
      )]
  }
| _ { 
    state#inbody;
    [ERRORTOKEN (
      state#get_srcref lexbuf,
        "raw \" cstring"
    )]
  }

and parse_raw_cqqqstring state = parse
| raw_qqqstring { 
      state#inbody;
      [CSTRING (
        state#get_srcref lexbuf,
        state#decode decode_raw_qqqstring (lexeme lexbuf)
      )]
  }
| _ { state#inbody; 
  [ERRORTOKEN (
    state#get_srcref lexbuf,
    "raw ''' cstring")] }

and parse_raw_cdddstring state = parse
| raw_dddstring { 
      state#inbody;
      [CSTRING (
        state#get_srcref lexbuf,
        state#decode decode_raw_dddstring (lexeme lexbuf)
      )] 
  }
| _ { 
     [ERRORTOKEN (
       state#get_srcref lexbuf,
       lexeme lexbuf)
     ] 
   }

and parse_hashbang state = parse 
| not_newline * newline {
    begin
      state#newline lexbuf;
      let lex = lexeme lexbuf in
      let n = String.length lex in
      [COMMENT_NEWLINE  (String.sub lex 0 (n-1))]
    end
  }
| _ { [ERRORTOKEN (
        state#get_srcref lexbuf,
  lexeme lexbuf)] }

and parse_C_comment state = parse 
| "/*" { 
      state#append_comment (lexeme lexbuf);
      state#incr_comment; 
      parse_C_comment state lexbuf
  }
| newline {
      state#newline lexbuf;
      state#append_comment (lexeme lexbuf);
      parse_C_comment state lexbuf
  }
| "*/" { 
      state#append_comment (lexeme lexbuf); 
      state#decr_comment; 
      if state#comment_level > 0 
      then parse_C_comment state lexbuf 
      else ()
      ;
      state#inbody
  }
| _ {
      state#append_comment (lexeme lexbuf);
      parse_C_comment state lexbuf 
  }

and parse_line state = parse
| not_newline * (newline | eof)
  { 
    state#newline lexbuf;
    lexeme lexbuf
  }

and parse_preprocessor state start_location start_position = parse
| ( not_newline* slosh space* newline)* not_newline* newline 
| ( not_newline* hash space* newline) (not_hash_or_newline not_newline* newline)+
  {
    let toks = handle_preprocessor state lexbuf 
      (lexeme lexbuf) pre_flx_lex start_location start_position
    in
    toks
  }


and pre_flx_lex state = parse
(* eof is not eaten up, so parent will find eof and emit ENDMARKER *)
| "//" not_newline * (newline | eof) {
      state#newline lexbuf;
      let lex = lexeme lexbuf in
      let n = String.length lex in
      [COMMENT_NEWLINE  (String.sub lex 2 (n-3))]
  }

| "/*" { 
      state#set_comment (lexeme lexbuf);
      parse_C_comment state lexbuf; 
      [COMMENT (state#get_comment)]
  }

| int_lit { 
      state#inbody;
      let sr = state#get_srcref lexbuf in
      let s = lexeme lexbuf in
      let n = String.length s in
      let converter, first =
        if n>1 && s.[0]='0' 
        then
          match s.[1] with
          | 'b' | 'B' -> binbig_int_of_string,2
          | 'o' | 'O' -> octbig_int_of_string,2
          | 'd' | 'D' -> decbig_int_of_string,2
          | 'x' | 'X' -> hexbig_int_of_string,2
          | _         -> decbig_int_of_string,0
        else decbig_int_of_string,0
      in
      let k = ref (n-1) in
      let t =
        if n >= 2 && s.[n-2]='i' && s.[n-1]='8'
        then (k:=n-2; "int8")
        else if n >= 2 && s.[n-2]='u' && s.[n-1]='8'
        then (k:=n-2; "uint8")
        else if n >= 3 && s.[n-3]='i' && s.[n-2]='1' && s.[n-1]='6'
        then (k:=n-3; "int16")
        else if n >= 3 && s.[n-3]='u' && s.[n-2]='1' && s.[n-1]='6'
        then (k:=n-3; "uint16")

        else if n >= 3 && s.[n-3]='i' && s.[n-2]='3' && s.[n-1]='2'
        then (k:=n-3; "int32")
        else if n >= 3 && s.[n-3]='u' && s.[n-2]='3' && s.[n-1]='2'
        then (k:=n-3; "uint32")

        else if n >= 3 && s.[n-3]='i' && s.[n-2]='6' && s.[n-1]='4'
        then (k:=n-3; "int64")
        else if n >= 3 && s.[n-3]='u' && s.[n-2]='6' && s.[n-1]='4'
        then (k:=n-3; "uint64")

        else begin
          let sign = ref "" in
          let typ = ref "int" in
          begin try while !k>first do 
            (match s.[!k] with  
            | 'u' | 'U' -> sign := "u"
            | 't' | 'T' -> typ := "tiny"
            | 's' | 'S' -> typ := "short"
            | 'i' | 'I' -> typ := "int"
            | 'l' | 'L' -> 
              typ := 
                if !typ = "long" then "vlong" else "long"
            | 'v' | 'V' -> typ := "vlong"
            | _ -> raise Not_found
            );
            decr k
          done with _ -> () end;
          incr k;
          !sign ^ !typ
        end
      in
      let d = String.sub s first (!k-first) in
      let v = (converter d) in
        [INTEGER (sr, t, v)]
  }

| floating_literal { 
    state#inbody;
    let str = lexeme lexbuf in
    let n = String.length str in
    let last_char = str.[n-1] in
    match last_char with
    | 'l'|'L' ->
      [FLOAT (state#get_srcref lexbuf,"ldouble", strip_us (String.sub str 0 (n-1)))]
    | 'f'|'F' ->
      [FLOAT (state#get_srcref lexbuf,"float",strip_us (String.sub str 0 (n-1)))]
    | _ ->
      [FLOAT (state#get_srcref lexbuf,"double",strip_us str)]
  }

(* Python strings *)
| quote  { state#inbody; parse_qstring state lexbuf }
| qqq    { state#inbody; parse_qqqstring state lexbuf }
| dquote { state#inbody; parse_dstring state lexbuf }
| ddd    { state#inbody; parse_dddstring state lexbuf }

(* C strings: type char*  *)
| ('c'|'C') quote  { state#inbody; parse_cqstring state lexbuf }
| ('c'|'C') qqq    { state#inbody; parse_cqqqstring state lexbuf }
| ('c'|'C') dquote { state#inbody; parse_cdstring state lexbuf }
| ('c'|'C') ddd    { state#inbody; parse_cdddstring state lexbuf }

(* Format strings *)
| ('f'|'F') quote  { state#inbody; parse_fqstring state lexbuf }
| ('f'|'F') qqq    { state#inbody; parse_fqqqstring state lexbuf }
| ('f'|'F') dquote { state#inbody; parse_fdstring state lexbuf }
| ('f'|'F') ddd    { state#inbody; parse_fdddstring state lexbuf }

(* interpolated strings *)
| ('q'|'Q') quote  { state#inbody; parse_Qqstring state lexbuf }
| ('q'|'Q') qqq    { state#inbody; parse_Qqqqstring state lexbuf }
| ('q'|'Q') dquote { state#inbody; parse_Qdstring state lexbuf }
| ('q'|'Q') ddd    { state#inbody; parse_Qdddstring state lexbuf }

(* wide strings *)
| ('w' | 'W') quote  { state#inbody; parse_wqstring state lexbuf }
| ('w' | 'W') qqq    { state#inbody; parse_wqqqstring state lexbuf }
| ('w' | 'W') dquote { state#inbody; parse_wdstring state lexbuf }
| ('w' | 'W') ddd    { state#inbody; parse_wdddstring state lexbuf }

(* UTF32 strings *)
| ('u' | 'U') quote  { state#inbody; parse_uqstring state lexbuf }
| ('u' | 'U') qqq    { state#inbody; parse_uqqqstring state lexbuf }
| ('u' | 'U') dquote { state#inbody; parse_udstring state lexbuf }
| ('u' | 'U') ddd    { state#inbody; parse_udddstring state lexbuf }

(* Python raw strings *)
| ('r'|'R') quote  { state#inbody; parse_raw_qstring state lexbuf }
| ('r'|'R') qqq    { state#inbody; parse_raw_qqqstring state lexbuf }
| ('r'|'R') dquote { state#inbody; parse_raw_dstring state lexbuf }
| ('r'|'R') ddd    { state#inbody; parse_raw_dddstring state lexbuf }

(* raw C strings: type char*  *)
| rqc quote  { state#inbody; parse_cqstring state lexbuf }
| rqc qqq    { state#inbody; parse_cqqqstring state lexbuf }
| rqc dquote { state#inbody; parse_cdstring state lexbuf }
| rqc ddd    { state#inbody; parse_cdddstring state lexbuf }

(* this MUST be after strings, so raw strings take precedence
  over identifiers, eg r'x' is a string, not an identifier,
  but x'x' is an identifier .. yucky ..
*)
| identifier { 
      state#inbody;
      let s = lexeme lexbuf in
      let s' = Flx_id.utf8_to_ucn s in
      let src = state#get_srcref lexbuf in
      try [
        let keywords = state#get_keywords in
        let n = String.length s' in
        if n >= Array.length keywords then raise Not_found;
        let keywords = keywords.(n) in
        (List.assoc s' keywords) (src,s')
      ]
      with Not_found ->
      [Flx_keywords.map_flx_keywords src s']
  } 

(* whitespace *)
| white + { 
      (* we do NOT say 'inbody' here: we want to accept
         #directives with leading spaces
      *)
      let spaces=lexeme lexbuf in
      let column = ref 0 in
      let n = String.length spaces in
      for i=0 to n-1 do match spaces.[i] with
        | '\t' -> column := ((!column + 8) / 8) * 8
        | ' ' -> incr column
        | _ -> raise (Failure "Error in lexer, bad white space character")
      done;
      [WHITE  (!column)]
  }

| slosh { [SLOSH] } 

| symchar + { 
    let s = lexeme lexbuf in
    let n = String.length s in
    let s',con,lim =
      try
        for i = 0 to n - 1 do
          if s.[i] = '/' && i+1<n then begin
            if s.[i+1] = '/' then raise (SlashSlash i);
            if s.[i+1] = '*' then raise (SlashAst i)
          end
        done;
        raise (Ok n)
      with 
      | SlashSlash i -> String.sub s 0 i,`SlashSlash,i
      | SlashAst i -> String.sub s 0 i,`SlashAst,i
      | Ok i -> String.sub s 0 i,`Ok,i
    in 
    let atstart = state#is_at_line_start in
    state#inbody; 
    let toks = state#tokenise_symbols  lexbuf s' in
    let toks =
      match toks,atstart with
      | [HASH _],true -> 
        let x = state#get_loc in
        let y = lexbuf.Lexing.lex_curr_p in
        parse_preprocessor state 
          { x with buf_pos = x.buf_pos } 
          { y with Lexing.pos_fname = y.Lexing.pos_fname }
          lexbuf
      | [HASHBANG _ | HASHBANGSLASH _ ],true  -> 
        (*
        print_endline "IGNORING HASHBANG";
        *)
        parse_hashbang state lexbuf
      | _ when con = `SlashSlash ->
        (*
        print_endline "EMBEDDED //";
        *)
        let lead = String.sub s (lim+2) (n-lim-2) in
        let lex = parse_line state lexbuf in
        let m = String.length lex in
        toks @ [COMMENT_NEWLINE  (lead ^ String.sub lex 0 (m-1))]

      | _ when con = `SlashAst ->
        (*
        print_endline "EMBEDDED /*";
        *)
        (* NOTE THIS WILL NOT HANDLE /**/ or any other
          sequence x/*xxxx*/ where the x's are special
          In particular x/***************/ will fail.
        *)
        let lead = String.sub s (lim+2) (n-lim-2) in
        state#set_comment lead;
        parse_C_comment state lexbuf; 
        toks @ [COMMENT (state#get_comment)]

      | _ -> toks
    in toks 
  }

(* end of line *)
| newline {
      state#newline lexbuf; 
      [NEWLINE ]
  }

(* end of file *)
| eof { 
  if state#get_condition = `Subscan then [ENDMARKER] else
  if state#condition_stack_length <> 1
  then 
    let sr = state#get_srcref lexbuf in
    let sr = Flx_srcref.slift sr in
    Flx_exceptions.clierr sr "Unmatched #if at end of file"
  else
    [ENDMARKER] 
  }
  
(* Anything else is an error *)
| _ { 
    state#inbody; 
    [
      ERRORTOKEN 
      (
        state#get_srcref lexbuf, lexeme lexbuf
      )
    ]
  }

{
}

@h = tangler('src/flx_lex.mli')
@select(h)
val pre_flx_lex : 
  Flx_lexstate.lexer_state -> 
  Lexing.lexbuf -> 
  Flx_parse.token list

val parse_line : 
  Flx_lexstate.lexer_state -> 
  Lexing.lexbuf -> 
  string

