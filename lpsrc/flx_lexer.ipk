@head(1,'Lexer')
@h = tangler('src/flx_prelex.mli')
@select(h)
val src_of_token : Flx_parse.token -> Flx_ast.srcref
val string_of_token : Flx_parse.token -> string
@h = tangler('src/flx_prelex.ml')
@select(h)
open Flx_parse

let string_of_string s = "\"" ^  Flx_string.c_quote_of_string s ^ "\""

let string_of_token (tok :Flx_parse.token): string =
  match tok with
  | NAME (sr,s) -> s
  | INTEGER (sr,t,i) -> Big_int.string_of_big_int i
  | FLOAT (sr,t,v) -> v
  | STRING (sr,s) -> Flx_string.c_quote_of_string s 
  | CSTRING (sr,s) -> Flx_string.c_quote_of_string s 
  | WSTRING (sr,s) -> Flx_string.c_quote_of_string s 
  | USTRING (sr,s) -> Flx_string.c_quote_of_string s 

  (* one character tokens *)
  | DOLLAR _ -> "$" 
  | QUEST _ -> "?" 
  | EXCLAMATION _ -> "!" 
  | LPAR _ -> "(" 
  | RPAR _ -> ")" 
  | LSQB _ -> "["
  | RSQB _ -> "]"
  | LBRACE _ -> "{"
  | RBRACE _ -> "}"
  | COLON _ -> ":"
  | COMMA _ -> ","
  | SEMI _ -> ";"
  | PLUS _ -> "+"
  | MINUS _ -> "-"
  | STAR _ -> "*"
  | SLASH _ -> "/"
  | VBAR _ -> "|"
  | AMPER _ -> "&"
  | LESS _ -> "<"
  | GREATER _ -> ">"
  | EQUAL _ -> "="
  | DOT _ -> "."
  | PERCENT _ -> "%"

  | BACKQUOTE _ -> "`"
  | TILDE _ -> "~"
  | CIRCUMFLEX _ -> "^"

  (* two character tokens *)
  | ANDLESS _ -> "&<"
  | ANDGREATER _ -> "&>"
  | EQEQUAL _ -> "=="
  | NOTEQUAL _ -> "!="
  | LESSEQUAL _ -> "<="
  | GREATEREQUAL _ -> ">="
  | LEFTSHIFT _ -> "<<"
  | RIGHTSHIFT _ -> ">>"
  | DOUBLESTAR _ -> "**"
  | STARSTAR _ -> "<POW>**"
  | LESSCOLON _ -> "<:"
  | COLONGREATER _ -> ":>"
  | DOTDOT _ -> ".."
  | COLONCOLON _ -> "::"

  | PLUSPLUS _ -> "++"
  | MINUSMINUS _ -> "--"
  | PLUSEQUAL _ -> "+="
  | MINUSEQUAL _ -> "-="
  | STAREQUAL _ -> "*="
  | SLASHEQUAL _ -> "/="
  | PERCENTEQUAL _ -> "%="
  | CARETEQUAL _ -> "^="
  | VBAREQUAL _ -> "|="
  | AMPEREQUAL _ -> "&="
  | TILDEEQUAL _ -> "~="
  | COLONEQUAL _ -> ":="
  | RIGHTARROW _ -> "->"
  | EQRIGHTARROW _ -> "=>"
  | LEFTARROW _ -> "<-"
  | LSQANGLE _ -> "[<"
  | RSQANGLE _ -> ">]"

  | LSQBAR _ -> "[<"
  | RSQBAR _ -> ">]"

  | LEFTSHIFTEQUAL _ -> "<<="
  | RIGHTSHIFTEQUAL _ -> ">>="
  | LEFTRIGHTARROW _ -> "<->"
  | ANDEQEQUAL _ -> "&=="
  | ANDNOTEQUAL _ -> "&!="
  | ANDLESSEQUAL _ -> "&<="
  | ANDGREATEREQUAL _ -> "&>="
  | DOTDOTDOT _ -> "..."

  (* keywords *)
@for n,t in flx_keywords: tangle('  |  '+t+' _ -> "'+n+'"')

  (* special things *)
  | SLOSH _ -> "\\" (* "\"" *) 

  | COMMENT s -> s (* C style comment, includes the /* */ pair *)
  | COMMENT_NEWLINE s -> "// " ^ s ^ "<NEWLINE>"
  | WHITE i -> String.make i ' '
  | NEWLINE -> "<NEWLINE>"
  | ENDMARKER -> "<<EOF>>"
  | ERRORTOKEN (sref,s) -> "<<ERROR '"^ s ^"'>>"

let src_of_token t = match t with
  | NEWLINE 
  | COMMENT _
  | COMMENT_NEWLINE _
  | WHITE _ 
  | ENDMARKER
    -> ("",0,0,0)

  | NAME    (s,_)
  | INTEGER (s,_,_)
  | FLOAT   (s,_,_)
  | STRING  (s,_)
  | CSTRING  (s,_)
  | WSTRING  (s,_)
  | USTRING  (s,_)

  | ERRORTOKEN (s,_) 

@for n,t in flx_keywords: tangle('  | '+t+' s')

  | EQUAL s 
  | PLUS s 
  | MINUS s 
  | STAR s 
  | SLASH s 
  | PERCENT s 
  | STARSTAR s 
  | AMPER s 
  | VBAR s 
  | EXCLAMATION s 
  | DOLLAR s 
  | QUEST s 
  | LPAR s 
  | RPAR s 
  | LSQB s 
  | RSQB s 
  | LBRACE s 
  | RBRACE s 
  | LSQANGLE s 
  | RSQANGLE s 
  | LSQBAR s 
  | RSQBAR s 
  | COLON s 
  | COMMA s 
  | SEMI s 
  | DOT s 
  | BACKQUOTE s 
  | LESS s 
  | GREATER s 
  | EQEQUAL s 
  | NOTEQUAL s 
  | LESSEQUAL s 
  | GREATEREQUAL s 
  | ANDLESS s 
  | ANDGREATER s 
  | ANDEQEQUAL s 
  | ANDNOTEQUAL s 
  | ANDLESSEQUAL s 
  | ANDGREATEREQUAL s 
  | TILDE s 
  | CIRCUMFLEX s 
  | LEFTSHIFT s 
  | RIGHTSHIFT s 
  | LEFTRIGHTARROW s 
  | DOUBLESTAR s 
  | PLUSPLUS s 
  | MINUSMINUS s 
  | PLUSEQUAL s 
  | MINUSEQUAL s 
  | STAREQUAL s 
  | SLASHEQUAL s 
  | PERCENTEQUAL s 
  | CARETEQUAL s 
  | VBAREQUAL s 
  | AMPEREQUAL s 
  | TILDEEQUAL s 
  | COLONEQUAL s 
  | LEFTSHIFTEQUAL s 
  | RIGHTSHIFTEQUAL s 
  | LEFTARROW s 
  | RIGHTARROW s 
  | EQRIGHTARROW s 
  | COLONGREATER s 
  | LESSCOLON s 
  | COLONCOLON s 
  | DOTDOT s 
  | SLOSH s 
  | DOTDOTDOT s 
    -> s

@h = tangler('src/flx_lexstate.ml','data')
@select(h)
open Flx_util
open Flx_parse
open Flx_string
open Big_int
open Flx_exceptions

exception Duplicate_macro of string

class comment_control = 
  object (self)
    val mutable nesting_level = 0
    val mutable text = ""

    method set_text s = text <- s; nesting_level <- 1
    method append s = text <- text ^ s
    method get_comment = text

    method incr = nesting_level <- nesting_level + 1
    method decr = nesting_level <- nesting_level - 1
    method get_nesting_level = nesting_level
  end

exception Found_file of string

type condition_t = [
 | `Processing
 | `Skip_to_endif
 | `Skip_to_else
]

class file_control 
  (filename' : string) 
  (basedir': string) 
  (incdirs': string list) 
= 
  object(self)
    val mutable buf_pos =  0
    val mutable last_buf_pos =  0
    val mutable line_no =  1
    val original_filename = filename'
    val incdirs = incdirs'
    val basedir = basedir'
    val mutable filename = filename'
    val mutable condition:condition_t list = [`Processing]
    val macros : (string,string list * Flx_parse.token list) Hashtbl.t = Hashtbl.create 97
    
    method incr_lex_counters lexbuf =
      line_no <- line_no + 1;
      last_buf_pos <- buf_pos;
      buf_pos <- Lexing.lexeme_end lexbuf

    method set_buf_pos x = buf_pos <- x
    method get_buf_pos = buf_pos
    method get_srcref lexbuf = 
      filename, 
      line_no, 
      Lexing.lexeme_start lexbuf - buf_pos + 1, 
      Lexing.lexeme_end lexbuf - buf_pos 

    method incr n = line_no <- line_no + n

    method set_line n = line_no <- n
    method set_filename f = filename <- f
    method get_relative f = 
      Filename.concat basedir f
    method get_absolute f = 
      try
        List.iter
        (fun d -> 
          let f = Filename.concat d f in 
          if Sys.file_exists f 
          then raise (Found_file f)
        )
        incdirs
        ;
        failwith ("Library File <" ^ f ^ "> not found in path")
      with Found_file s -> s 

    method store_macro name params body = 
      Hashtbl.add macros name (params,body)
      
    method undef_macro name = Hashtbl.remove macros name

    method get_macro name = 
      try Some (Hashtbl.find macros name)
      with Not_found -> None
    
    method get_macros = macros

    method get_incdirs = incdirs
    method get_condition = List.hd condition
    method push_condition c =  condition <- (c :: condition)
    method pop_condition = condition <- List.tl condition
    method set_condition c = condition <- (c :: List.tl condition)
    method condition_stack_length = List.length condition
  end

class lexer_state filename basedir incdirs (ocs_env':Ocs_types.env) = 
  object (self)
    val ocs_env = ocs_env'
    val comment_ctrl = new comment_control
    val file_ctrl = new file_control filename basedir incdirs 
    val mutable at_line_start = true

    method get_ocs_env = ocs_env

    method is_at_line_start = at_line_start

    method inbody = at_line_start <- false
    method get_srcref lexbuf = file_ctrl#get_srcref lexbuf
    method string_of_srcref lexbuf =
      match self#get_srcref lexbuf with
      (filename, lineno, scol,ecol) ->
      "File \"" ^ filename ^ "\"" ^
      ", Line " ^ string_of_int lineno ^
      ", Columns " ^ string_of_int scol ^
      "-" ^ string_of_int ecol

    (* comments *)
    method comment_level = comment_ctrl#get_nesting_level
    method incr_comment = comment_ctrl#incr
    method decr_comment = comment_ctrl#decr

    method set_comment text = comment_ctrl#set_text text
    method append_comment text = comment_ctrl#append text
    method get_comment = comment_ctrl#get_comment

    (* line counting *)
    method newline lexbuf = 
      at_line_start <- true;
      file_ctrl#incr_lex_counters lexbuf

    method adj n = file_ctrl#incr n

    (* string decoders *)
    method decode decoder (s : string) : string = 
      let lfcount s = 
        let n = ref 0 in
        for i = 0 to (String.length s) - 1 do
          if s.[i] = '\n' then incr n
        done;
        !n
      in 
        file_ctrl#incr (lfcount s); 
        decoder s

    method set_line n = file_ctrl#set_line n
    method set_filename f = file_ctrl#set_filename f

    method get_incdirs = file_ctrl#get_incdirs
    method get_relative f = file_ctrl#get_relative f
    method get_absolute f = file_ctrl#get_absolute f
    
    method get_condition = file_ctrl#get_condition 
    method push_condition c = file_ctrl#push_condition c
    method pop_condition = file_ctrl#pop_condition
    method set_condition c = file_ctrl#set_condition c
    method condition_stack_length = file_ctrl#condition_stack_length
    
    method store_macro name parms body = file_ctrl#store_macro name parms body
    method undef_macro name = file_ctrl#undef_macro name
    method get_macro name = file_ctrl#get_macro name
    method get_macros = file_ctrl#get_macros
    method add_macros (s:lexer_state) = 
      let h = self#get_macros in
      Hashtbl.iter 
      (fun k v -> 
        if Hashtbl.mem h k 
        then raise (Duplicate_macro k) 
        else Hashtbl.add h k v 
      )
      s#get_macros
end

@h = tangler('src/flx_lexstate.mli','data')
@select(h)
open Flx_ast
open Flx_string

exception Duplicate_macro of string

class comment_control :
  object
    val mutable nesting_level : int
    val mutable text : string
    method append : string -> unit
    method decr : unit
    method get_comment : string
    method get_nesting_level : int
    method incr : unit
    method set_text : string -> unit
  end

type condition_t = [
 | `Processing
 | `Skip_to_endif
 | `Skip_to_else
]

class file_control :
  string ->
  string ->
  string list ->
  object
    val mutable buf_pos : int
    val filename : string
    val mutable last_buf_pos : int
    val mutable line_no : int
    val mutable condition : condition_t list
    val macros : (string,string list * Flx_parse.token list) Hashtbl.t 
    
    method get_buf_pos : int
    method get_srcref : Lexing.lexbuf -> srcref
    method incr : int -> unit
    method incr_lex_counters : Lexing.lexbuf -> unit
    method set_buf_pos : int -> unit
    method set_line : int -> unit
    method set_filename : string -> unit
    method get_relative : string -> string
    method get_incdirs : string list
    method get_absolute : string -> string

    method get_condition : condition_t 
    method push_condition : condition_t -> unit
    method pop_condition : unit
    method set_condition : condition_t -> unit
    method condition_stack_length : int

    method store_macro : string -> string list -> Flx_parse.token list -> unit
    method undef_macro : string -> unit
    method get_macro : string -> (string list * Flx_parse.token list) option
    method get_macros : (string,string list * Flx_parse.token list) Hashtbl.t 
  end

class lexer_state :
  string ->
  string ->
  string list ->
  Ocs_types.env ->
  object
    val ocs_env : Ocs_types.env
    val comment_ctrl : comment_control
    val file_ctrl : file_control

    method get_ocs_env : Ocs_types.env
    method adj : int -> unit
    method append_comment : string -> unit
    method comment_level : int
    method decode : (string -> string) -> string -> string
    method decr_comment : unit
    method get_comment : string
    method get_srcref : Lexing.lexbuf -> srcref
    method incr_comment : unit
    method newline : Lexing.lexbuf -> unit
    method set_comment : string -> unit
    method is_at_line_start : bool
    method inbody: unit
    method string_of_srcref : Lexing.lexbuf -> string
    method set_line : int -> unit
    method set_filename : string -> unit
    method get_incdirs : string list
    method get_relative : string -> string
    method get_absolute : string -> string

    method get_condition : condition_t 
    method push_condition : condition_t -> unit
    method pop_condition : unit
    method set_condition : condition_t -> unit
    method condition_stack_length : int

    method store_macro : string -> string list -> Flx_parse.token list -> unit
    method undef_macro : string -> unit
    method get_macro : string -> (string list * Flx_parse.token list) option
    method get_macros : (string,string list * Flx_parse.token list) Hashtbl.t 
    method add_macros : lexer_state -> unit
end

@h = tangler('src/flx_preproc.mli')
@select(h)
open Flx_ast
open Flx_parse
open Flx_lexstate
open Lexing

val is_in_string : string -> char -> bool
val is_white : char -> bool
val is_digit : char -> bool
val strip_us : string -> string

val pre_tokens_of_lexbuf : 
   (lexer_state -> lexbuf -> token list) ->
  lexbuf -> lexer_state -> 
  token list

val pre_tokens_of_string :
  (lexer_state -> lexbuf -> token list) ->
  string -> string -> 
  token list

(*
val line_directive :
  lexer_state -> range_srcref -> string -> 
  lexbuf -> 
  token list

val undef_directive :
  lexer_state -> range_srcref -> string -> 
  (lexer_state -> lexbuf -> token list) -> 
  token list

val define_directive :
  lexer_state -> range_srcref -> string -> 
  (lexer_state -> lexbuf -> token list) -> 
  token list

val if_directive :
  lexer_state -> range_srcref -> string ->
  (lexer_state -> lexbuf -> token list) ->
  token list

val ifdef_directive :
  lexer_state -> range_srcref -> string -> 
  token list

val ifndef_directive :
  lexer_state -> range_srcref -> string -> 
  token list

val else_directive :
  lexer_state -> range_srcref -> string -> 
  token list

val elif_directive :
  lexer_state -> range_srcref -> string ->
  (lexer_state -> lexbuf -> token list) ->
  token list

val endif_directive :
  lexer_state -> range_srcref -> string -> 
  token list

val divert_directive :
  lexer_state ->
  range_srcref ->
  string ->
  (lexer_state -> lexbuf -> token list) ->
  token list

val undivert_directive :
  lexer_state ->
  range_srcref ->
  string ->
  (lexer_state -> lexbuf -> token list) ->
  token list

val include_directive :
  bool ->
  lexer_state -> range_srcref -> string ->
  (lexer_state -> lexbuf -> token list) ->
  'a -> token list

val scheme_file_directive :
  lexer_state -> range_srcref -> string ->
  (lexer_state -> lexbuf -> token list) ->
  'a -> token list

val scheme_string_directive :
  lexer_state ->
  range_srcref ->
  string ->
  (lexer_state -> lexbuf -> token list) -> 'b -> token list
*)

val handle_preprocessor :
  lexer_state -> lexbuf -> string ->
  (lexer_state -> lexbuf -> token list) ->
  (lexer_state -> lexbuf -> string) ->
  token list

@h = tangler('src/flx_preproc.ml','data')
@select(h)
open Flx_util
open Flx_parse
open Flx_string
open Big_int
open Flx_exceptions
open Flx_lexstate
open Flx_preproc

let substr = String.sub
let len = String.length

let is_in_string s ch =
  try 
    ignore(String.index s ch);
    true 
  with Not_found -> 
    false

let is_white = is_in_string " \t"
let is_digit = is_in_string "0123456789"

let strip_us s = 
  let n = String.length s in
  let x = Buffer.create n in
  for i=0 to n - 1 do
    match s.[i] with 
    | '_' -> ()
    | c -> Buffer.add_char x c
  done;
  Buffer.contents x


let pre_tokens_of_lexbuf lexer buf state =
  let rec get lst = 
    let ts = lexer state buf in 
    match ts with
    | [Flx_parse.ENDMARKER] -> lst
    | _ -> 
      match state#get_condition with
      | `Processing ->
        get (List.rev_append ts lst)
      | _ ->
        get lst
  in 
    List.rev (get [])

let pre_tokens_of_string lexer s filename =
  let e = Ocs_top.make_env() in
  let state = new lexer_state filename "" [] e in
  pre_tokens_of_lexbuf lexer (Lexing.from_string s) state



let line_directive state sr s lexbuf =
  let i = ref 0 in
  let a = 
    let a = ref 0 in
    while is_digit s.[!i] do
      a := !a * 10 + dec_char2int s.[!i];
      incr i
    done;
    !a
  in
  if !i = 0 
  then clierr sr "digits required after #line"
  else begin
    while is_white s.[!i] do incr i done;
    if s.[!i] <> '\n'
    then begin
      if s.[!i]<>'"'
      then clierr sr "double quote required after line number in #line"
      else begin
        incr i;
        let j = !i in
        while s.[!i]<>'"' && s.[!i]<>'\n' do incr i done;

        if s.[!i]='\n'
        then clierr sr "double quote required after filename in #line directive"
        else begin
          let filename = String.sub s j (!i-j) in
          state#set_filename filename;
          state#set_line (a-1)
        end
      end
    end else begin
      (* print_endline ("SETTING LINE " ^ string_of_int a); *)
      state#set_line a
    end
  end;
  [NEWLINE]


(* output expansion of input in reverse order with exclusions *)
let rec expand' state exclude toks =
  (* output expansion of input 
    in reverse order 
    with bindings and 
    with exclusions,
    this function is tail rec and used as a loop
  *)
  let rec aux exclude inp out bindings = 
    match inp with
    | [] -> out
    | h :: ts ->
      (* do not expand a symbol recursively *)
      if List.mem h exclude
      then aux exclude ts (h :: out) bindings
      else
        (* if it is a parameter name, replace by argument *)
        let b = 
          try Some (List.assoc h bindings)
          with Not_found -> None
        in match b with
        | Some x ->
          (* note binding body is in reverse order *)
          aux exclude ts (x @ out) bindings
        
        | None ->
        match h with
        | Flx_parse.NAME (sr,s) ->
          begin match state#get_macro s with
          (* not a macro : output it *)
          | None -> aux exclude ts (h :: out) bindings

          (* argumentless macro : output expansion of body,
            current bindings are ignored
          *)
          | Some ([], body) -> 
            let body = expand' state (h::exclude) body
            in aux exclude ts (body @ out) bindings
            
          | Some (params,body) ->
            failwith "Can't handle macros with arguments yet"
          end
        | _ -> aux exclude ts (h :: out) bindings

  in aux [] toks [] []

let eval toks =
  let e = Flx_tok.parse_tokens Flx_parse.expression toks in
  (*
  print_endline ("Evaluated expression as " ^ Flx_print.string_of_expr e);
  *)
  let e = Flx_macro.expand_expression "PREPROC_EVAL" e in
  (*
  print_endline ("Folded expression is    " ^ Flx_print.string_of_expr e);
  *)
  e
  
let expand state toks = List.rev (expand' state [] toks)

let eval_bool state lexer sr s =
  (*
  print_endline("Evaluating bool from string '" ^ s ^"'");
  *)
  let toks = pre_tokens_of_string lexer s "DUMMY FILE NAME" in
  (*
  print_endline "Tokenised";
  *)
  let toks = List.rev (ENDMARKER :: List.rev toks) in
  (*
  print_endline "PreTokens = "; Flx_tok.print_pre_tokens toks;
  *)
  let toks = Flx_lex1.translate toks in
  (*
  print_endline "Pretoken stream translated to token stream";
  print_endline "Tokens = "; Flx_tok.print_tokens toks;
  *)
  let toks = expand state toks in
  (*
  print_endline "Token stream expanded";
  print_endline "Tokens = "; Flx_tok.print_tokens toks;
  *)
  let e = eval toks in
  (*
  print_endline "Tokens parsed as expression and folded to constant";
  print_endline ("Value is " ^ Flx_print.string_of_expr e);
  *)
  match e with
  | `AST_typed_case (sr,v,`TYP_unitsum 2) ->
    (* NOTE: the type case term is 1 origin, even though the
      generated code is 0 origain
    *)
    v = 2

  | x -> 
    clierr sr 
    (
      "Preprocessor constant expression of boolean type required\n" ^
      "Actually got:\n" ^
      Flx_print.string_of_expr x
    )

let rec parse_params sr toks = match toks with
  | NAME (_,id) :: COMMA _ :: ts ->
    let args, body = parse_params sr toks in
    id :: args, body

  | NAME (_,id) :: RPAR _ :: ts ->
    [id], ts

  | RPAR _ :: ts -> [], ts

  | h :: _ -> 
    let sr = Flx_srcref.slift (Flx_prelex.src_of_token h) in
    clierr sr "Malformed #define directive"
  | [] ->
    clierr sr "Malformed #define directive"
   
let parse_macro_function state sr name toks =
  let args, body = parse_params sr toks in
  state#store_macro name args body

let parse_macro_body state sr name toks =
  match toks with
  | LPAR _ :: ts -> parse_macro_function state sr name ts
  | _ -> state#store_macro name [] toks
  
let undef_directive state sr s lexer =
  let src = Lexing.from_string s in
  let toks = pre_tokens_of_lexbuf lexer src state in
  let toks = Flx_lex1.translate toks in
  List.iter
  begin function
  | NAME (sr,name) -> state#undef_macro name
  | h ->
    let sr = Flx_srcref.slift (Flx_prelex.src_of_token h) in
    clierr sr "#define requires identifier"
  end
  toks
  ;
  []
 
let define_directive state sr s lexer =
  let src = Lexing.from_string s in
  let toks = pre_tokens_of_lexbuf lexer src state in
  let toks = Flx_lex1.translate toks in
  match toks with
  | NAME (sr,name) :: ts -> 
    let sr = Flx_srcref.slift sr in
    begin match state#get_macro name with
    | None ->
      parse_macro_body state sr name ts; 
      []
    | Some _ -> clierr sr ("Duplicate Macro definition for " ^ name)
    end

  | h :: _ ->
    let sr = Flx_srcref.slift (Flx_prelex.src_of_token h) in
    clierr sr "#define requires identifier"
  | [] ->
    clierr sr "#define requires identifier"
  
let if_directive state sr s lexer =
  state#push_condition 
  (
    match eval_bool state lexer sr s with
    | true -> `Processing
    | false -> `Skip_to_else
  )
  ;
  []

let ifdef_directive state sr s =
  begin match state#get_macro s with
  | None -> state#push_condition `Skip_to_else
  | Some _ -> state#push_condition `Processing
  end
  ;
  []

let ifndef_directive state sr s =
  begin match state#get_macro s with
  | None -> state#push_condition `Processing
  | Some _ -> state#push_condition `Skip_to_else
  end
  ; 
  []

let else_directive state sr s =
  begin match state#get_condition with
  | `Processing -> state#set_condition `Skip_to_endif
  | `Skip_to_endif -> ()
  | `Skip_to_else -> state#set_condition `Processing
  end
  ;
  []

let elif_directive state sr s lexer =
  begin match state#get_condition with
  | `Processing -> state#set_condition `Skip_to_endif
  | `Skip_to_endif -> ()
  | `Skip_to_else -> 
    state#set_condition 
    (
      match eval_bool state lexer sr s with
      | true -> `Processing
      | false -> `Skip_to_else
    )
  end
  ;
  []


let endif_directive state sr lexbuf =
  if state#condition_stack_length < 2
  then 
    clierr sr "Unmatched endif"
  else
    state#pop_condition;
    []

let get_ident_opt cmd state sr s lexer =
  let toks = pre_tokens_of_string lexer s "DUMMY FILE NAME" in
  let toks = List.rev (ENDMARKER :: List.rev toks) in
  let toks = Flx_lex1.translate toks in
  match toks with
  | NAME (sr,name) :: [ENDMARKER] -> Some name
  | [ENDMARKER] ->  None
  | _ ->
    clierr sr ("#" ^ cmd ^ " requires identifier")
 
let divert_directive state sr s lexer =
  begin match get_ident_opt "divert" state sr s lexer with
  | Some name -> print_endline ("Divert " ^ name)
  | None -> print_endline "Divert <none>"
  end
  ;
  []

let undivert_directive state sr s lexer =
  begin match get_ident_opt "undivert" state sr s lexer with
  | Some name -> print_endline ("UNDivert " ^ name)
  | None -> print_endline "UNDivert <none>"
  end
  ;
  []

let find_include_file state s sr =
  if s.[0]<>'"' && s.[0]<>'<'
  then clierr sr "'\"' or '<' required after #include"
  ;
  let rquote = if s.[0]='"' then '"' else '>' in
  let i = ref 1 in
  let j = !i in
  while s.[!i]<>rquote && s.[!i]<>'\n' do incr i done
  ;

  if s.[!i]='\n'
  then clierr sr "double quote required after filename in #include directive"
  ;
  let filename = String.sub s j (!i-j) in
  let filename=
    if rquote = '"'
    then state#get_relative filename 
    else state#get_absolute filename
  in
    (* 
      print_endline (
      "//Resolved in path: \"" ^ filename ^ "\""
    );
    *)
    filename

let include_directive is_import state sr s pre_flx_lex lexbuf =
  let filename = find_include_file state s sr in
  let pre_tokens_of_filename filename =
  let incdirs = state#get_incdirs in
  let basedir = Filename.dirname filename in
  let ocs_env = state#get_ocs_env in
  let state' = new lexer_state filename basedir incdirs ocs_env in
  let infile = open_in filename in
  let src = Lexing.from_channel infile in
  let toks = pre_tokens_of_lexbuf pre_flx_lex src state' in
    close_in infile; 
    if is_import then begin
      try state#add_macros state'
      with Duplicate_macro k -> clierr sr 
      ("Duplicate Macro " ^ k ^ " imported")
    end;
    toks

 in
   pre_tokens_of_filename filename

let scheme_file_directive state sr s pre_flx_lex lexbuf =
  let filename = find_include_file state s sr in
  let basedir = Filename.dirname filename in
  let ocs_env = state#get_ocs_env in
  let state' = new lexer_state filename basedir state#get_incdirs ocs_env in
  let th = Ocs_top.make_thread () in
  let sp = Ocs_port.string_output_port () in
  let th = { th with Ocs_types.th_stdout = Ocs_types.Sport sp } in
  begin try
     Ocs_prim.load_file ocs_env th filename;
     let s = Ocs_port.get_string sp in
     let src = Lexing.from_string s in
     let toks = pre_tokens_of_lexbuf pre_flx_lex src state' in
     toks

   with
     | Ocs_error.Error err ->
       clierr sr ("Scheme error " ^ err)
     | Ocs_error.ErrorL ((file, line), err) ->
       let sr2 = file,line,1,line+1,1 in
       clierr2 sr sr2
       ("Scheme error " ^ err)
  end

let scheme_string_directive state sr s pre_flx_lex lexbuf =
  let e = state#get_ocs_env in
  let th = Ocs_top.make_thread () in
  let sp = Ocs_port.string_output_port () in
  let th = { th with Ocs_types.th_stdout = Ocs_types.Sport sp } in
  begin try
     Ocs_prim.load_string e th s;
     let s = Ocs_port.get_string sp in
     let src = Lexing.from_string s in
     let toks = pre_tokens_of_lexbuf pre_flx_lex src state in
     toks

   with
     | Ocs_error.Error err ->
       clierr sr ("Scheme error " ^ err)
     | Ocs_error.ErrorL ((file, line), err) ->
       let sr2 = file,line,1,line+1,1 in
       clierr2 sr sr2
       ("Scheme error " ^ err)
  end

let slosh_cat s = 
  let mode = ref `seek_eol in
  for i = String.length s - 1 downto 0 do
    let ch = s.[i] in 
      if ch = '\n' then 
        mode := `seek_slosh
      else if ch = ' ' && !mode = `seek_slosh then ()
      else if !mode = `seek_slosh && ch = '\\' then 
        (s.[i]<-' '; mode :=`seek_eol)
      else mode := `seek_eol
  done;
  s

let handle_preprocessor state lexbuf s pre_flx_lex parse_line =
  let s = slosh_cat s in
  let sr = state#get_srcref lexbuf in
  let sr = Flx_srcref.slift sr in
  let ident,s =
    let i = ref 0 in
    while is_white s.[!i] do incr i done;
    let n = ref 1 in 
    while 
      not (is_white s.[!i + !n]) && 
      not (s.[!i + !n]='\n') 
    do incr n done;

    let ident = String.sub s !i !n in

    (* scan for next non-white *)
    let j = ref (!i + !n) in
    while is_white s.[!j] do incr j done;

    (* scan back for last non-white *)
    n := String.length s - 1;
    while !n > !j && is_white(s.[!n-1]) do decr n done;
    let ssl = !n - !j in
    let rest = String.sub s !j ssl in
    if ssl >0 && rest.[0] = '#' then
      let buf = Buffer.create 1000 in
      let rec loop () =
        let s = parse_line state lexbuf in
        state#newline lexbuf;
        let n = len s in
        if n>0 && s.[0] = '#'
        then Buffer.contents buf
        else ( 
          Buffer.add_string buf s; 
          loop ()
        )
      in 
        ident,loop ()
    else
      ident,rest
  in
  (*
  print_endline ("PREPRO i=" ^ ident^", t='"^s^"'");
  *)

  let result = match ident with

  (* print a warning *)
  | "warn" -> 
    begin match state#get_condition with
    | `Processing ->
      print_string s;
      [NEWLINE]
    | _ -> []
    end
    
  | "line" ->
    line_directive state sr s lexbuf

  | "include" 
  | "import" -> 
    let is_import = ident = "import" in
    begin match state#get_condition with
    | `Processing ->
      include_directive is_import state sr s pre_flx_lex lexbuf
    | _ -> []
    end

  | "scheme_include" -> 
    begin match state#get_condition with
    | `Processing ->
      scheme_file_directive state sr s pre_flx_lex lexbuf
    | _ -> []
    end

  | "scheme" -> 
    begin match state#get_condition with
    | `Processing ->
      scheme_string_directive state sr s pre_flx_lex lexbuf
    | _ -> []
    end

  | "define" -> 
    begin match state#get_condition with
    | `Processing ->
      define_directive state sr s pre_flx_lex
    | _ -> []
    end

  | "undef" -> 
    begin match state#get_condition with
    | `Processing ->
      undef_directive state sr s pre_flx_lex
    | _ -> []
    end

  | "if" -> 
    if_directive state sr s pre_flx_lex 

  | "ifdef" -> 
    ifdef_directive state sr s

  | "ifndef" -> 
    ifndef_directive state sr s

  | "else" -> 
    else_directive state sr s 

  | "elif" -> 
    elif_directive state sr s pre_flx_lex

  | "endif" -> 
    endif_directive state sr lexbuf
  
  | "divert" -> 
    divert_directive state sr s pre_flx_lex

  | "undivert" -> 
    undivert_directive state sr s pre_flx_lex


  | _ -> 
    print_endline (state#string_of_srcref lexbuf);
    print_endline 
    (
      "LEXICAL ERROR: IGNORING UNKNOWN PREPROCESSOR DIRECTIVE \"" ^
      ident ^ "\""
    );
    [NEWLINE]
  in 
    state#newline lexbuf;
    result


@h = tangler('src/flx_lex.mll','data')
@select(h)
{
open Flx_util
open Flx_parse
open Flx_string
open Big_int
open Flx_exceptions
open Flx_lexstate
open Flx_preproc

let lexeme = Lexing.lexeme
let lexeme_start = Lexing.lexeme_start
let lexeme_end = Lexing.lexeme_end

let substr = String.sub
let len = String.length

(* string parsers *)
let decode_qstring s = let n = len s in unescape (substr s 0 (n-1)) 
let decode_dqstring s = let n = len s in unescape (substr s 0 (n-1)) 
let decode_qqqstring s = let n = len s in unescape (substr s 0 (n-3)) 
let decode_dddstring s = let n = len s in unescape (substr s 0 (n-3)) 

let decode_raw_qstring s = let n = len s in substr s 0 (n-1) 
let decode_raw_dqstring s = let n = len s in substr s 0 (n-1) 
let decode_raw_qqqstring s = let n = len s in substr s 0 (n-3) 
let decode_raw_dddstring s = let n = len s in substr s 0 (n-3)

(* WARNING: hackery: adjust this when lex expression 'white'
   is adjutsed
*)

} 

(* ====================== REGULAR DEFINITIONS ============================ *)
(* special characters *)
let quote = '\''
let dquote = '"'
let slosh = '\\'
let linefeed = '\n'
let tab = '\t'
let space = ' '
let formfeed = '\012'
let vtab = '\011'
let carriage_return = '\013'
let underscore = '_'

(* character sets *)
let bindigit = ['0'-'1']
let octdigit = ['0'-'7'] 
let digit = ['0'-'9']
let hexdigit = digit | ['A'-'F'] | ['a'-'f']
let lower = ['a'-'z']
let upper = ['A'-'Z']
(* let letter = lower | upper *)
let letter = lower | upper
let hichar = ['\128'-'\255']
let white = space | tab

(* nasty: form control characters *)
let form_control = linefeed | carriage_return | vtab | formfeed
let newline_prefix = linefeed | carriage_return
let newline = formfeed | linefeed  | carriage_return linefeed
(* let newline = newline_prefix form_control * *)

let ordinary = letter | digit | hichar |
  '!' | '#' | '$' | '%' | '&' | '(' | ')' | '*' |
  '+' | ',' | '-' | '.' | '/' | ':' | ';' | '<' |
  '=' | '>' | '?' | '@' | '[' | ']' | '^' | '_' |
  '`' | '{' | '|' | '}' | '~'

let printable = ordinary | quote | dquote | slosh

(* identifiers *)
let ucn = 
    "\\u" hexdigit hexdigit hexdigit hexdigit 
  | "\\U" hexdigit hexdigit hexdigit hexdigit hexdigit hexdigit hexdigit hexdigit

let prime = '\''
let idletter = letter | underscore | hichar | ucn
let identifier = idletter (idletter | digit | prime )* 

(* integers *)
let bin_lit  = '0' ('b' | 'B') (underscore? bindigit) +
let oct_lit  = '0' ('o' | 'O') (underscore? octdigit) +
let dec_lit  = ('0' ('d' | 'D'))? digit (underscore? digit) *
let hex_lit  = '0' ('x' | 'X') (underscore? hexdigit)  +
let type_suffix = 
  't'|'T'|'s'|'S'|'i'|'I'|'l'|'L'|'v'|'V'|"ll"|"LL"
  | "i8" | "i16" | "i32" | "i64"
  | "u8" | "u16" | "u32" | "u64"
  | "I8" | "I16" | "I32" | "I64"
  | "U8" | "U16" | "U32" | "U64"
let signind = 'u' | 'U'
let suffix = type_suffix? signind? | signind? type_suffix?
let int_lit = (bin_lit | oct_lit | dec_lit | hex_lit) suffix

(* floats: Follows ISO C89, except that we allow underscores *)
let decimal_string = digit (underscore? digit) *
let hexadecimal_string = hexdigit (underscore? hexdigit) *

let decimal_fractional_constant = 
  decimal_string '.' decimal_string?
  | '.' decimal_string
  
let hexadecimal_fractional_constant = 
  ("0x" |"0X")
  (hexadecimal_string '.' hexadecimal_string?
  | '.' hexadecimal_string)

let decimal_exponent = ('E'|'e') ('+'|'-')? decimal_string
let binary_exponent = ('P'|'p') ('+'|'-')? decimal_string

let floating_suffix = 'L' | 'l' | 'F' | 'f' | 'D' | 'd'
let floating_literal = 
  (
    decimal_fractional_constant decimal_exponent? |
    hexadecimal_fractional_constant binary_exponent?
  )
  floating_suffix?

(* Python strings *)
let qqq = quote quote quote
let ddd = dquote dquote dquote 

let escape = slosh _ 

let dddnormal = ordinary | quote | escape | white | newline
let dddspecial = dddnormal | dquote dddnormal | dquote dquote dddnormal

let qqqnormal = ordinary | dquote | escape | white | newline
let qqqspecial = qqqnormal | quote qqqnormal | quote quote qqqnormal

let raw_dddnormal = ordinary | quote | slosh | white | newline
let raw_dddspecial = raw_dddnormal | dquote raw_dddnormal | dquote dquote raw_dddnormal

let raw_qqqnormal = ordinary | dquote | slosh | space | newline
let raw_qqqspecial = raw_qqqnormal | quote raw_qqqnormal | quote quote raw_qqqnormal

let qstring = (ordinary | dquote | escape | white) * quote
let dqstring = (ordinary | quote | escape | white) * dquote
let qqqstring = qqqspecial * qqq
let dddstring = dddspecial * ddd

let raw_qstring = (ordinary | dquote | escape | white) * quote
let raw_dqstring =  (ordinary | quote | escape | white) * dquote

let raw_qqqstring = raw_qqqspecial * qqq
let raw_dddstring = raw_dddspecial * ddd

let not_newline_or_slosh = ordinary | quote | dquote | white
let not_newline = not_newline_or_slosh | slosh
let quoted_filename = dquote (ordinary | quote | white | slosh)+ dquote

(* ====================== PARSERS ============================ *)
(* string lexers *)
rule parse_qstring state = parse
| qstring { 
      state#inbody;
      [STRING (
        state#get_srcref lexbuf, 
        state#decode decode_qstring (lexeme lexbuf)
      )] 
  }
| _ { 
    [ERRORTOKEN (
      state#get_srcref lexbuf, 
      "' string"
    )] 
  }

and parse_dqstring state = parse
| dqstring {
      state#inbody;
      [STRING (
        state#get_srcref lexbuf, 
        state#decode decode_dqstring (lexeme lexbuf)
      )] 
  }
| _ { 
    state#inbody; 
    [ERRORTOKEN (
      state#get_srcref lexbuf, 
      "\" string"
    )]
  }

and parse_qqqstring state = parse
| qqqstring { 
      state#inbody;
      [STRING (
        state#get_srcref lexbuf, 
        state#decode decode_qqqstring (lexeme lexbuf)
      )] 
  }
| _ { 
    state#inbody;
    [ERRORTOKEN (
      state#get_srcref lexbuf, 
      "''' string"
    )] 
  }

and parse_dddstring state = parse
| dddstring { 
      state#inbody;
      [STRING (
        state#get_srcref lexbuf,
        state#decode decode_dddstring (lexeme lexbuf)
      )] 
  }
| _ { 
    state#inbody;
    [ERRORTOKEN (
      state#get_srcref lexbuf,
      "\"\"\" string"
    )] 
  }

and parse_cqstring state = parse
| qstring { 
      state#inbody;
      [CSTRING (
        state#get_srcref lexbuf, 
        state#decode decode_qstring (lexeme lexbuf)
      )] 
  }
| _ { 
    [ERRORTOKEN (
      state#get_srcref lexbuf, 
      "' string"
    )] 
  }

and parse_cdqstring state = parse
| dqstring {
      state#inbody;
      [CSTRING (
        state#get_srcref lexbuf, 
        state#decode decode_dqstring (lexeme lexbuf)
      )] 
  }
| _ { 
    state#inbody; 
    [ERRORTOKEN (
      state#get_srcref lexbuf, 
      "\" string"
    )]
  }

and parse_cqqqstring state = parse
| qqqstring { 
      state#inbody;
      [CSTRING (
        state#get_srcref lexbuf, 
        state#decode decode_qqqstring (lexeme lexbuf)
      )] 
  }
| _ { 
    state#inbody;
    [ERRORTOKEN (
      state#get_srcref lexbuf, 
      "''' string"
    )] 
  }

and parse_cdddstring state = parse
| dddstring { 
      state#inbody;
      [CSTRING (
        state#get_srcref lexbuf,
        state#decode decode_dddstring (lexeme lexbuf)
      )] 
  }
| _ { 
    state#inbody;
    [ERRORTOKEN (
      state#get_srcref lexbuf,
      "\"\"\" string"
    )] 
  }

and parse_wqstring state = parse
| qstring { 
      state#inbody;
      [WSTRING (
        state#get_srcref lexbuf, 
        state#decode decode_qstring (lexeme lexbuf)
      )] 
  }
| _ { 
    [ERRORTOKEN (
      state#get_srcref lexbuf, 
      "' string"
    )] 
  }

and parse_wdqstring state = parse
| dqstring {
      state#inbody;
      [WSTRING (
        state#get_srcref lexbuf, 
        state#decode decode_dqstring (lexeme lexbuf)
      )] 
  }
| _ { 
    state#inbody; 
    [ERRORTOKEN (
      state#get_srcref lexbuf, 
      "\" string"
    )]
  }

and parse_wqqqstring state = parse
| qqqstring { 
      state#inbody;
      [WSTRING (
        state#get_srcref lexbuf, 
        state#decode decode_qqqstring (lexeme lexbuf)
      )] 
  }
| _ { 
    state#inbody;
    [ERRORTOKEN (
      state#get_srcref lexbuf, 
      "''' string"
    )] 
  }

and parse_wdddstring state = parse
| dddstring { 
      state#inbody;
      [WSTRING (
        state#get_srcref lexbuf,
        state#decode decode_dddstring (lexeme lexbuf)
      )] 
  }
| _ { 
    state#inbody;
    [ERRORTOKEN (
      state#get_srcref lexbuf,
      "\"\"\" string"
    )] 
  }

and parse_uqstring state = parse
| qstring { 
      state#inbody;
      [WSTRING (
        state#get_srcref lexbuf, 
        state#decode decode_qstring (lexeme lexbuf)
      )] 
  }
| _ { 
    [ERRORTOKEN (
      state#get_srcref lexbuf, 
      "' string"
    )] 
  }

and parse_udqstring state = parse
| dqstring {
      state#inbody;
      [USTRING (
        state#get_srcref lexbuf, 
        state#decode decode_dqstring (lexeme lexbuf)
      )] 
  }
| _ { 
    state#inbody; 
    [ERRORTOKEN (
      state#get_srcref lexbuf, 
      "\" string"
    )]
  }

and parse_uqqqstring state = parse
| qqqstring { 
      state#inbody;
      [USTRING (
        state#get_srcref lexbuf, 
        state#decode decode_qqqstring (lexeme lexbuf)
      )] 
  }
| _ { 
    state#inbody;
    [ERRORTOKEN (
      state#get_srcref lexbuf, 
      "''' string"
    )] 
  }

and parse_udddstring state = parse
| dddstring { 
      state#inbody;
      [USTRING (
        state#get_srcref lexbuf,
        state#decode decode_dddstring (lexeme lexbuf)
      )] 
  }
| _ { 
    state#inbody;
    [ERRORTOKEN (
      state#get_srcref lexbuf,
      "\"\"\" string"
    )] 
  }

and parse_raw_qstring state = parse
| raw_qstring { 
      state#inbody;
      [STRING (
        state#get_srcref lexbuf,
        state#decode decode_raw_qstring (lexeme lexbuf)
      )] 
  }
| _ { 
    state#inbody;
    [ERRORTOKEN (
     state#get_srcref lexbuf,
    "raw ' string")] 
  }

and parse_raw_dqstring state = parse
| raw_dqstring { 
      state#inbody;
      [STRING (
        state#get_srcref lexbuf,
        state#decode decode_raw_dqstring (lexeme lexbuf)
      )]
  }
| _ { 
    state#inbody;
    [ERRORTOKEN (
      state#get_srcref lexbuf,
        "raw \" string"
    )]
  }

and parse_raw_qqqstring state = parse
| raw_qqqstring { 
      state#inbody;
      [STRING (
        state#get_srcref lexbuf,
        state#decode decode_raw_qqqstring (lexeme lexbuf)
      )]
  }
| _ { state#inbody; 
  [ERRORTOKEN (
    state#get_srcref lexbuf,
    "raw ''' string")] }

and parse_raw_dddstring state = parse
| raw_dddstring { 
      state#inbody;
      [STRING (
        state#get_srcref lexbuf,
        state#decode decode_raw_dddstring (lexeme lexbuf)
      )] 
  }
| _ { 
     [ERRORTOKEN (
       state#get_srcref lexbuf,
       lexeme lexbuf)
     ] 
   }

and parse_cpp_comment state = parse 
| not_newline * newline {
      state#newline lexbuf;
      let lex = lexeme lexbuf in
      let n = String.length lex in
      [COMMENT_NEWLINE  (String.sub lex 0 (n-1))]
  }
| _ { [ERRORTOKEN (
        state#get_srcref lexbuf,
  lexeme lexbuf)] }

and parse_hashbang state = parse 
| not_newline * newline {
    begin
      state#newline lexbuf;
      let lex = lexeme lexbuf in
      let n = String.length lex in
      [COMMENT_NEWLINE  (String.sub lex 0 (n-1))]
    end
  }
| _ { [ERRORTOKEN (
        state#get_srcref lexbuf,
  lexeme lexbuf)] }

and parse_C_comment state = parse 
| "/*" { 
      state#append_comment (lexeme lexbuf);
      state#incr_comment; 
      parse_C_comment state lexbuf
  }
| newline {
      state#newline lexbuf;
      state#append_comment (lexeme lexbuf);
      parse_C_comment state lexbuf
  }
| "*/" { 
      state#append_comment (lexeme lexbuf); 
      state#decr_comment; 
      if state#comment_level > 0 
      then parse_C_comment state lexbuf 
      else ()
      ;
      state#inbody
  }
| _ {
      state#append_comment (lexeme lexbuf);
      parse_C_comment state lexbuf 
  }

and parse_line state = parse
| not_newline * newline  
  { 
    state#newline lexbuf;
    lexeme lexbuf
  }

and parse_preprocessor state = parse
| ( not_newline * slosh space * newline) * not_newline * newline 
  {
    handle_preprocessor state lexbuf (lexeme lexbuf) pre_flx_lex parse_line
  }


and pre_flx_lex state = parse
| "//" 
  { 
    parse_cpp_comment state lexbuf 
  }
| "/*" { 
      state#set_comment (lexeme lexbuf);
      parse_C_comment state lexbuf; 
      [COMMENT (state#get_comment)]
  }

| identifier { 
      state#inbody;
      let s = lexeme lexbuf in
      let s' = Flx_id.utf8_to_ucn s in
      [Flx_keywords.map_flx_keywords 
        (state#get_srcref lexbuf) 
        s'
      ]
  } 

| int_lit { 
      state#inbody;
      let sr = state#get_srcref lexbuf in
      let s = lexeme lexbuf in
      let n = String.length s in
      let converter, first =
        if n>1 && s.[0]='0' 
        then
          match s.[1] with
          | 'b' | 'B' -> binbig_int_of_string,2
          | 'o' | 'O' -> octbig_int_of_string,2
          | 'd' | 'D' -> decbig_int_of_string,2
          | 'x' | 'X' -> hexbig_int_of_string,2
          | _         -> decbig_int_of_string,0
        else decbig_int_of_string,0
      in
      let k = ref (n-1) in
      let t =
        if n >= 2 && s.[n-2]='i' && s.[n-1]='8'
        then (k:=n-2; "int8")
        else if n >= 2 && s.[n-2]='u' && s.[n-1]='8'
        then (k:=n-2; "uint8")
        else if n >= 3 && s.[n-3]='i' && s.[n-2]='1' && s.[n-1]='6'
        then (k:=n-3; "int16")
        else if n >= 3 && s.[n-3]='u' && s.[n-2]='1' && s.[n-1]='6'
        then (k:=n-3; "uint16")

        else if n >= 3 && s.[n-3]='i' && s.[n-2]='3' && s.[n-1]='2'
        then (k:=n-3; "int32")
        else if n >= 3 && s.[n-3]='u' && s.[n-2]='3' && s.[n-1]='2'
        then (k:=n-3; "uint32")

        else if n >= 3 && s.[n-3]='i' && s.[n-2]='6' && s.[n-1]='4'
        then (k:=n-3; "int64")
        else if n >= 3 && s.[n-3]='u' && s.[n-2]='6' && s.[n-1]='4'
        then (k:=n-3; "uint64")

        else begin
          let sign = ref "" in
          let typ = ref "int" in
          begin try while !k>first do 
            (match s.[!k] with  
            | 'u' | 'U' -> sign := "u"
            | 't' | 'T' -> typ := "tiny"
            | 's' | 'S' -> typ := "short"
            | 'i' | 'I' -> typ := "int"
            | 'l' | 'L' -> 
              typ := 
                if !typ = "long" then "vlong" else "long"
            | 'v' | 'V' -> typ := "vlong"
            | _ -> raise Not_found
            );
            decr k
          done with _ -> () end;
          incr k;
          !sign ^ !typ
        end
      in
      let d = String.sub s first (!k-first) in
      let v = (converter d) in
        [INTEGER (sr, t, v)]
  }

| floating_literal { 
    state#inbody;
    let str = lexeme lexbuf in
    let n = String.length str in
    let last_char = str.[n-1] in
    match last_char with
    | 'l'|'L' ->
      [FLOAT (state#get_srcref lexbuf,"ldouble", strip_us (String.sub str 0 (n-1)))]
    | 'f'|'F' ->
      [FLOAT (state#get_srcref lexbuf,"float",strip_us (String.sub str 0 (n-1)))]
    | _ ->
      [FLOAT (state#get_srcref lexbuf,"double",strip_us str)]
  }

(* one character sequences *)
| "$" { state#inbody; [DOLLAR        (state#get_srcref lexbuf)] }
| "(" { state#inbody; [LPAR          (state#get_srcref lexbuf)] }
| ")" { state#inbody; [RPAR          (state#get_srcref lexbuf)] }
| "[" { state#inbody; [LSQB          (state#get_srcref lexbuf)] }
| "]" { state#inbody; [RSQB          (state#get_srcref lexbuf)] }
| "{" { state#inbody; [LBRACE        (state#get_srcref lexbuf)] }
| "}" { state#inbody; [RBRACE        (state#get_srcref lexbuf)] }
| "!" { state#inbody; [EXCLAMATION   (state#get_srcref lexbuf)] }
| ":" { state#inbody; [COLON         (state#get_srcref lexbuf)] }
| "," { state#inbody; [COMMA         (state#get_srcref lexbuf)] }
| ";" { state#inbody; [SEMI          (state#get_srcref lexbuf)] }
| "+" { state#inbody; [PLUS          (state#get_srcref lexbuf)] }
| "-" { state#inbody; [MINUS         (state#get_srcref lexbuf)] }
| "*" { state#inbody; [STAR          (state#get_srcref lexbuf)] }
| "/" { state#inbody; [SLASH         (state#get_srcref lexbuf)] }
| "|" { state#inbody; [VBAR          (state#get_srcref lexbuf)] }
| "&" { state#inbody; [AMPER         (state#get_srcref lexbuf)] }
| "<" { state#inbody; [LESS          (state#get_srcref lexbuf)] }
| ">" { state#inbody; [GREATER       (state#get_srcref lexbuf)] }
| "=" { state#inbody; [EQUAL         (state#get_srcref lexbuf)] }
| "." { state#inbody; [DOT           (state#get_srcref lexbuf)] }
| "%" { state#inbody; [PERCENT       (state#get_srcref lexbuf)] }
| "`" { state#inbody; [BACKQUOTE     (state#get_srcref lexbuf)] }
| "~" { state#inbody; [TILDE         (state#get_srcref lexbuf)] }
| "^" { state#inbody; [CIRCUMFLEX    (state#get_srcref lexbuf)] }
| "!" { state#inbody; [EXCLAMATION   (state#get_srcref lexbuf)] }
| "?" { state#inbody; [QUEST         (state#get_srcref lexbuf)] }

(* two character sequences *)
| "=>" { state#inbody; [EQRIGHTARROW (state#get_srcref lexbuf)] }
| "&<" { state#inbody; [ANDLESS      (state#get_srcref lexbuf)] }
| "&>" { state#inbody; [ANDGREATER   (state#get_srcref lexbuf)] }
| ".." { state#inbody; [DOTDOT       (state#get_srcref lexbuf)] }
| "::" { state#inbody; [COLONCOLON   (state#get_srcref lexbuf)] }
| "==" { state#inbody; [EQEQUAL      (state#get_srcref lexbuf)] }
| "<>" 
| "!=" { state#inbody; [NOTEQUAL     (state#get_srcref lexbuf)] }
| "<=" { state#inbody; [LESSEQUAL    (state#get_srcref lexbuf)] }
| ">=" { state#inbody; [GREATEREQUAL (state#get_srcref lexbuf)] }
| "<<" { state#inbody; [LEFTSHIFT    (state#get_srcref lexbuf)] }
| ">>" { state#inbody; [RIGHTSHIFT   (state#get_srcref lexbuf)] }
| "**" { state#inbody; [STARSTAR     (state#get_srcref lexbuf)] }
| "\\" { state#inbody; [SLOSH        (state#get_srcref lexbuf)] }
| "++" { state#inbody; [PLUSPLUS     (state#get_srcref lexbuf)] }
| "--" { state#inbody; [MINUSMINUS   (state#get_srcref lexbuf)] }
| "+=" { state#inbody; [PLUSEQUAL    (state#get_srcref lexbuf)] }
| "-=" { state#inbody; [MINUSEQUAL   (state#get_srcref lexbuf)] }
| "*=" { state#inbody; [STAREQUAL    (state#get_srcref lexbuf)] }
| "/=" { state#inbody; [SLASHEQUAL   (state#get_srcref lexbuf)] }
| "%=" { state#inbody; [PERCENTEQUAL (state#get_srcref lexbuf)] }
| "^=" { state#inbody; [CARETEQUAL   (state#get_srcref lexbuf)] }
| "|=" { state#inbody; [VBAREQUAL    (state#get_srcref lexbuf)] }
| "&=" { state#inbody; [AMPEREQUAL   (state#get_srcref lexbuf)] }
| "~=" { state#inbody; [TILDEEQUAL   (state#get_srcref lexbuf)] }
| ":=" { state#inbody; [COLONEQUAL   (state#get_srcref lexbuf)] }
| "<-" { state#inbody; [LEFTARROW    (state#get_srcref lexbuf)] }
| "->" { state#inbody; [RIGHTARROW   (state#get_srcref lexbuf)] }
| "<:" { state#inbody; [LESSCOLON    (state#get_srcref lexbuf)] }
| ":>" { state#inbody; [COLONGREATER (state#get_srcref lexbuf)] }
| "[<" { state#inbody; [LSQANGLE     (state#get_srcref lexbuf)] }
| ">]" { state#inbody; [RSQANGLE     (state#get_srcref lexbuf)] }
| "[|" { state#inbody; [LSQBAR       (state#get_srcref lexbuf)] }
| "|]" { state#inbody; [RSQBAR       (state#get_srcref lexbuf)] }

(* three character sequences *)
| "<<=" { state#inbody; [LEFTSHIFTEQUAL (state#get_srcref lexbuf)] }
| ">>=" { state#inbody; [RIGHTSHIFTEQUAL(state#get_srcref lexbuf)] }
| "..." { state#inbody; [DOTDOTDOT      (state#get_srcref lexbuf)] }
| "<->" { state#inbody; [LEFTRIGHTARROW (state#get_srcref lexbuf)] }
| "&==" { state#inbody; [ANDEQEQUAL      (state#get_srcref lexbuf)] }
| "&<>" 
| "&!=" { state#inbody; [ANDNOTEQUAL     (state#get_srcref lexbuf)] }
| "&<=" { state#inbody; [ANDLESSEQUAL    (state#get_srcref lexbuf)] }
| "&>=" { state#inbody; [ANDGREATEREQUAL (state#get_srcref lexbuf)] }

(* Python strings *)
| quote  { state#inbody; parse_qstring state lexbuf }
| qqq    { state#inbody; parse_qqqstring state lexbuf }
| dquote { state#inbody; parse_dqstring state lexbuf }
| ddd    { state#inbody; parse_dddstring state lexbuf }

(* C strings: type char*  *)
| ('c'|'C') quote  { state#inbody; parse_cqstring state lexbuf }
| ('c'|'C') qqq    { state#inbody; parse_cqqqstring state lexbuf }
| ('c'|'C') dquote { state#inbody; parse_cdqstring state lexbuf }
| ('c'|'C') ddd    { state#inbody; parse_cdddstring state lexbuf }

(* wide strings *)
| ('w' | 'W') quote  { state#inbody; parse_wqstring state lexbuf }
| ('w' | 'W') qqq    { state#inbody; parse_wqqqstring state lexbuf }
| ('w' | 'W') dquote { state#inbody; parse_wdqstring state lexbuf }
| ('w' | 'W') ddd    { state#inbody; parse_wdddstring state lexbuf }

(* UTF32 strings *)
| ('u' | 'U') quote  { state#inbody; parse_uqstring state lexbuf }
| ('u' | 'U') qqq    { state#inbody; parse_uqqqstring state lexbuf }
| ('u' | 'U') dquote { state#inbody; parse_udqstring state lexbuf }
| ('u' | 'U') ddd    { state#inbody; parse_udddstring state lexbuf }

(* Python raw strings *)
| ('r'|'R') quote  { state#inbody; parse_raw_qstring state lexbuf }
| ('r'|'R') qqq    { state#inbody; parse_raw_qqqstring state lexbuf }
| ('r'|'R') dquote { state#inbody; parse_raw_dqstring state lexbuf }
| ('r'|'R') ddd    { state#inbody; parse_raw_dddstring state lexbuf }

(* whitespace *)
| white + { 
      (* we do NOT say 'inbody' here: we want to accept
         #directives with leading spaces
      *)
      let spaces=lexeme lexbuf in
      let column = ref 0 in
      let n = String.length spaces in
      for i=0 to n-1 do match spaces.[i] with
        | '\t' -> column := ((!column + 8) / 8) * 8
        | ' ' -> incr column
        | _ -> raise (Failure "Error in lexer, bad white space character")
      done;
      [WHITE  (!column)]
  }

(* Preprocessor Directive *)
| "#!" {
    if state#is_at_line_start
    then parse_hashbang state lexbuf
    else [
      ERRORTOKEN 
      (state#get_srcref lexbuf,
       "#")
     ]
  }

| "#" {
    if state#is_at_line_start
    then parse_preprocessor state lexbuf
    else [
      ERRORTOKEN 
      (state#get_srcref lexbuf,
       "#")
     ]
  }

(* end of line *)
| newline {
      state#newline lexbuf; 
      [NEWLINE ]
  }

(* end of file *)
| eof { 
  if state#condition_stack_length <> 1
  then 
    let sr = state#get_srcref lexbuf in
    let sr = Flx_srcref.slift sr in
    Flx_exceptions.clierr sr "Unmatched #if at end of file"
  else
    [ENDMARKER] 
  }
  
(* Anything else is an error *)
| _ { 
    state#inbody; 
    [
      ERRORTOKEN 
      (
        state#get_srcref lexbuf, lexeme lexbuf
      )
    ]
  }

{
}

@h = tangler('src/flx_lex.mli')
@select(h)
val pre_flx_lex : 
  Flx_lexstate.lexer_state -> 
  Lexing.lexbuf -> 
  Flx_parse.token list

val parse_line : 
  Flx_lexstate.lexer_state -> 
  Lexing.lexbuf -> 
  string
