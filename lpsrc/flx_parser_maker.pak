@head(1,"Parser generator generators")
These files generate various grammars for Felix, aiding
production of manual pages, elkhound, and Felix based
parsers for Felix, lua, and a few other languages.
@p()
The Felix Felix grammar is partly extracted from the working
Ocamlyacc grammar actually used to build the compiler.

@execfile(os.path.join('config','flx_data.py'))
@h = tangler('spkgs/flx_parser_maker.py')
@select(h)
iscr_source = ["lpsrc/flx_parser_maker.pak"]

@h = tangler('script/elk_flx_grgen','python')
@select(h)
import string
import sys

ts ={}
nt = ""
d = {}

def isterminal(w):
  return w[0] in "ABCDEFGHIJKLMNOPQRSTUVWXYZ"

def isnonterminal(w):
  return not isterminal(w)

def check_symbol(w):
  if isterminal(w): ts[w]=1

def cvt_symbol(w):
  if isterminal(w):
    return "TOK_" + w
  else:
    return w

def handle_line(l):
  global nt
  if l == '  | /* empty */':
    d[nt]=d[nt]+[[]]
  elif l[0]<>' ':
    nt = l[:-1]
    d[nt]=[]
  else:
    x = string.split(l)[1:]
    for w in x: check_symbol(w)
    d[nt]=d[nt]+[x]


f = open(sys.argv[1])
state = 1
for l in f:
  if l[:2] == '/*' and l[-3:-1]=='*/':
    #print "skipping",l,
    pass
  else:
    if l[:2]=='/*': state = 0
    if state:
      if l <> "\n":
        handle_line(l[:-1])
    else:
      #print "skippy",l,
      pass
    if l[:2]=='*/': state = 1
f.close()

ts['WSTRING']=1
ts['USTRING']=1
ts = ts.keys()
ts.sort()
print "terminals {"
print "   0 : TOK_EOF;"
j = 1
for k in ts:
  print ' ',"%4d" % j,': TOK_'+k,';'
  j = j + 1
print
attrtoks = ["CSTRING","STRING","WSTRING","USTRING","NAME","FLOAT","INTEGER"]
for k in attrtoks:
  print "  token(void*) TOK_"+k+";"

print "}"
print

print "context_class Flx_Elk_Parser : public UserActions {};"
print
print "nonterm(void*) top {"
print "  -> _1:compilation_unit { return _1; }"
print "}"
print
ntno = 1
for k in d.keys():
  print "nonterm(void*)",k," {"
  v = d[k]
  for p in v:
    print "  -> ",
    acnt = 0
    for w in p:
      s = cvt_symbol(w)
      if w in attrtoks or not isterminal(s):
        acnt = acnt + 1
        print "_"+str(acnt)+":"+s,
      else:
        print cvt_symbol(w),
    print "  {"
    print "     void **data = (void**)malloc(sizeof(void*)*"+str(acnt+1)+");"
    print "     data[0] = (void*)"+str(acnt)+";"
    for i in range(1,acnt+1):
      print "     data["+str(i)+"] = _"+str(i)+";"
    print "     return (void*)data;"
    print "  }"
  print "}"
  print

@h = tangler('script/flx_flx_grgen','python')
@select(h)
import string
import sys

ts ={}
nt = ""
d = {}

def isterminal(w):
  return w[0] in "ABCDEFGHIJKLMNOPQRSTUVWXYZ"

def isnonterminal(w):
  return not isterminal(w)

def check_symbol(w):
  if isterminal(w): ts[w]=1

def cvt_symbol(w):
  if isterminal(w):
    return "TOK_" + w
  else:
    return w

def cvt_symbol2(w):
  if isterminal(w):
    return "TOK_" + w
  else:
    return "nt_"+w

def handle_line(l):
  global nt
  if l == '  | /* empty */':
    d[nt]=d[nt]+[[]]
  elif l[0]<>' ':
    nt = l[:-1]
    d[nt]=[]
  else:
    x = string.split(l)[1:]
    for w in x: check_symbol(w)
    d[nt]=d[nt]+[x]


f = open(sys.argv[1])
state = 1
for l in f:
  if l[:2] == '/*' and l[-3:-1]=='*/':
    #print "skipping",l,
    pass
  else:
    if l[:2]=='/*': state = 0
    if state:
      if l <> "\n":
        handle_line(l[:-1])
    else:
      #print "skippy",l,
      pass
    if l[:2]=='*/': state = 1
f.close()

attrtok_map = {
  "CSTRING":'string',
  "STRING":'string',
  "WSTRING":'string',
  "USTRING":'string',
  "NAME":'string',
  "FLOAT":'string',
  "INTEGER":'(string * string)',
}
attrtoks = attrtok_map.keys()

print 'include "std";'
print 'include "flx_token";'
print
print 'module flx_grammar'
print '{'
print '  open flx_token;'

print

print '  '+"nonterm top:compilation_unit_t ="
print '  '+"  | _1:nt_compilation_unit =>  _1 "
print '  '+";"
print
for k in d.keys():
  print '  '+"union",k+"_t ="
  v = d[k]
  pno = 0
  nps = len(v)
  for p in v:
    pno = pno + 1
    s = "  |  "
    acnt = 0
    l = len (p)
    j = 0
    s= "  | " + k+"_"+str(pno)
    lead = " of "
    for w in p:
      ss = cvt_symbol(w)
      if w in attrtoks:
        s = s + lead +attrtok_map[w]
        lead = " * "
      elif not isterminal(w):
        acnt = acnt + 1
        s=s+ lead + ss+ "_t"
        lead = " * "
      j = j + 1
    print '  '+s
  print '  '+";"
  print

  print '  '+"nonterm nt_"+k,":",k+"_t ="
  v = d[k]
  pno = 0
  nps = len(v)
  for p in v:
    pno = pno + 1
    s = "  |  "
    acnt = 0
    l = len (p)
    j = 0
    for w in p:
      ss = cvt_symbol2(w)
      if w in attrtoks or not isterminal(w):
        acnt = acnt + 1
        s=s+ "_"+str(acnt)+":"+ss
      else:
        s=s+ss
      j = j + 1
      if j != l:
        s=s+" "
    print '  '+s

    s= "    => " + k+"_"+str(pno)
    if acnt > 0: s=s+"("
    for i in range(1,acnt+1):
      s = s + "_"+str(i)
      if i != acnt:
        s= s + ", "
    if acnt > 0: s=s+")"
    print '  '+s
    if pno != nps: print
  print '  '+";"
  print
print "}"

@h = tangler('script/flx_tokgen','python')
@select(h)
import string
import sys
import os
execfile ("config"+os.sep+"flx_data.py")
ts ={}
for k,t in flx_keywords: ts[t]=1
for t,l in flx_1_char_syms: ts[t]=1
for t,l in flx_2_char_syms: ts[t]=1
for t,l in flx_3_char_syms: ts[t]=1
ts['ERROR']=1

nt = ""
d = {}

def isterminal(w):
  return w[0] in "ABCDEFGHIJKLMNOPQRSTUVWXYZ"

def isnonterminal(w):
  return not isterminal(w)

def check_symbol(w):
  if isterminal(w): ts[w]=1

def cvt_symbol(w):
  if isterminal(w):
    return "TOK_" + w
  else:
    return w

def cvt_symbol2(w):
  if isterminal(w):
    return "TOK_" + w
  else:
    return "nt_"+w

def handle_line(l):
  global nt
  if l == '  | /* empty */':
    d[nt]=d[nt]+[[]]
  elif l[0]<>' ':
    nt = l[:-1]
    d[nt]=[]
  else:
    x = string.split(l)[1:]
    for w in x: check_symbol(w)
    d[nt]=d[nt]+[x]


f = open(sys.argv[1])
state = 1
for l in f:
  if l[:2] == '/*' and l[-3:-1]=='*/':
    #print "skipping",l,
    pass
  else:
    if l[:2]=='/*': state = 0
    if state:
      if l <> "\n":
        handle_line(l[:-1])
    else:
      #print "skippy",l,
      pass
    if l[:2]=='*/': state = 1
f.close()

attrtok_map = {
  "ERROR":'string',
  "CSTRING":'string',
  "STRING":'string',
  "WSTRING":'string',
  "USTRING":'string',
  "NAME":'string',
  "FLOAT":'string',
  "INTEGER":'(string * string)',
}
attrtoks = attrtok_map.keys()

ts['WSTRING']=1
ts['USTRING']=1
ts = ts.keys()
ts.sort()
print 'include "std";'
print
print 'module flx_token'
print '{'

print "  union flx_token_t = "
print "    | TOK_EOF                                 //0"
j = 1
for k in ts:
  s = '  | TOK_'+k
  if k in attrtoks:
    s = s + " of " + attrtok_map[k]
  print '  '+(s  + " " *45)[:45]+'//'+str(j)
  j = j + 1
print '  '+";"
print
print "}"

@h = tangler('script/elk_flx_lexgen','python')
@select(h)
import string
import sys

ts ={}
nt = ""
d = {}

def check_symbol(w):
  if w[0] in "ABCDEFGHIJKLMNOPQRSTUVWXYZ":
    ts[w]=1

def cvt_symbol(w):
  if w[0] in "ABCDEFGHIJKLMNOPQRSTUVWXYZ":
    return "TOK_" + w
  else:
    return w

def handle_line(l):
  global nt
  if l == '  | /* empty */':
    d[nt]=d[nt]+[[]]
  elif l[0]<>' ':
    nt = l[:-1]
    d[nt]=[]
  else:
    x = string.split(l)[1:]
    for w in x: check_symbol(w)
    d[nt]=d[nt]+[x]


f = open(sys.argv[1])
state = 1
for l in f:
  if l[:2] == '/*' and l[-3:-1]=='*/':
    #print "skipping",l,
    pass
  else:
    if l[:2]=='/*': state = 0
    if state:
      if l <> "\n":
        handle_line(l[:-1])
    else:
      #print "skippy",l,
      pass
    if l[:2]=='*/': state = 1
f.close()

ts['WSTRING']=1
ts['USTRING']=1
ts = ts.keys()
ts.sort()
print '#include "elk_flx_lexer.h"'
print '#include "stdio.h"'
print '#include "string.h"'
print
print "enum Elk_Flx_Lexcode {"
print "  TOK_EOF,                    //0"
j=1
for k in ts:
    print ('  TOK_'+k+',                        ')[:30]+'//'+str(j)
    j = j + 1
print "};"
print

print "char *Elk_Flx_Lexcode_Desc[] = {"
print '  "TOK_EOF",                    //0'
j=1
for k in ts:
  print ('  "TOK_'+k+'",                        ')[:30]+'//'+str(j)
  j = j + 1
print "};"
print

n = len(ts)+1
print "static int const ntoks = "+str(n)+";"


print r"""
string Elk_Flx_Lexer::tokenDesc() const
{
  switch (type) {
    case TOK_NAME:
    case TOK_INTEGER:
    case TOK_FLOAT:
    case TOK_STRING:
    case TOK_CSTRING:
    case TOK_WSTRING:
    case TOK_USTRING:
    {
      char buffer[200];
      strcpy(buffer,tokenKindDesc(type));
      strcat(buffer,"=");
      strcat(buffer,(char*)sval);
      return buffer;
    }
    default:               return tokenKindDesc(type);
  }
}

string Elk_Flx_Lexer::tokenKindDesc(int kind) const {
  return Elk_Flx_Lexcode_Desc[kind];
}

void Elk_Flx_Lexer::nextToken(LexerInterface *lex) {
  char buffer[100];
  if (fgets(buffer,100,stdin)) {
    int n = strlen(buffer) - 1; //skip the newline
    memmove(buffer+4,buffer,n);
    memcpy(buffer,"TOK_",4);
    n+=4;
    buffer[n]=0;
    buffer[n+1]=0;
    char *extra = buffer;
    // skip non white
    while(*extra && *extra != ' ')++extra;

    // terminate token
    *extra++ = 0;

    // skip white
    while(*extra && *extra == ' ')++extra;

    int k;
    for(k=0; k<ntoks; ++k)
      if(strcmp(Elk_Flx_Lexcode_Desc[k],buffer)==0)
        break;
    if(k==ntoks) {
      printf("Invalid token '%s'",buffer);
      abort();
    }
    lex->type = k;
    lex->sval = (SemanticValue)extra;
  }
  else {
    lex->type = TOK_EOF;
    lex->sval = NULL;
  }
  return;
}


#ifdef TEST_LEXER
int main()
{
  Elk_Flx_Lexer lexer;
  for (;;) {
    lexer.getTokenFunc()(&lexer);    // first call yields a function pointer

    // print the returned token
    string desc = lexer.tokenDesc();
    printf("%s\n", (char const*)desc);

    if (lexer.type == TOK_ENDMARKER) {
      break;
    }
  }

  return 0;
}
#endif // TEST_LEXER
"""

@h = tangler('misc/elk_flx_lexer.h')
@select(h)
#ifndef __FLX_ELK_FLX_LEXER_H__
#define __FLX_ELK_FLX_LEXER_H__

#include "lexerint.h"
// read characters from stdin, yield tokens for the parser
class Elk_Flx_Lexer : public LexerInterface {
public:
  // function that retrieves the next token from
  // the input stream
  static void nextToken(LexerInterface *lex);
  virtual NextTokenFunc getTokenFunc() const
    { return &Elk_Flx_Lexer::nextToken; }

  // debugging assistance functions
  string tokenDesc() const;
  string tokenKindDesc(int kind) const;
};

#endif

@h = tangler('script/get_grammar','python')
@select(h)
import string
import sys
f = open(sys.argv[1])
start = 0
count = 0
for l in f:
  if start:
    if l[:2]=='%%': start=0
    else:
      out = ""
      for ch in l:
        if ch == '{': count+=1
        elif ch == '}': count-=1
        elif count == 0: out += ch
      out = string.strip(out)
      if out:
        if out[0]=='|':
          out = string.strip(out[1:])
          if out=="": print "  | /* empty */"
          else: print "  | "+out
        elif out[-1]==':':
          print
          print out
        elif out==';': pass
        else:
          print out
  elif l[:2]=='%%': start=1
f.close()


@h = tangler('script/load_grammar','python')
@select(h)
# get ocamlyacc grammar and store as Python
# the header part is between %{ and %}
# the prelude is after that and before %%
# then comes the grammar up to %%
# then the trailer

import string
import sys
f = open(sys.argv[1])
lines = [line for line in f]

i = 0
def upto(prefix=None):
  global i
  s = []
  while 1:
    line = f[i]
    i = i + 1
    if prefix == None and
      i == len(lines) or
      prefix =- line[:len(prefix)]:
      return s
    s = s.append(line)

head = upto('%}')
prelude = upto('%%')
grammr = upto('%%')
footer = upto()

print "head:"
print head
print "prelude"
print prelude
print "body"
print body
print "footer"
print footer
sys.exit(0)

start = 0
count = 0
for l in f:
  if start:
    if l[:2]=='%%': start=0
    else:
      out = ""
      for ch in l:
        if ch == '{': count+=1
        elif ch == '}': count-=1
        elif count == 0: out += ch
      out = string.strip(out)
      if out:
        if out[0]=='|':
          out = string.strip(out[1:])
          if out=="": print "  | /* empty */"
          else: print "  | "+out
        elif out[-1]==':':
          print
          print out
        elif out==';': pass
        else:
          print out
  elif l[:2]=='%%': start=1
f.close()


